---
title: "A Hitchhiker's Guide to Reproducible Research" # replace with something serious
author: Aaron Peikert and Andreas M. Brandmaier 
output:
  bookdown::pdf_document2:
    keep_tex: TRUE
    toc: FALSE
repro:
  packages:
    - tidyverse
    - usethis
    - gert
    - aaronpeikert/repro@7bfaf98
    - here
    - rstudio/webshot2@f62e743
    - targets
    - renv
    - slider
    - patchwork
    - knitr
    - pander
    - lavaan
    - furrr
    - future.batchtools
    - rticles
  scripts:
    - R/simulation.R
    - R/simulation_funs.R
    - R/link.R
csl: apa7.csl
numbered_headings: true
bibliography: ["references.bib", "packages.bib"]
abstract: "`r tryCatch(trimws(readr::read_file('abstract.Rmd')))`"
---

<!-- the HTML comments, like this one, are meta comments, mainly describing the intent --->
<!-- each sentence below a heading summarizes what I want to say there --->
<!-- "Hands-on:" means concrete practical application, they roughly proceed from easy/familiar to hard/unfamiliar-->

```{r setup, include=FALSE}
library(repro)
automate_load_packages()
automate_load_scripts()
source(here::here("R", "link.R"))
```

```{r echo=FALSE}
# define a print method for objects of the class data.frame
knit_print.data.frame = function(x, ...) {
  res = paste(c('', '', kable(x)), collapse = '\n')
  asis_output(res)
}
# register the method
registerS3method("knit_print", "data.frame", knit_print.data.frame)
```

# Why you should care about reproducibility

A cornerstone of scientific progress is the possibility to critically assess the validity of peer scientistsâ€™ scientific conclusions.
Scientific claims should not be credible because of their originators' authority but by the transparency and replicability of their supporting evidence [@aac4716].
Here, *replicability* refers to the degree to which original studies and their derivatives yield consistent results.
In other words, a successful replication study finds a consistent effect with an original study in a new sample of data.
Enabling the assessment of scientific conclusions requires a systematic and rigorous approach to research.
Researchers across various fields have identified a *crisis* of non-replicability of findings [e.g. @aac4716]. <!--citation needed, e.g. [@aac4716]-->

<!-- define replicability and then... 
 define reproducibility -->
Even worse, researchers might often not even be able to achieve reproducibility---defined as the ability to obtain identical results from the *same* data with the *same* computer code.
Over the past decades, computational statistical methods have radically changed the ability of researchers from all disciplines to process and analyze data with complex statistical models.
With these advances came challenges that contribute to broader concerns of irreproducibility in the published literature, among them the lack of exact and unambiguous documentation of computational methods and each computational step taken.
With computationally intensive data analyses, access to the steps taken to process data and generate findings is as important as access to the data themselves [@vanlissa2020worcs].
Only with access to both data and procedures can we guarantee full computational reproducibility, meaning that the creations emanating from a given data analysis (a statistical result or an entire manuscript describing the results) can be exactly reproduced at any time in the future, by any person, and on any computer.

In fact, the research community has long accepted reproducibility as an important element of "good scientific practice." <!-- examples missing -->
However, ensuring reproducibility of computational analyses is still tedious and difficult, and there are still no widely accepted standards that can guide researchers when setting up their data analyses.
Even though transparency has increased across scholarly disciplines (that is, data and methods are increasingly openly shared), most of these open repositories do not provide sufficient information to reproduce relevant computational and statistical results.
For example, @hardwicke2018 attempted to replicate open materials in the journal Cognition.
Out of 35 published articles, results of 22 articles could be reproduced but in 11 of these cases, assistance from the original authors was required.
For 13 articles, at least one outcome could not be reproduced â€”-- even with the original authors' assistance.
@obels2020 showed that in 62 Registered Reports, 41 had data available, and 37 had analysis scripts available.
The authors could execute only 31 of the scripts without error and were able to reproduce the results of only 21 articles (within a reasonable period of time).
Thus, there is a dire need to establish standards for reproducible workflows for scientific rigor.

From our personal experience, the lack of a proper incentive structure and the dreary perspective of having to spend several extra hours if not days to properly set of a code repository to become reproducible, makes many scholars skip this extra step.
If reproduction would require merely minutes instead of hours, it could greatly facilitate collaboration within and across research projects.
<!-- Guiding principle: spending machine compute time instead of human research time -->
To achieve this boost in productivity, we must place the burden of reproducibility upon computers instead of the researchers.
To this end, we demand that a scientific product (no matter whether this is a preliminary report or a full research article) should both be understandable by researchers and automatically reproducible by computers with minimal constraints to the researchers' own workflows.

<!-- Which problems are solved by which tool (like in Andreas talk)-->
<!--stolen from: https://brandmaier.github.io/reproducible-data-analysis-materials/KULeuvenQuantPsy2020.html#11 -->
We identified the following common four causes for non-reproducibility:

1.	Multiple inconsistent versions of code and data; for example, the dataset has changed over time because outliers were removed at a later stage, or the code has changed because a bug was removed at some point in time;
2.	Copy-and-paste errors; for example, results are manually copied from a computing environment into a text processor, causing inconsistencies between the reported result and the reproduced result;
3.	Ambiguous order of code execution; for example, with multiple data and code files, it may be unclear which scripts should be executed in what order;
4.	Broken dependencies; for example, a given analysis depends on a software package that might not be available on a different computer or no longer exists.

We have developed a workflow that leverages established tools and practices from software engineering to achieve long-term computational reproducibility of scientific data analyses resting on four pillars that address the aforementioned causes of non-reproducibility [@Peikert2019]:

1. Version control
2. Dynamic document creation
3. Dependency tracking
4. Software management


This tutorial follows the recommendations of @Peikert2019 and uses _Git_ for version control, _R Markdown_ for dynamic document creation, _Make_ for dependency tracking, and _Docker_ for software management.
The interplay of these four components is shown in Figure \@ref(fig:schematic).
However, which tools you use to implement these concepts is a matter of taste and project requirements, and other choices are possible depending on the context and the degree of reproducibility one aims for [e.g., @vanlissa2020worcs; @turingway2021turing].
At several points, we suggest other viable alternatives.

```{r schematic, eval = TRUE, echo = FALSE, fig.cap="Schematic illustration of the interplay of the four components (in dashed boxes) central to the reproducible workflow: version control (Git), dependency tracking (Make), software management (Docker), and dynamic document generation (R Markdown). Git tracks changes to the project over time. Make manages dependencies among the files. Docker provides a container in which the final report is built using dynamic document generation in R Markdown. Reproduced from @Peikert2019.  "}
# file gets downloaded in Makefile
knitr::include_graphics("images/nutshell.svg", auto_pdf = TRUE)
```

This tutorial guides the reader through each step of an open research project with an emphasis on guaranteeing reproducibility from the very first step on.
This paper was designed to help researchers to practically apply the TOP guidelines [except for number 8. which does not apply to single research projects, @nosekPromotingOpenResearch2015; @vanlissa2020worcs].
We start by sketching an initial idea in a repository shared with colleagues that potentially collaborate only remotely.
We continue with the creation of a concrete data analysis plan, which ideally is already implemented in code, and a dynamic report document that can be preregistered.
Once the empirical data has been collected, the results can be calculated and appear directly in the dynamically generated document, where ideally only the discussion section needs to be added before the whole archive can be permanently stored as a fully reproducible digital artifact using a unique DOI in a persistent location.

We have structured the tutorial with a _learning by doing_ approach in mind, such that readers can follow along at their own computers.
We explicitly encourage readers to try out all R commands for themselves.
Unless stated otherwise, all code blocks are meant to be run in the statistical programming language R (tested with version `r with(R.version, paste0(major, ".", minor))`).

Here, we present a workflow that has proven useful for our way of working with a strong focus on Open Science.
However, not all steps are necessary to achieve reproducibility.
We have marked those steps that can be omitted as "optional" in the section headings.

Table \@ref(tab:overview) shows all steps required to to create and open and reproducible research project as they are presented in the remainder of the manuscript.

```{r overview, echo=FALSE}
overview <-
  read_csv(
    here::here("data", "overview.csv"),
    col_types = "cc",
    col_names = c("Phase", "Steps"),
    trim_ws = FALSE
  )
pander::pander(
  overview,
  split.tables = Inf,
  split.cells = Inf,
  missing = "",
  keep.line.breaks = TRUE,
  style = "grid",
  justify = 'left',
  caption = "\\label{tab:overview} Overview over all steps to create and open and reproducible research project."
)
```

# Setup

<!-- keep it brief, let `repro` do the work -->

This tutorial is targeted at scholars with at least intermediate R knowledge and we assume that you have already installed R and RStudio (if not, check the `r link("https://github.com/aaronpeikert/repro-tutorial/blob/main/install.md", "installation guide")`).
To create reproducible research projects in R, we have implemented our workflow in the  [`repro`-package](https://github.com/aaronpeikert/repro)[^repropackage]:

[^repropackage]: https://github.com/aaronpeikert/repro

```r
# -- type this on the R console --
if(!requireNamespace("remotes"))install.packages("remotes")
remotes::install_github("aaronpeikert/repro")
library("repro")
```

The [`repro`-package](https://github.com/aaronpeikert/repro) contains several helpers to work with the suggested tools for reproducibility.
For example, the package can check if your computer has all the required software packages installed and ready.

```{r}
# -- type this on the R console --
check_git()
check_make()
check_docker()
```

```{r, eval=FALSE}
check_github()
```


If any of these commands find that a given software requirement is not installed or set up properly, you will get a message with detailed instructions (tailored to your operating system) on how to set up your computer.
Please make sure that all three commands pass before you continue with the tutorial.
This process is only required once per computer and does not need to be repeated for each project.

# Planning Phase

<!-- researcher should begin early to enjoy most benefits -->
Reproducibility is invaluable for effective collaboration---including with your "past self".
While you might be tempted to consider reproducibility only when you about to finish a project, upfront investments pay off through your increased productivity.
Therefore, we suggest starting early, ideally from the moment you decide to pursue a project to incorporate tools to help you achieve reproducibility gradually.
The first step towards reproducibility is to bundle all required files in one folder.
This way, you never lose anything, and you can easily share the files without missing anything.
<!-- Hands-on: Introducing RProjects-->
RStudio's _R Projects_ facilitate this concept of a project folder.
The `repro` package offers an R Studio template, which sets up a file structure that follows best practices and conventions for placing scripts and data [@rextensions].
To create one, click on:

> File â†’ New Project... â†’ New Directory â†’ New Project

## Markdown

<!--Keep notes.-->
 A great tool to capture ideas or meeting notes is Markdown.
 Markdown is a lightweight text format that is very close to plain text with a minimal subset of reserved symbols for formatting instructions.
 This way, Markup does not need any specialized software for editing, it is highly fool-proof (unlike for example LaTeX) and can be ported to various document formats, such as HTML Websites, a Microsoft Word document, a typeset PDF file (for example, via LaTeX journal templates), or a Powerpoint presentation.
We suggest to use Markdown for everything, starting from simple sketches of your ideas to your scientific manuscripts[@R-rticles] and presentations[@revealjs]---and even your CV [@vitae].
To create a Markdown document, click on: 

> File â†’ New File â†’ Markdown File

Markdown is simply text, where some characters let you add a minimal amount of structure:

```{r, results='asis', echo=FALSE}
# in case we want to reuse this example later
markdown_example <- "
<!--this is a Markdown file -->
# Header

Normal text.
Important **word**.

To do list:

* do research
* do more research
* spend time with family
"
cat("```markdown", markdown_example, "```", sep = "")
```

We will later see how this simplicity enables us to create many different document formats (i.e. Word, PDF, HTML) and incorporate code (i.e. R, Python, Julia).

## Git

Another advantage of Markdown is that because of its likeness to plain text, you can easily version-control it with Git and leverage Gits features for tracking, displaying, and merging sequential and concurrent changes.
You may be familiar with Microsoft Words' "Track Changes"-Feature.
Version control is similar in its basic idea but more potent in its features.
Instead of tracking only one file, you can keep track of the whole project directory, and instead of losing all changes when you agree to them, the entire history is preserved transparently.
For example, this paper has been going through `r system2("git", c("rev-list",  "--count",  "HEAD"), stdout = TRUE)` `r link("https://github.com/aaronpeikert/repro-tutorial/commits/main", "iterations")`.
To activate Git in a given project, you can call:
<!-- Hands-on: Introduce Git -->

```r
# `package::function()` â†’ use function from package without `library(package)`
usethis::use_git()
```

Recording each change you made helps you to iterate more quickly because you know that you can effortlessly go back to previous versions.
For example, you can write down a rough project idea, knowing your collaborators and you will iterate and improve.
<!-- This is maybe the place to introduce the example-->
First, you could create a new Markdown file named `idea.md` that looks like this:

```markdown
<!--this is a Markdown file -->

# Hypothesis

Machiavellism is higher in male persons.

# Analytic Strategy

Independent t-test

# Sample

Can we use openpsychometrics.org data?
```

And you could commit it in Git.
For convenience, we use the [`gert`-package](https://github.com/r-lib/gert) [@R-gert], which wraps most git commands into R functions.
Alternatively, we suggest to either work directly in the terminal (for example, RStudio offers a command shell with various options accessible in the Menu Tools â†’ Terminal), or work using the Git pane in RStudio that offers buttons for basic operations.
<!--I am leaning towards doing everything in R, not sure though-->
<!--For Git we maybe should use RStudio's Git pane?-->

To commit your first changes to the project, start with staging all files to be included in the commit using `git_add()`.
Here, we stage our idea collection in the file `idea.md` and then commit it to our local repository with a meaningful commit message that will later help us keep track of all the changes over time.

```r
gert::git_add("idea.md")
gert::git_commit("add a first concept")
```

Now assume that after discussing with your colleagues, you settle onto a final analysis strategy using latent variable models with tests for measurement invariance because power is greater and you want to exclude bias in measurement as possible confounder [@flake2020].
This results in changes to your notes as follows:

```markdown
<!--this is a Markdown file -->

# Hypothesis

Machiavellism is higher in male persons.

# Analytic Strategy

Multigroup Confirmatory Factor Analysis with equal loadings and intercepts

# Sample

Can we use openpsychometrics.org data? 
```

Again, stage the changes to this file and commit like before using `git_add()` and `git_commit()` or use the following shorthand to stage all new and modified files and then commit:

```r
# -- type this on the R console --
gert::git_commit_all("switch to CFA to exclude bias in measurement as possible confounder")
```

And GitHub can show you what has changed, like in Figure \@ref(fig:idea-change).
This detection of what has changed works on a line-by-line basis.
We recommend therefore to keep each sentence in a Markdown on its own line to generate clean difference views.
GitHub is, in its essence, a web interface to Git, and you may learn more about how to use it under the section on [GitHub].

```{r idea-change-screenshot, include=FALSE}
# unfortunately css selectors won't work for pdf so we'll use png instead
idea_change <- here::here("images", "idea-change.png")
if(!file.exists(idea_change)){
  webshot2::webshot("https://github.com/aaronpeikert/repro-tutorial/commit/76f5d4e78b3d3bf22e84e9137756bc17767dff3b?diff=split",
                  file = idea_change,
                  selector = "div#diff-2f8beaa39e5c98706d8abb3361a8383776f8df8a195a21c0e6eeb25e5aa48f45", # only select file change
                  zoom = 2)# higher resolution
}
```

```{r idea-change, echo=FALSE, fig.cap="A screenshot of how a change tracked in Git is represented by GitHub.  ", out.width='100%'}
knitr::include_graphics(idea_change)
```

<!--I am troubled by how we should recommend to learn Git-->
Git can be challenging to learn, because it requires a rather complex mental model of maintaining potentially differing versions between different repositories (typically one central repository and one local repository per each author while each repository may have different branches).
However, there is a subsets of commands which we present here and in the section on [GitHub] (commit, push, pull) which enable you to use Git for simple version control.
There are many tutorials on learning Git and @vuorreCuratingResearchAssets2018 provide a good starting point.
However Git is designed for collaboration, we therefore recommend finding a partner to attend an online lecture or workshop to learn the ins and outs of Git.

## Simulation (optional)

<!-- impress with neatness â†’ sample size planning, preregistration and analysis in one ðŸ˜Ž-->
<!-- but explain that good science is often not as neat, strive for the ideal -->

```{r, include=FALSE}
source(here::here("R", "simulation_funs.R"))
```

```{r, include=FALSE}
loadings_jones_paulhus <- c(38, 31, 40, 52, 59, 71, 62, 46, 51)/100
cohend_jones_paulhus <- c(24, 29, 35)/100
```


There are probably few things as frustrating as realizing that the data that you gathered cannot answer the research question you had in mind.
A step to prevent unpleasant surprises is to simulate data.
The machine learning community has begun to routinely simulate data to test models and their implementation [@braiekTestingMachineLearning2020].
Other than checking technical and statistical aspects of the model, simulation is also beneficial on a conceptual level [@axelrodAdvancingArtSimulation1997].
With simulated data you can check if the expected data and the planned analysis match.
First of all, simulated data can be used as an initial check that the code for the planned analysis runs without error. 
This is particularly important if you want to preregister pieces of your analysis before data were collected.
For example, we can use simulated data to check whether a given data analysis outputs parameter estimates when we simulate missing data values.
Second, Monte Carlo simulations allow us to form realistic expectations about the unbiasedness and precision of parameter estimates and it can help with sample size planning to obtain a sensible study design. 
For a variety of standard models, there are analytic strategies to evaluate precision or statistical power, however, Monte Carlo simulation offers more flexibility in terms of mode realistic modeling conditions (e.g., missing data or non-normality).
For example, @maccallum1999sample used Monte Carlo simulations to investigate optimal sample size for exploratory factor analysis, or @curran1996robustness studied the robustness of the $\chi^2$ test statistic under non-normal data. 
No matter which analytic strategy you plan, it will come with assumptions about the data generating process under investigation. 
A simulation forces us to deal with these assumptions and their consequences explicitly.
If you identify unrealistic assumptions, you can either deliberately violate these assumptions and, by simulation, check if the analytic strategy is robust or modify the modeling approach to explicitly address them.
We will show an example using structural equation modeling (SEM) for our Machiavellism example.
Note that the general process applies to any statistical model, but the simulation here is specific to our hypothetical research question.
Typically, you have to make a number of educated guesses to simulate data that closely matches your expectation.
These guesses strongly depend on the study you are planning, and there cannot be a one-fits-all solution.
Since we typically use parametric statistical models (general linear model or SEM) or hypothesis tests that can be expressed as Gaussian models, we can usually use these models directly to simulate data.

If you are unfamiliar with the specifics of SEM, you may briefly skim the next sections and continue in section ["Random Number Generation"](#prng).
To simulate data in SEM, one first specifies a model (typically in the form of a structural model and a measurement model), selects plausible population values for all parameters, computes the model-implied mean vector and covariance matrix, and then samples as many observations as needed from a multivariate normal distribution following with the given mean vector and covariance matrix.

The selected model parameters should express our expectations for the Machiavellism subscale of the short dark triad [SD3, @Jones2013].

As a starting point, we can use already published information.
@Jones2013 report a set of factor loadings from an Exploratory Structural Equation Model [@asparouhovExploratoryStructuralEquation2009].
The estimates for the construction sample, which was recruited from Amazonâ€™s Mechanical Turk (MTurk) and consists of 489 adults from Canada and the United States, can be seen in Table \@ref(tab:loadings))
They report a gender difference (at the manifest level), which depending on the study ranges between `r min(cohend_jones_paulhus)` and `r max(cohend_jones_paulhus)`.

```{r loadings, echo=FALSE}
knitr::kable(t(data.frame(Item = as.character(1:9),
                        Loading = loadings_jones_paulhus)),
             caption = "Factor loadings from an Exploratory Structrural Equation Model as reported in Jones \\& Paulhus (2013)")
```

Typically, it is recommended to choose conservative values in the study planning phase [@brandmaier2018precision].
This way, we make sure that our analytic strategy is viable even in non-optimal circumstances, for example if the measurement instrument works less well then expected or the population characteristics are more different than expected.
As a heuristic guess, we assume that the factor loadings are 30% lower than those reported by @Jones2013 and that the standardized mean difference is only 0.2.
These assumption can be simply translated into [`lavaan` [@R-lavaan]](https://lavaan.ugent.be)[^lavaan] model syntax:

```{r}
model_truth <-
"MACH =~ 0.27*x1 + 0.22*x2 + 0.28*x3 + 0.36*x4 + 0.41*x5 + 0.5*x6 + 0.43*x7 +
0.32*x8 + 0.36*x9
MACH ~ c(0, 0.2)*1
MACH ~~ c(1, 1)*MACH"
```

[^lavaan]: https://lavaan.ugent.be

Data simulation is particularly easy in lavaan syntax using the function `simulateData`.
More complicated simulations (for example, including missing data simulation) are possible with other packages, e.g., simsem [@simsem], faux [@faux], or simstudy [@goldfeldSimstudyIlluminatingResearch2020].
The following code simulates data from the population model `model_truth` with 1000 cases in each group.

```{r, include=FALSE}
set.seed(1234)
```

```{r}
simulated <- lavaan::simulateData(
  model_truth,
  sample.nobs = c(1000, 1000),
  meanstructure = TRUE,
  standardized = TRUE
)
```

And then fit the statistical model:

```{r}
model <- "MACH =~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9
MACH ~~ c(1, NA)*MACH
MACH ~ c(0, NA)*1 + c(zero, diff)*1"
fit <-
  lavaan::cfa(
    model,
    data = simulated,
    group = "group",
    meanstructure = TRUE,
    std.lv = TRUE
  ) 
```

Now, `lavaan` warns us that this model may not be identified.
We overlooked a crucial assumption of this model, that is, measurement invariance.
This assumption has also been incorporated into our simulation of the data.
There is a good chance that this assumption is violated.
We therefore adjust our analysis plan to include tests for measurement invariance.
We propose the following procedure. 
After fitting the model with strong measurement invariance <!--AP: citation needed-->, we automatically free equality constraints, first on loadings, and then on intercepts based on the Lagrange Multiplier test.
The model is iteratively refit and improved until at most three loadings, or three intercepts are freed, or the Lagrange Multiplier test is not significant for the constraints on loadings or intercepts.
While reconsidering our modeling assumptions, it is also unlikely that the data are perfectly normally distributed.
First, we have rating scale items and thus our observations are categorical.
Second, @Jones2013 reported means not in the middle of the scale, thus suggesting some skewness to the items.
For simplicity of our illustration, we do not simulate these likely data problems (but doing so would undoubtedly make the simulation more realistic).

If you are satisfied with your assumptions for the data simulation, it helps to write a function that expresses them, like this one:
<!--Hands-on: build simple functions tailored for your analysis-->

```{r generate_data}
```

And to build another function that captures your planned analysis.
Note that this function is again composed of several functions, which you may inspect on `r link("https://github.com/aaronpeikert/repro-tutorial/blob/main/R/simulation_funs.R", "GitHub")`.

```{r planed_analysis}
```

```{r extract_results}
```

With its help we can simulate 1000 cases and fit our analysis.

```{r}
generate_data(1000) %>% 
  planed_analysis() %>% 
  extract_results()
```

Note, that this certainly results in different numbers each time we run it (see ["Random Number Generation"](#prng)).
Of course it is already helpful to simulate a single dataset to test if the analysis works in principle, but repeating the analysis several times and vary parameters lets you verify more properties of your analysis.
First, you can check if the planned analysis recovers the simulated values (`std_estimate` should be close to the simulated mean difference of 0.2).
Second, it enables you to plan an appropriate sample size because you are able to quantify the sampling variability.

To complete a such a  Monte-Carlo simulation study we need a function that repeats the analysis for different conditions (here only for different n):

```{r simulation_study}
```

Such Monte-Carlo simulation can help you decide between several tradeoffs.
One historically important tradeoff is between sample size and power (the probability to detect an effect of given size).
More recently there has been growing interest in other resource relevant parameters as well [!citation], such as number of measurement occasions, reliability of measures etc.
To show that there can also be alternative approaches for sample size planning beyond null hypothesis testing, we choose to visualize the density distribution of the parameter of interest (latent mean difference by gender of Machiavellism) instead of power.
Such process still lets you formulate expectations for the precision of the analysis, e.g. with a sample size of 1000 per group 80% of the simulated standardized effect size fall into a range of +/- .1, but without necessarily rely on null hypothesis testing.
For simplicity we do not show the code for the simulation/graphic, but it is available in the accompanying [GitHub repository](https://github.com/aaronpeikert/repro-tutorial)[^gh-repo]:.

[^gh-repo]: https://github.com/aaronpeikert/repro-tutorial

```{r, include=FALSE}
res <- read_csv(here::here("data", "simulation_results.csv"), col_types = "ddddd")
```

```{r hash}
```

```{r, include=FALSE}
res_interval <- res %>%
  select(n_obs, std_estimate) %>% 
  filter(!is.na(std_estimate)) %>% 
  group_by(n_obs) %>% 
  summarise(interval = list(interval(std_estimate, seq(0.05, .2, 0.01))), 
            .groups = "drop", median = median(std_estimate)) %>% 
  unnest(c(interval)) %>% 
  mutate(width = upper - lower, perc = 1 - alpha) %>% 
  group_by(alpha) %>% 
  arrange(n_obs) %>% 
  mutate(across(c(width, upper, lower),
                ~slide_dbl(.x, mean, na.rm = TRUE, .before = 2, .after = 0, .complete = FALSE))) %>% 
  ungroup() %>% 
  arrange(perc)
```

```{r, include=FALSE}
choosen_perc <- .8
choosen_width <- .2
true_diff <- .2
est_diff <- res %>% filter(n_obs == max(n_obs)) %>% pull(std_estimate) %>% mean()
minn <- filter(res_interval, perc == choosen_perc, width < choosen_width) %>% 
  filter(n_obs == min(n_obs))
sim_n <- pull(res, n_obs)
sim_nmax <- max(sim_n)
sim_n_unique <- unique(sim_n)
sim_nstep <- sim_n_unique[2] - sim_n_unique[1]
```

```{r, include=FALSE}
plot_interval <- res_interval %>%
  ggplot(aes(
    n_obs,
    ymin = lower,
    ymax = upper,
    fill = perc,
    group = alpha
  )) +
  geom_ribbon() +
  geom_line(aes(y = median), color = "white") +
  geom_hline(yintercept = true_diff, linetype = "dotted") +
  scale_fill_viridis_c(
    begin = 1,
    end = 0,
    breaks  = 1 - seq(0.05, .2, 0.05),
    labels = . %>% scales::percent(accuracy = 1)
  ) +
  scale_y_continuous(n.breaks = 10) +
  scale_x_continuous(breaks = seq(0, sim_nmax, sim_nstep * 10)) +
  theme_minimal() +
  labs(x = "N per Group", y = "Standardized Effect Size") +
  theme(legend.title = element_blank()) +
  NULL
```

```{r, include=FALSE}
plot_width <- res_interval %>%
  ggplot(aes(n_obs, width, color = perc, group = perc)) +
  geom_line() +
  geom_point(
    data = minn,
    size = 3,
    color = "black",
    shape = 3
  ) +
  scale_color_viridis_c(
    begin = 1,
    end = 0,
    breaks = 1 - seq(0.05, .2, 0.05),
    labels = . %>% scales::percent(accuracy = 1)
  ) +
  theme_minimal() +
  scale_y_log10(breaks = seq(0, 0.8, .1)) +
  scale_x_continuous(breaks = seq(0, sim_nmax, sim_nstep * 10)) +
  labs(x = "N per Group", y = "Width of quantile\n(Standardized Effect Size)") +
  theme(legend.title = element_blank()) +
  NULL
```

```{r, include=FALSE}
simulation_plot <- plot_interval +
  theme(axis.text.x = element_blank(), axis.title.x = element_blank()) + 
  plot_width +
  plot_layout(nrow = 2, guides = 'collect') +
  plot_annotation(tag_levels = 'A')
```

```{r, simulation-plot, echo=FALSE, fig.cap="*Plot A* depicts the range of effect sizes we observed in the simulation, where the colour encodes the density. The light yellow interval shows where 80% of the estimates land. The dotted line represents the simulated true effect size; the white line shows the median observed effect sizes. *Plot B* represents the width of the interval shown in Plot A on a logarithmic scale. The cross shows the minimum required sample size so that 80% of the estimates land in a range of +/- .1.  "}
simulation_plot
```

### Random Number Generation (optional) {#prng}

Strict reproducibility is at stake with any code that is based on the computer's pseudo random number generator (PRNG) because random numbers will almost always differ with each execution of the code.
To consistently get the same results while preserving the pseudo-random property of the generated numbers, it is good practice to set a random seed that sets the PRNG into a restorable state, in R code: `set.seed(1235)`. Note however that the consistency of a given seed is not necessarily maintained across different R versions because the default algorithm for generating pseudo random numbers may change (see @Peikert2019 for a demonstration with different R versions).
It is a little more complicated to achieve uncorrelated random numbers in parallelized code (e.g., when the same code is run with different portions of data in a computer cluster).
The standard random number generator (RNG) produces high quality random numbers, but is not safe to use in parallelized code (e.g., when the same code is run with different portions of data in a computer cluster).
The problem is that concurrent random number streams are correlated, because they repeat after some time.
Therefore, if you set a seed for one worker and another seed for another worker, you could end up with the same random numbers but with a phase shift.
@lecuyerGoodParametersImplementations1999 proposed a more computationally expensive RNG, designed to generate many uncorrelated streams.
While this RNG is available as a low level interface in R (with `RNGkind("L'Ecuyer-CMRG")`), we rely on the excellent parallelization framework around the [`future`-package](futureverse.org)[^futureverse] to handle this for us across all workers:

```{r}
set.seed(1235)
runif(1)
set.seed(1235)
runif(1)
```

```{r}
future::value(future::future(runif(1), seed = 1235))
future::value(future::future(runif(1), seed = 1235))
```

[^futureverse]: https://www.futureverse.org

## Preregistration (optional)

<!--Hands-on: build simple functions tailored for your analysis-->

Preregistration is the specification of a research plan in advance of data analysis.
The goal of preregistration is transparency and a clear-cut divide into exploration and confirmation.
Thus, it serves as a tool that may help to increase the credibility of empirical results.
Without getting into too much detail of the meta-scientific arguments for preregistration, we want to note two advantages.
First, a preregistration is an opportunity to plan a study carefully.
Gathering empirical data requires many resources, and better planning increases the chances that these efforts are worthwhile.
For us, a good preregistration is the same as a good plan; the only difference is that a preregistration is made public beforehand.

The second advantage is transparency.
A preregistration offers the interested reader more insight into the research process than a paper alone could.
@szollosi_is_2020 argue that preregistration can not fulfil its promise to reduce the false discovery rate of science if we allow deviating from them.
While such an argument has some merits, we believe that often transparency outweighs these concerns [@nosekPreregistrationHardWorthwhile2019].
Deviation from a preregistration is still possible.
If researchers deviate from their preregistration, they probably have good reason to do so, which means they have learned something from the process of conducting the study, and any deviation will simply be explicitly marked as such.
A preregistration, together with version control, allows their readers to inform themselves about these lessons learned.

### Preregistration as Code

Currently, there are various templates available that guide researchers in developing data analysis plans for preregistration [@osfstandard; @socpsytemp; @howtoreplicate]. 
Still, with any natural-language representation of an analysis plan, there remains ambiguity when turning the natural language description of a given data analysis into code [@sep-ambiguity].
Here, we discuss preregistration as code (PAC) with the R package repro, which leaves no room for ambiguity.
PAC essentially reverses the process of empirical research.
Before researchers gather data, they write a reproducible, dynamically generated manuscript including the sections Introduction, Methods, and Results---initially based on simulated data.
Then, the source code of this manuscript serves as an unambiguous description of the analysis plan and is published as a preregistration.

PAC works well within this reproducible framework.

First, the preregistration becomes a part of the reproducible repository.
It is part of the set of version-controlled files and provides transparency about when the preregistration file, data files, and code files were changed over the course of a research project.
If code was version-controlled and tagged (see Section [Git]) before data collection, all deviations from the preregistration or any post-hoc additions that were performed after data collection are transparent and are visible and comparable in the version history.

One way to ensure that you can expect your analysis to work with real data is to simulate data.
We, for example, simulated data for the Machiavellism subscale of the short dark triad (SD3) and analyzed this with structural equation modelling (SEM).
The simulation showed that we have to gather at least `r minn$n_obs` observations per group to archive enough precision and that our solution to a data problem we expect (insufficient measurement invariance) introduces a bias of `r round((est_diff - true_diff)*100/true_diff, 2)`%.
These results are also visualized in Figure \@ref(fig:simulation-plot).
If you went to the trouble to simulate data, we argue that you get a preregistration almost for free.
A preregistration in code describes sufficiently how you want to analyze the data, but ideally, you also convey the why.
Dynamic document generation combines code and text, enabling you to explain your reasoning.
See the section on [R Markdown] for an example of a dynamic document generation paradigm.
In fact, this manuscript and therefore the here presented Machiavellism example were preregistered as a full manuscript, and you can see the results on real data in section [Reproduce a Project].

### R Markdown

<!--Hands on: Introduce R Markdown-->
As @knuthCWEBSystemStructured states: "The main idea is to regard a program as a communication to human beings rather than as a set of instructions to a computer."

Above, we have already suggested [Markdown] as a universal and portable text format. 
R Markdown extends regular Markdown by allowing useres you to include R code chunks (in fact, arbitrary computer code; see @riedererChapter15Other) into a Markdown document.
Upon rendering of the document, the code blocks are executed and their output is dynamically inserted into the document.
This allows the creation of (conditionally) formatted text, statistical results, or figures that are guaranteed to be up-to-date because they are created every time anew as the document is rendered to its output format (e.g., presentation slides or a journal article).

To create an R Markdown document, click on: 

> File â†’ New File â†’ R Markdown...

The code is separated from the text by three backticks, like here:

````md
```{r}`r ''`
set.seed(1235)
runif(1)
```
````

When you include code into an R Markdown, this code is run, and its results are included in the rendered document.
With R Markdown, you can write whole articles (`r link("https://github.com/aaronpeikert/repro-tutorial/blob/main/manuscript.Rmd", "like the one you are reading")`).
You could, in principle, even write large parts of the manuscript as part of your preregistration, i.e. the introduction, theoretical background, the method section and parts of the discussion.
If you have simulated data, you could even include fake results.
Packages like [`stargazer`](https://cran.r-project.org/web/packages/stargazer/vignettes/stargazer.pdf)[^stargazer] [@stargazer], [`apaTables`](https://dstanley4.github.io/apaTables/articles/apaTables.html)[^apaTables] [@apaTables] and [`papaja`](http://frederikaust.com/papaja_man/)[^papaja] [@papaja] help you to create dynamically generated professional looking results.
The package [`report`](https://github.com/easystats/report)[^report] [@report] is particularly noteworthy because it not only generates tables, but also a simple interpretation of the effects as actual prose (e.g it quantifies the size of an effect).
Such a dynamic document is arguably the most concrete preregistration possible.
One advantage is that writing the manuscript and writing the preregistration are no longer two distinct tasks.
Therefore a preregistration can be thought of as simply rearranging tasks that you have to do anyway.

[^report]: https://github.com/easystats/report
[^stargazer]: https://cran.r-project.org/web/packages/stargazer/vignettes/stargazer.pdf
[^apaTables]: https://dstanley4.github.io/apaTables/articles/apaTables.html
[^papaja]: http://frederikaust.com/papaja_man/
<!--Hands on: gitignore resulting .html/pdf-->
<!-- A preregistration needs to be public -->

### Preregistration on GitHub

An essential aspect of a preregistration is that you have to make it public.
Of course, if you commit the preregistration to Git and upload it into a public repository, it is also public.
However, simply using GitHub as a preregistration site is not ideal.
First, you can delete your GitHub repository whenever you want.
Second, while Git meticulously records when you made which change, committing and publishing are two different steps.
Therefore, you can choose to upload to GitHub after conducting the study while still making it look like you have made the preregistration public before conducting the study.
Third, any person with some experience in Git can retroactively change the history.
You can alleviate these downsides by using zenodo.org to give a digital object identifier (DOI) to your preregistration because zenondo.org saves an unchangeable copy of your repository.
You can learn more about this in [Creating a public release].

## Prepare public release (optional)

An open research project usually entails that you publish content to the public web, without an intermediate publisher.
Since you are directly responsible for such content, you have to prepare a few things that are usually done by the publisher for you.

### README (optional)

One of the most significant benefits of open research is that other people can freely reuse materials, ideally including everything from stimuli, over computerized experiments, to data analysis code.
To use your materials, they first have to find them and learn how to use them.
Interested users find out about your project via the readme.
Historically[^readme-age], a readme file is the default place to look for human readable information in a computer archive 
the first thing they see when they visit your project on [GitHub].
It is supposed to explain to visitors what your project is about and how to reproduce it.
You could, for example, include the abstract of your (planned) paper here.

[^readme-age]: Apparently, the use of readme files can be traced back to at least 1974 as part of UCI LISP in this file: http://pdp-10.trailing-edge.com/decuslib10-04/01/43,50322/read.me.html

To add a readme to a repository run:

<!--Hands on: Add a readme-->
```r
usethis::use_readme_rmd()
```

This command will create a file with the name `README.Rmd`.
<!--knit/rmarkdown will be explained in section above-->
When you knit this file, it, in turn, creates a `README.md`.
That means you can include R code or plots etc., within the readme.
The `README.md` file will look pleasant in the browser on `r link("https://github.com/aaronpeikert/repro-tutorial#readme", "GitHub")`.

### License

Note that publishing data, code, or other materials is not sufficient to allow other users to use it, modify it, or redistribute it.
For example, according to German copyright law, a repository without a license must be assumed to be protected by copyright ([UrhG[^urhg]](https://www.gesetze-im-internet.de/englisch_urhg/englisch_urhg.html), Section 1, 7, 11).
In many countries (i.e. the USA, European Union), a content creator has exclusive right to use their creation[@stoddenLegalFrameworkReproducible2009].
A license is the legal authorization of other people to use what you have created.
<!---exact copy from our previous manuscript, do we need to change? -->
In our experience, the [Creative Commons - Attribution license (CC-BY)](https://creativecommons.org/licenses/by/4.0/) is often appropriate for sharing texts, R Markdown files, generated figures, and other media (such as stimulus materials if you created them).
In contrast, scripts and any other computer code are often best shared under the [MIT license](https://opensource.org/licenses/MIT) (or similar permissive licenses).
Copyright does often not apply to data, we therefore recommend to share data with a CC0 license to clarify that you do not hold copyright to it.
Giving data another license than CC0 is often just confusing, because a license is invalid if there is no copyright.
All licenses assure maximal freedom for future users while requiring the attribution, if possible, of the original authors in derivative work.
These licenses are also in line with the recommendations by the Reproducible Research Standard [@stoddenEnhancingReproducibilityComputational2016; @stoddenEnablingReproducibleResearch2009].
A great resource to choose a license is [choosealicense.com](https://choosealicense.com).
However, no resource, including our recommendation, replaces legal advice.
<!-- plagiary end -->
It is important to get written consent from collaborators when you change/add the license.
However, if their first contribution occurs after adding a license, they give their consent by contributing.
To add a CC-BY license to your project, simply run (there are similar commands for other licenses):

<!--Hands on: Add a license-->
```r
usethis::use_ccby_license()
```

### Data Documentation

[@codebook]

### Code of Conduct

We strongly suggest that you strive for respectful interactions with anyone who might want to contribute.
We hope it comes naturally to you.
To communicate that contributors can expect a welcoming and respectful environment, you can add the `r link( "https://www.contributor-covenant.org", "Contributor Covenant")` to your repository.
Contributing to an open project is daunting, and a public pledge to value any contributions (even when you do not incorporate them) can help others to take the first step.
The Contributor Covenant is a code of conduct used by many open source project.

To add the Contributor Covenant to your project use:

<!--Hands on: Add a code of conduct-->
```r
usethis::use_code_of_conduct()
```

However, there is some controversy around how to handle violations of a code of conduct.
For that or any other reason, you may want to change the Contributor Covenant to fit your project's needs (which you are allowed to do because it is published with a CC-BY license).
You can also consider alternative Codes of Conduct, like the `r link("https://opensource.microsoft.com/codeofconduct/", "Microsoft Open Source Code of Conduct")` or the `r link("https://berlincodeofconduct.org", "Berlin Code of Conduct")`.

If you have considered what you want to tell the public about the project with a readme, have chosen a license and thought about how to make collaboration welcoming, you may publish your Git repository.
See the section on [GitHub] on how to publish your project.

## Collaboration

Often, reproducibility is reduced to the merits it has for the reader.
However, we argue that reproducibility is vital for collaboration, not only in the sense that the consumer of a scientific product builds upon it but also that the creators work together.
The time that you invest into ensuring reproducibility is directed at ensuring effective collaboration.
Therefore, reproducibility is at its core about collaboration.

### GitHub

On your local machine, you use Git to track changes.
Online services like GitHub allow you to share these changes with others and collaborate with them.
GitHub is not the only solution; there is also [GitLab](gitlab.com) or [BitBucket](https://bitbucket.org), which are as feature-rich as GitHub.
GitLab has the advantage that your institution can host it themselves; therefore, you are not relying on commercial service providers.
Anyhow, we focus on GitHub because it is the most popular service.
To upload your changes to GitHub, you need to create an account under https://github.com/join.
As a researcher, you are eligible for the `r link("https://help.github.com/en/articles/applying-for-an-educator-or-researcher-discount", "Researcher/Educator discount")`.
The following command creates a new GitHub repository under your GitHub account (beware this is public):
<!--Hands-on: Introduce Github-->

```r
usethis::use_github()
```

To create a private repository (only you and people you invite have access), use:

```r
usethis::use_github(private = TRUE)
```

Only from this moment on files are uploaded to the internet and only those you have explicitly added and committed in Git.

After you have initially uploaded your repository to GitHub, you need to regularly synchronize your local copy and GitHub.
Uploading commits to GitHub is called "push":

```r
gert::git_push()
```

Downloading changes, e.g. from your collaborators is called "pull".

```r
gert::git_pull()
```

### GitHub Issues

<!--Hands on: Introduce GitHub Issues-->
Effective collaboration needs effective organization.
GitHub offers a ticket system called "Issues".
Here you and your collaborators can discuss and distribute tasks.

To take a look at the issue page of your project, you may use:

```r
usethis::browse_github_issues()
```

```{r issue-screenshot, include=FALSE}
# unfortunately css selectors won't work for pdf so we'll use png instead
issue <- here::here("images", "issue.png")
if(!file.exists(issue)){
  webshot2::webshot("https://github.com/aaronpeikert/repro-tutorial/issues/20",
                  file = issue,
                  selector = "div#repo-content-pjax-container", # only select file change
                  zoom = 2)# higher resolution
}
```

```{r issue, echo=FALSE, fig.cap="A screenshot of an example issue.  ", out.width='100%'}
knitr::include_graphics(issue)
```

One advantage of using the issue system of GitHub is that it enables you to link changes you made to their tasks by including the issue number (i.e. #20) in the commit message:

```r
# do not run:
gert::git_add(c("manuscript.Rmd", "references.bib"))
gert::git_commit("explain the basics of issues #20")
```

You see the linkage in Figure \@ref(fig:issue).

While it is a valuable device for working with direct collaborators, it is also great to facilitate review [@rougierSustainableComputationalScience2017].
Even if a journal does not support this, it enables post-publication review.
Readers of your manuscript can ask questions or point out how to improve it after you have published it.

### GitHub Pull Requests

<!--Hands on: Introduce GitHub PR and review-->
There are various ways, in which other scholars may interact with your published repository.
For example, in our exposition of the workflow for reproducibility [@Peikert2019], once we had published our preprint, scholars previously unknown to us started to send us spelling corrections.
When multiple persons start modifying a project in parallel, keeping track of the edits and how they are supposed to be merged is crucial.
Clearly, allowing everyone to edit the project is difficult.

_Pull requests_ allow anyone to contribute to a project, even though they were not granted write access, while the authors still retain full control over all changes.
If someone wants to contribute, but they have no permission to write to a repository, they can create a copy of the project, called "fork", where they can push their changes.
For example if you found an error in this manuscript, you can not simply change it, because you have no write access.
However, you can create a fork in your account, where you have write permission:

```r
usethis::create_from_github("aaronpeikert/repro-tutorial", fork = TRUE)
```
In this copy you then make any changes you want.
You could then request that we (or anyone with write permissions) "pull" these changes.
You can view new pull request for any project with:

```r
usethis::browse_github_pulls()
```

Before incorporating changes from someone else, they can be reviewed and adepted.
This review step can also be useful for collaboration, because you can invite a collaborator to take a look at changes you want to do.
It is therefore sometimes useful to create a pull request even though you have permission to push, because it invites your collaborators to comment or change your proposal.

You can initiate a new pull request with:

```r
usethis::pr_init("myrevision")
```

Then you make and commit your changes and open a new pull request with:

```r
usethis::pr_push()
```

After you or someone else has merged the pull request on GitHub, you can wrap it up with:

```r
usethis::pr_finish()
```

This process is especially useful if you want the feedback of a more senior researcher, because they do not need to know how to use Git but have a convenient web interface to provide comments or changes.

# Analyzing Data

While you already know how to write dynamic documents with [Markdown] and [R Markdown], and track changes with [Git] and [GitHub], two components to achieve long-term reproducibility are still missing, namely handling internal and external dependencies.

## Internal Dependencies

<!--I am not sure this is the right place, should we introduce this earlier? -->

A reader of your reproducible manuscript should be able to derive how they are to reproduce it (ideally by a statement in your [README]).
To do that, it is often crucial to understand how different project components relate to each other.
You cannot run an analysis without data; you cannot render a dynamic document without an analysis, and so forth.
Dependency tracking means that you capture the interrelations between the components of your analysis.
The "components" of an analysis are often loosely mapped onto files.
For a simple project, it might be enough to have a single R Markdown file.
However, while it is easy to understand how to reproduce the end result, this approach quickly gets unwieldy.
To better navigate the data analysis, a researcher often splits it across several scripts.
For example, they create a script to load the data and another to run the analysis.
This modularisation facilitates reuse because the loading script may be used by other projects building on the same data, while the analysis script may be employed on different data and so forth.
The downside is that reproducibility is complicated because instead of reproducing everything in one step, multiple steps are required.

Of course, there a several ways to solve this problem.
One could build an R package to separate analysis and data or use code externalization in the RMarkdown to facilitating reuse or create a "main" script that combines all steps.
We found Make to be particularly useful for two reasons.
First, it is language agnostic, therefore fusing steps that are carried out in different programming languages.
Second, it does implement a simple yet powerful caching mechanism of results which is welcome in computing-intensive analysis often found in neuroimaging or machine learning.

Make is build around the concept of computational recipes.
Like cooking recipes, the dish you want to create is at the top, followed by the necessary ingredients with the procedure at last.
In Make the dish is usually a file and is called "target".
The ingredients are called "requirements" and are usually also files.
The procedures are formulated as shell commands.
One (hypothetical) example is this:

```
spaghetti_arrabiata.pdf: spaghetti_arrabiata.Rmd arrabiata_sauce.csv pasta.csv
  Rscript -e 'rmarkdown::render("spagetti_arrabiata.Rmd")'
```

An important property is that each requirement can be itself be a target, like here:

```
spaghetti_arrabiata.pdf: spagetti_arrabiata.Rmd arrabiata_sauce.csv pasta.csv
  Rscript -e 'rmarkdown::render("spaghetti_arrabiata.Rmd")'

pasta.csv: cook_pasta.R
  Rscript -e 'source("cook_pasta.R")'

arrabiata_sauce.csv: cook_sauce.R canned_tomatoes.csv
  Rscript -e 'source("cook_sauce.R")'
```

When you cook something, and you have the necessary ingredients already at hand, would you throw it away just to prepare it again?
Unlikely.
Make behaves the same.
If a file already exists, it does not recreate it unnecessarily.
However, if a requirement is newer than its target, make recreates the target to reflect the updated dependency.
Therefore, when you update the data file, the analysis is rerun.
Because Make knows the whole dependency tree, its caching is smarter than many alternatives.

One alternative that is even smarter than Make is the R package [`targets` [@targets2021]](https://books.ropensci.org/targets/)[^targets].
`targets` can infer more about your project than Make, because it analysis the R code you write to build the dependency tree.
For this inference to work well, you have to write everything in R, and you have to write everything strictly functional.

[^targets]: https://books.ropensci.org/targets/

<!--Hands on: Introduce Make-->
<!--Hands on: Add targets for README.Rmd, preregistration.Rmd -->

The `repro` package can automatically write Makefiles for simple scenarios.
For this purpose, `repro` utilizes the YAML metadata:

```
---
title: "Preregistration"
author: "Aaron Peikert"
date: "4/12/2021"
output: html_document
repro:
  scripts:
    - R/simulation.R
---
```

This command will go through all R Markdown files, and their YAML metadata, in your repository and infer a `Makefile` from them:

```r
automate_make()
```

The generated `Makefile` automatically contains targets for all R Markdown documents.
To, for example, generate the `README.md` from the `README.Rmd` you can run:

```bash
# this should be run in the terminal
make README.md
```

It is common practice to have a target all, that encompasses all important files of the project as the first target.
If you do not specify a target, Make defaults to the first target, which is usually `all`.
`repro` reminds you of that and suggests which file you might want to add to the target `all`.
You can generally change the `Makefile` anyway you want, e.g. to add targets which are not supported by `repro`.
After you have done so, you can then run in the terminal:

```bash
# this should be run in the terminal
make
```

And it will fully reproduce the project, if you have all required software installed.

## External Dependencies

```{r, include=FALSE}
container_size <- as.numeric(fs::file_size("reprotutorial.sif"))/1024^3
```


Modern data analysis is only feasible because researchers rely on much external software.
One downside is that a considerable number of external dependencies may hinder collaboration and long term reproducibility.
The dependency on a single R package may easily create a dependency tree of more than hundred direct or indirect dependencies [@epskamp2019].
The critical problem is the fragility of a software setup.
Many components have to fit perfectly together, and each change, e.g. an update, risks that balance .
At the same time, software updates are very much necessary to remove errors or improve methodology [@epskamp2019].
Another problem is that the software environment is also variable and what works on one machine might not work on any other.
Containers directly address such issues.
Instead of leaving it to the user which software should be installed how on their computer, Docker provides all software used in an analysis, including the operating system itself.
The operating system level is important because some functionality may be passed to software layers beneath the programming language, such as calls to random number generators, linear algebra libraries and such.
Docker does this without interfering with the already installed software by using a virtual software environment that is independent of the host software environment.
Such a snapshot of the software stack is called an "image".
Packaging all needed software in such an image requires considerable amounts of storage space.
There are two major strategies helping to keep the storage requirements reasonable.
First, there is a community that maintains images for particular purposes.
For example, there are ready-made images for R, based on Ubuntu (a Linux operating system) containers [@boettigerIntroductionRockerDocker2017].
Users can then install whatever they additionally need on top of these pre-made images.
`r ifelse(is.na(container_size), "", glue::glue("The image that was used for this article uses {round(container_size, 2)}Gb of disc space."))`
Our image includes Ubuntu, R, RStudio, LaTeX as well as many R packages like the tidyverse [@tidyverse], amounting to `r installed.packages() %>% as.data.frame() %>% filter(is.na(Priority)) %>% nrow()` packages.
Second, instead of saving a binary image of the software, one can also save a container recipe, a so-called  `Dockerfile`.
This recipe is a textual description of all commands that need to be executed to recreate the image.
Such files are tiny (the `r link("https://github.com/aaronpeikert/repro-tutorial/blob/main/Dockerfile", "Dockerfile")` for this project is `r round(as.numeric(fs::file_size("Dockerfile"))/1024^1, 2)`Kb big) and are an unambiguous description of how to recreate a given container image.
However, they rely on the assumption that all software repositories will remain accessible and will continue to make available historic software versions.
For archiving, we therefore recommend that a full image is always kept (see Section on [Archiving Docker]).
With the `repro`-package adding R packages dependencies is as simple as adding them to the header under the tag `packages:`

```
---
title: "Preregistration"
author: "Aaron Peikert"
date: "4/12/2021"
output: html_document
repro:
  scripts:
    - R/simulation.R
  packages:
    - tidyverse
    - here
    - lavaan
    - furrr
---
```
<!--AP emphasize more how cool it is that this stuff is automatically generated by repro!-->
With the R version that you currently use, this specification is then translated into a `Dockerfile` that makes sure that the container image has all these packages installed.
The `Dockerfile` also contains the current date and fixes packages versions to the version of the package that were available on that day on the [Comprehensive R Archive Network (CRAN)](https://cran.r-project.org)[^cran] by relying on the [CRAN timemachine of Microsoft R Application Network (MRAN)](https://mran.microsoft.com/timemachine)[^mran].
<!-- Hands on: Introduce Docker -->

```r
automate_docker()
```

[^cran]: https://cran.r-project.org
[^mran]: https://mran.microsoft.com/timemachine

From the `Dockerfile` you can then build an image.
While you can do so manually we prefer to use Make, see [Internal Dependencies].
The `Makefile` build by `repro` does contain a recipe for creating the `Dockerfile`.

```bash
make docker
```

The `Makefile` from `repro` does also include a mechanism to send commands conditionally through the Docker container.
This mechanism is triggered by `DOCKER=TRUE` in the make call:

```bash
# this should be run in the terminal
make DOCKER=TRUE
```
The function `automate()` combines `automate_docker()` & `automate_make()`.

You can also manually edit the `Dockerfile` by amending the file `.repro/Dockerfile_manual` to install software beyond R packages.
Remember to run `repro::automate()` after you made edits to `.repro/Dockerfile_manual`.
We recommend @wiebelsLeveragingContainersReproducible2021 as a starting point for researcher who want to better understand how they can employ Docker for research.


<!-- after this everything is optional? -->

### renv (alternative)

Docker can be a bit unfriendly, because it requires a complicated mental model and some knowledge about how to install software on Linux.
An R-focused alternative is the R package [`renv`](https://rstudio.github.io/renv/articles/renv.html)[^renv][@R-renv].
`renv` isolates all R packages used in a project and records there version as well as the source from where they where installed.
This mechanism is faster and simpler to use than Docker, at the expense of only tracking R packages.
`renv` therefore is already of great use for reproducibility [@vanlissa2020worcs].
However, even when your project relies solely on R, external dependencies can still change the outcome of computations, like the operating system, the compilers used, the BLAS (Basic Linear Algebra Subprograms, responsible for carrying out many computations like matrix multiplication needed in the linear regression) system in use and so forth.
Most importantly the R version itself has to match.
For example, @Peikert2019 illustrate how the generation of random numbers across different R versions is a threat to reproducibility.

[^renv]: https://rstudio.github.io/renv/articles/renv.html

### High-performance computing environments (optional)

The `repro` package is particularly well-suited for computationally intensive data analyses and simulations that  are to run on distributed computing environments.
Through the use of containers, `repro` ensures identical execution across platforms, guaranteeing that a given data analysis or simulation designed and tested on a local machine will execute identically on a remote machine or multiple different remote machines across an entire computing cluster.
Thereby, the repro package makes massive parallel computing in heterogeneous computing environments less error-prone while ensuring the full documentation of all computational steps taken â€” even if some or all of them were run in parallel on distributed platforms.
The sheer volume of data often necessitates distributed computing, e.g. the European Bioinformatics Institute stores more than 120 Petabyte of publicly accessible data [@cookEuropeanBioinformaticsInstitute2018], single FMRI studies quickly amount to hundreds of Gigabyte[^openneuro] and even purely behavioral data like those from the moral machine experiment [>10GB, @awadMoralMachineExperiment2018] can become too large to be handled by a single local computer.

[^openneuro]: https://openneuro.org/public/datasets

As default container solution, `repro` uses Docker, which is a widely adopted standard for cloud computing.
However, Docker requires administrator rights for execution.
Whereas this is no issue on most modern cloud computing infrastructures, it is often an issue in classic HPC environments.
Singularity is an alternative solution [@singularity].
It was designed to run without administrator rights, while maintaining full compatibility with Docker images.

If you already have a Docker image, you can choose if you want to run this image with Docker or with Singularity.
Importantly, this choice does not affect how code is executed inside the container.
Like with Docker, the default `Makefile` generated by `repro` comes with a mechanism to automatically send all commands to Singularity.
To reproduce an analysis with Singularity, simple add the parameter `SINGULARITY=TRUE` as follows:

```bash
make SINGULARITY=TRUE
```

<!-- Hands on: add data via automate() -->
<!-- Hands on: automatically download data via manually editing the Makefile -->

## Reproducing a Project

A reproducible project should not only be reproducible in theory, but it should be simple to reproduce.
Typically, it is argued that results should be reproducible by a Ph.D. student from the same discipline who had experience with the statistical software used [@obels2020].
Even if all code and data are available in a repository, if reproducibility takes too long because of a lack of structure and documentation, it must be regarded as non-reproducible [@obels2020].

Ideally, you have a statement in the README that contains a short description of how to reproduce the project, whereas most steps are automatic and standardized.
This statement should also list all software requirements.
For example, you can download any git repository with these commands:

```bash
# this should be run in the terminal
git clone https://github.com/aaronpeikert/repro-tutorial.git
cd repro-tutorial
```

Projects that combine Make and Docker and adhere to the standard proposed by @Peikert2019 can then be reproduced with:

```bash
# this should be run in the terminal
make docker &&
make -B DOCKER=TRUE 
```

Typically there is a need to repeatedly reproduce a project during the development phase (e.g., on every change of  the analysis cod or, every change to the dataset).
The `repro`-package contains a function to reproduce all targets:

```{r}
repro::rerun()
```

This function automatically figures out how to reproduce an entire project and also recognizes non-default reproducibility tools (like `renv` and `singularity`). 
Of course, this automatic derivation of tools and steps is limited to the packages we have discussed here and the program may not always succeed if researchers deviate from our suggestions.

If you have written a [Preregistration as Code] you can add the real data file to the project and (re)produce the entire project with this single command.
In fact, until after we preregistered the study `r link("https://github.com/aaronpeikert/repro-tutorial/releases/tag/ v0.0.1-preregistration", "preregistration")` were the results below based on simulated data.
Only after we preregistered the whole manuscript and upon submission, they were based on actual data from https://openpsychometrics.org/_rawdata/.

```{r}
read_csv(here("data", "sd3.csv"), col_types = "dddddddddd") %>% 
  planed_analysis() %>% 
  extract_results()
```

Note that both [this tutorial](https://github.com/aaronpeikert/repro-tutorial)[^gh-repo2] as well as our article on the [reproducible workflow](https://github.com/aaronpeikert/reproducible-research)[^gh-repo3] [@Peikert2019] were created using this workflow and can be reproduced from their source code repositories.

[^gh-repo2]: https://github.com/aaronpeikert/repro-tutorial
[^gh-repo3]: https://github.com/aaronpeikert/reproducible-research

## Closed Data (optional)

<!-- Hands-on: check changed hash -->
A prerequisite to reproducibility is the permanent availability of both code and data [@akhlaghi2020towards].
We use containers to guarantee that all code components are available and work as expected.
Ideally, all data are also stored well documented [@codebook] in the repository following the FAIR principles [@wilkinsonFAIRGuidingPrinciples2016].
We believe that sharing data is highly beneficial for the research community [for a more comprehensive argument for publishing data see @vanlissa2020worcs], but this is not always feasible.

Sometimes it is just the required storage capacity that exceeds what Git can handle (Git gets slow around more than 100mb).
In such case we strongly recommend using [DataLad [@DataladDatalad2021]](http://handbook.datalad.org/en/latest/)[^datalad] or the below procedure to guarantee that the data has not changed.
In practice, however, there are various reasons, which may prohibit you from making your data publicly available, for example legal or regulatory or privacy concerns.
You may find yourself in a situation where you can make the data only available on request [e.g. @share].
A major threat to reproducibility is that the data has changed in the meantime or multiple versions are circulating and it is not exactly clear what version was used originally.
To at least check that the data was not changed, we suggest to provide a so called "hash", which is a cryptographic fingerprint.
If the data changes, so does its fingerprint or hash.
The user can therefore compare if the data they have received is the same that was used in the original publication, as long as the original publication does contain the hash of the data.

To verify data integrity, you can include a line of code after reading the data that looks like this:

[^datalad]: http://handbook.datalad.org/en/latest/

```{r}
# create a dummy data.frame with two columns
x <- data.frame(VAR1 = c(1, 2, 3, 4), VAR2 = c(0, 4, 6, 9))
# compute hash using md5
hash <- digest::digest(x, "md5")
if(hash != "5ba412f5a26f43842971dd74954fcdeb"){
  warning("Mismatch between original and current data!\nHash now is:\n    '", hash, "'")
}
```

We employed the same mechanism for our simulation to verify that the simulation is indeed reproducible[^simhash].

[^simhash]: https://github.com/aaronpeikert/repro-tutorial/blob/main/R/hash.R

A compromise between making the data publicly available and keeping it closed is the publication of synthetic data [@vanlissa2020worcs].
Synthetic data are an attempt to recreate the marginal distribution and bivariate relationships in simulated data. The R package `synthpop`[@nowokSynthpopBespokeCreation2016] provides functions to generate synthetic data .
Of course, the results of such synthetic data should generally not be interpreted but are rather meant to illustrate the workings and the structure of the output of a given data analysis.
Also, synthetic data provides an opportunity for readers to test if a project is in principle reproducible, even though they have no access to the data.

# Archival and Dissemination

The open nature of the here proposed workflow does blur the lines between published and unpublished.
However, even when your Git repository is public, you may want to signify important stages of your project, like preregistration, preprint, submission and acceptance to the journal.
These milestones deserve to be archived.
A project is only then properly archived when it is stored at least at two independent storage locations and reproducible from all locations.
Therefore, you should make extra sure that the project at these points is reproducible.
To ensure reproducible, you could ask a coworker or collaborator to reproduce the project independently from you.
The sections [Creating a DOI] and [Creating a Release] describe how you can easily create three different openly accessible longterm archived copies of your project.

When you disseminate the paper across more locations than just GitHub it can be ambiguous which version of code and data have created the document (e.g. the manuscript.pdf).
To create a clear link between the repository version and some end result you can automatically include the hash of the commit.
Either you use Git from within R:

```{r}
system2("git", c("rev-parse",  "--short",  "HEAD"), stdout = TRUE)
```

Or you can use the `gert` package:

```{r}
strtrim(gert::git_commit_id(), 7)
```

## Creating a DOI

The approach we advocate for distributes the research project, e.g. across GitHub, preregistration platforms, the publishers site, and more.
Digital Object Identifiers (DOI) tie all these locations together under one link [@derisiWhatWhysDOIs2003].
Not only can a DOI resolve to several locations, several parts of a research project can get their own DOI besides the DOI of the publication.
This is useful to link or cite a specific part of a research project, such as the preregistration, an illustration in the paper or the data that was gathered.
The DOI can also contain information about the license of the object it refers to.
<!-- public releases like preregistration, preprint -->
zenodo.org [@zenodo] is a publicly funded service provider that archives digital artifacts for research and provides doi for these archives.
While the service is independent of GitHub---in terms of storage facilities and financing---you can link GitHub and zenodo.org.
Please note that you can only link public GitHub repositories to zenodo.
You may log into zenodo.org through your GitHub account.

> Log into zenodo.org â†’ Account â†’ [GitHub](https://zenodo.org/account/settings/github/)[^zenodo-github]

[^zenodo-github]: https://zenodo.org/account/settings/github/

After you have linked a GitHub repository you trigger the archivation by creating a GitHub release, as described in the next section.

## Creating a Release

In software development, a release is the distribution of a piece of computer software indicating a certain maturity in development and level of completeness.
GitHub has a "release" function, which creates a link to a specific snapshot of your project that then is available as a single compressed file.
We believe that the idea of software releases can be leveraged for singling out important versions in a research project.
We found it useful to label the following stages of development as release:

* Preregistration (repository contains the final data analysis plan; also see the Section on PAC)
* Completion of data collection (repository contains all study data)
* Preprint (repository exactly reproduces a preprint that is hosted externally)
* Submission or Resubmission (repository exactly reproduces a given version (re)submitted to a journal)
* In Press (repository exactly reproduces the final manuscript as accepted by a scientific journal)

You have the opportunity to describe the release in more detail in the accompanying release notes and to add extra files (e.g., complementing the Markdown source code of a journal article with a rendered PDF document).

## Archiving Docker

A valid point of criticism is that this workflow introduces a bulk of software solutions and services that may not be available for long.
Superficially it relies on R, RStudio, Pandoc, Git, Make, Docker, Homebrew (OS X only) and Chocolaty (Windows only) as software and on GitHub, DockerHub, MRAN and Ubuntu Package Archive as services.
These many breakpoints can be reduced to one service and one software.
That is because all required software is bundled into the container, whose image can be stored.
Hence, one needs a storage provider which saves the container image and the other files associated with the project.
We already recommend to store all project files redundantly (your computer, on GitHub and zenodo.org).
Building the Docker image from the Dockerfile still requires DockerHub, MRAN, and Ubuntu Package Archive to be online.
We therefore suggest that you include an already build image into the releases.
The Makefile generated by `repro` does include a target to facilitate this.
To write a compressed image to disk use:

```bash
# this should be run in the terminal
make save-docker
```

Unfortunately, this image can only be used when Docker is installed.
To remove any software dependency including Docker from the archived project, one can convert the Docker image to a full system image.
Such system image can subsequently be booted on any personal computer without relying on any software or service and therefore grantees reproducibility for decades to come.
Currently, `repro` does not facilitate this conversion, but we hope to include this feature soon.

<!-- Hands on: add commit hash to results-->
<!-- Hands on: git tag + github release-->
<!-- Hands on: explain how to archive data-->
<!-- Hands on: explain how to archive docker images-->


```{r, results='asis', echo=FALSE}
link_index()
```

# Summary

Here, we provided a step-by-step tutorial with what we think are best practices for creating a fully reproducible repository for data analysis projects that center on the statistical programming language R.
The workflow avoids common threats to reproducibility, such as copy-and-paste errors, missing files, missing software dependencies, or side-effects through changing dependent software packages, changes in the programming language, or even at the level of the operating system itself.
However, the workflow as proposed by @Peikert2019 requires mastering up to four different software tools and their specification languages, which are typically not taught in the curriculum of social and behavioral sciences but rather belong to computer science and software development.
Currently, this is a serious obstacle in adopting the workflow for researchers who strive for reproducibility but have little training in computer science and software development.
The repro package, which we introduced here, supports scholars in creating reproducible workflows.
It abstracts tedious and error-prone tasks such as the creation of a Makefile or the specification of a container.
By extending the YAML specification that is used for storing meta information in R Markdown files, which are becoming increasingly popular among R users, repro helps users to implement @Peikert2019's workflow with minimal effort.
Most importantly, `repro`-is implemented such that the package itself is not needed to reproduce a workflow that was generated according to the workflow.
Importantly, this means that users that adopt the workflow do not have to rely on the future availability and maintenance of the `repro`-package.

# References

```{r, include=FALSE}
knitr::write_bib(c(.packages(), "here", "rticles", "gert", "bookdown", "lavaan", "knitr", "targets", "renv", "tidyverse"),
                 "packages.bib")
```
