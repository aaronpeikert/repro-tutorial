---
title: "A Hitchhiker's Guide to Reproducible Research" # replace with something serious
author: Aaron Peikert and Andreas M. Brandmaier 
output:
  bookdown::pdf_document2:
    keep_tex: TRUE
    toc: FALSE
repro:
  packages:
    - tidyverse
    - usethis
    - gert
    - aaronpeikert/repro@7bfaf98
    - here
    - rstudio/webshot2@f62e743
    - targets
    - renv
    - slider
    - patchwork
    - knitr
    - pander
    - lavaan
    - furrr
    - future.batchtools
    - rticles
  scripts:
    - R/simulation.R
    - R/simulation_funs.R
    - R/link.R
csl: apa7.csl
numbered_headings: true
bibliography: ["references.bib", "packages.bib"]
abstract: "`r tryCatch(trimws(readr::read_file('abstract.Rmd')))`"
---

<!-- the HTML comments, like this one, are meta comments, mainly describing the intent --->
<!-- each sentence below a heading summarizes what I want to say there --->
<!-- "Hands-on:" means concrete practical application, they roughly proceed from easy/familiar to hard/unfamiliar-->

```{r setup, include=FALSE}
library(repro)
automate_load_packages()
automate_load_scripts()
source(here::here("R", "link.R"))
```

```{r echo=FALSE}
# define a print method for objects of the class data.frame
knit_print.data.frame = function(x, ...) {
  res = paste(c('', '', kable(x)), collapse = '\n')
  asis_output(res)
}
# register the method
registerS3method("knit_print", "data.frame", knit_print.data.frame)
```

# Introduction

The papers aim is twofold.
First, we want to introduce the reader to a workflow that ensures reproducibility---defined as the ability to obtain identical results from the *same* data with the *same* computer code. 
The research community has long accepted reproducibility as integral to "good scientific practice."
Often, however, projects can not be reproduced by other research teams, despite the best efforts by the creators.
We present four reasons that lead to irreproducibility, explain four solutions from the software engineering community that eliminate these and show the researchers how the R package `repro` helps them to implement the solutions.
The second objective is to present an alternative preregistration scheme that builds upon a reproducible workflow, the so-called preregistration as code (PAC), where the researchers write the bulk of the manuscript, including the sections Introduction, Methods, and Results---initially based on simulated data, before they gather data.

This tutorial is aimed at researchers who are willing to make code, data, and materials available and should help them to ensure reproducibility of relevant statistical results.
While the willingness to share materials is required for reproducing a project it is not sufficient.
For example, @hardwicke2018 attempted to reproduce results from open materials in the journal Cognition. 
Out of 35 published articles, results of 22 articles could be reproduced but in 11 of these cases, assistance from the original authors was required.
For 13 articles, at least one outcome could not be reproduced—--even with the original authors' assistance.
@obels2020 showed that in 62 Registered Reports, 41 had data available, and 37 had analysis scripts available.
The authors could execute only 31 of the scripts without error and were able to reproduce the results of only 21 articles (within a reasonable period of time).

Even though transparency has increased across scholarly disciplines (that is, data and methods are increasingly openly shared), most of these open repositories do not provide sufficient information to reproduce relevant computational and statistical results.
These failed attempts to reproduce highlight the need for widely accepted reproducibility standards.
To increase the proportion of reproducible projects, such reproducibility standard has to balance the trade-off between being rigorous and being easy to use .
A rigorous standard increases the likelihood of any individual project to be actually reproducible.
An easy to use standard on the other hand is more likely to be employed across research projects.
The remainder of this paper is structured along these lines.
The first part introduces theoretical concepts and software solutions that together form a rigorous reproducibility standard.
The second part is a tutorial that shows how these software solutions can be easily employed by researchers with the help of the R package `repro`.

# Concepts and Software Solutions

<!-- could andreas write this? --->

# Tutorial

# Preregistration as Code 

We argue that a preregistration as code (PAC) is an excellent planning tool that offers several benefits over traditional preregistration.
For a PAC, researchers write a reproducible, dynamically generated manuscript including the sections Introduction, Methods, and Results before they gather data.

We consider three criteria when planning a study.
First, the plan should be comprehensive, e.g. state the research question and describe the study design and analysis.
Second, the plan should be effective, meaning that the researchers can reasonably expect it to answer the research question at hand. 
Third, the plan should be efficient; therefore, it only employs the necessary amount of resources.

To ensure a comprehensive plan, we suggest that researchers borrow from widely employed standards for writing an empirical manuscript.
The introduction and theoretical background provide the basis for the studies design and can be written before gathering data.
We argue that researchers can plan more effectively if they formulate the planned analysis as a method section that again follows best practices for describing empirical research.
To ensure that the analysis is technically feasible, they can translate their method section into executable computer code for the data analysis using simulated data.
The researchers can include a "faked " results section based on the simulated data, employing dynamic document generation.
Repeated simulation with varying parameters, a so-called Monte Carlo simulation, can then be employed to plan resources to assure efficiency.
For example, researchers can assess the power (the probability to detect an effect of a given size) for varying numbers of observations in the simulation to determine the required sample size for their study.

Defining the research questions and planning data analysis before observing the research outcomes is called preregistration [https://www.pnas.org/content/115/11/2600].
Preregistration increases the credibility of empirical results in three ways [preregistration is hard and worthwhile, Nosek].

> First, preregistration of analysis plans makes clear which analyses were planned a priori and which were conducted post hoc. This improves calibration of uncertainty for unplanned analyses, and diagnosticity of statistical inferences for planned analyses. [...] Doing so has the benefit of strengthening statistical inferences as compared with unplanned analyses. [...] Second, preregistration enables detection of questionable research practices such as selective outcome reporting (John et al. 2012) or Hypothesizing After the Results are Known (HARKing; Kerr, 1998). Third, preregistration of studies can reduce the impact of publication bias—particularly the prioritization of publishing positive over negative results—by making all planned studies discoverable whether or not they are ultimately published.

A preregistration as code offers four advantages over classical preregistration.
First, a PAC removes any ambiguity regarding the translation of an analysis plan into code.
Second, despite being rigorous, it offers the flexibility to incorporate data-dependent decisions if they can be formulated as code.
Researchers can, for example, formulate conditions under which they prefer one analysis over the other, i.e. if distributional assumptions are not met, employ robust methods or automated variable selection mechanisms.
Third, deviations from the preregistration are more explicit because they are reflected in changes to the code a property facilitated by version control.
Fourth, the preregistration is merely a development stage of the final manuscript, thus saving the researchers from writing and the reviewer from evaluating two separate documents.

# References

```{r, include=FALSE}
knitr::write_bib(c(.packages(), "here", "rticles", "gert", "bookdown", "lavaan", "knitr", "targets", "renv", "tidyverse"),
                 "packages.bib")
```
