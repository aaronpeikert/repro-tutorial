---
title: "A Hitchhiker's Guide to Reproducible Research" # replace with something serious
output: bookdown::pdf_document2
repro:
  packages:
    - usethis
    - git2r
    - aaronpeikert/repro@7bfaf98
    - here
    - rstudio/webshot2@f62e743
    - targets
    - renv
  scripts:
    - R/lavaan_helper.R
    - R/simulation.R
    - R/link.R
csl: apa7.csl
numbered_headings: true
bibliography: ["references.bib", "packages.bib"]
---

<!-- the HTML comments, like this one, are meta comments, mainly describing the intent --->
<!-- each sentence below a heading summarizes what I want to say there --->
<!-- "Hands-on:" means concrete practical application, they roughly proceed from easy/familiar to hard/unfamiliar-->

```{r setup, include=FALSE}
library(repro)
library(tidyverse)
automate_load_scripts()
source(here::here("R", "link.R"))
```

# Why you should care about reproducibility

A cornerstone of scientific progress is the possibility to critically assess the validity of peer scientists’ scientific conclusions. Scientific claims should not be credible because of their originators' authority but by the transparency and replicability of their supporting evidence [@aac4716]. Enabling this assessment requires a systematic and rigorous approach to research.
<!-- define replicability and then... 
 define reproducibility -->
<!-- is a direct quote from my master thesis -->
<!-- I wouldn't want to cite it. @brandmaier, would you please rewrite? -->
A reproducible scientific product allows other researchers to obtain the same results from the same dataset in a way that enables substantive criticism and therefore facilitates replication.
<!-- lure the reader in -->
<!-- expand: who is calling it a requirement, maybe metascientific arguments-->
In fact, the the research community has long accepted reproducibility as an important element of “good scientific practice. <!-- examples missing --> However, ensuring reproducibility of computational analyses is still tedious and difficult, and there are still no widely accepted standards that can guide researchers when setting up their data analyses.  Even though transparency has increased across scholarly disciplines (that is, data and methods are increasingly openly shared), most of these open repositories do not provide sufficient information to reproduce relevant computational and statistical results. For example, @hardwicke2018 attempted to replicate open materials in the journal Cognition. Out of 35 published articles, results of 22 articles could be reproduced but in 11 of these cases, assistance from the original authors was required. For 13 articles, at least one outcome could not be reproduced — even with the original authors' assistance. @obels2020 showed that in 62 Registered Reports, 41 had data available, and 37 had analysis scripts available. The authors could execute only 31 of the scripts without error and were able to reproduce the results of only 21 articles (within a reasonable period of time). Thus, there is a dire need to establish standards for reproducible workflows in order to move current research from just being transparent to becoming a cumulative computational knowledge acquisition process.

From our personal experience, the lack of a proper incentive structure and the dreary perspective of having to spend several extra hours if not days to properly set of a code repository to become reproducible, makes many scholars skip this extra step. If reproduction would require merely minutes instead of hours, it could greatly facilitate collaboration within and across research projects.
<!-- Guiding principle: spending machine compute time instead of human research time -->
To archive this boost, we must place the burden of reproducing something upon computers instead of human researchers.
A scientific document should therefore be understood by researchers but reproduced by computers.

This tutorial walks through the creation of such a document that can be reproduced automatically.
Though reproducibility is a small part of the research process, it can serve as the mortar of open science building blocks (e.g. preregistration, open data, postpublication review).
Therefore, we show the whole lifecycle of an open research project, where some parts are essential to archive reproducibility.
All code chunks are meant for the reader to try out for themselves.
If not declared otherwise the code is meant to run in R (tested with version `r with(R.version, paste0(major, ".", minor))`).
Still, the reader can safely skip sections marked as optional when unfit for their research question or methods.
However, any empirical data analysis will suffer from the following threats to reproducibility:
<!-- Which problems are solved by which tool (like in Andreas talk)-->
<!--stolen from: https://brandmaier.github.io/reproducible-data-analysis-materials/KULeuvenQuantPsy2020.html#11 -->

1. Multiple versions of scripts/data (e.g., the dataset has changed over time, i.e., was further cleaned or extended)
2. Copy&paste errors (e.g., inconsistency between reported result and reproduced result)
3. unclear which scripts should be executed in which order
4. Broken software dependencies (e.g., analysis broken after an update, missing package)

It is common to rely on the craftiness of the researcher to debug these problems, but we could prevent them altogether with:

1. Version control
2. Dynamic document creation
3. Dependency tracking
4. Software management

We deem each concept necessary to archive long-term reproducibility.
However, which tools you use to implement these concepts is a matter of taste and project requirements.
This tutorial follows the recommendations of @Peikert2019 and utilize Git for version control, R Markdown for dynamic document creation, Make for dependency tracking, and Docker for software management.
Their interplay is shown in Figure \@ref(fig:schematic).
At several points, we suggest other viable alternatives.

```{r schematic, eval = TRUE, echo = FALSE, fig.cap="Schematic illustration of the interplay of the four components (in dashed boxes) central to the reproducible workflow: version control (Git), dependency tracking (Make), software management (Docker), and dynamic document generation (R Markdown). Git tracks changes to the project over time. Make manages dependencies among the files. Docker provides a container in which the final report is built using dynamic document generation in R Markdown. Reproduced from @Peikert2019.  "}
# file gets downloaded in Makefile
knitr::include_graphics("images/nutshell.svg", auto_pdf = TRUE)
```

# Setup

<!-- keep it brief, let `repro` do the work -->

We assume that you have already installed R and RStudio (if not, check the `r link("https://github.com/aaronpeikert/repro-tutorial/blob/master/install.md", "installation guide")`).
Additionally, you'll need the [`repro`-package](https://github.com/aaronpeikert/repro)[^repropackage]:

[^repropackage]: https://github.com/aaronpeikert/repro

```r
# -- type this on the R console --
if(!requireNamespace("remotes"))install.packages("remotes")
remotes::install_github("aaronpeikert/repro")
library("repro")
```

The [`repro`-package](https://github.com/aaronpeikert/repro) contains several helpers to work with reproducibility tools.
For example, it can check if your computer is set up to use the tools we rely on:

```{r}
# -- type this on the R console --
check_git()
```

```{r}
# -- type this on the R console --
check_make()
```

```{r}
# -- type this on the R console --
check_docker()
```

If these commands detect that something is not installed or set up, they will give you detailed instructions (tailored to your operating system) to remedy the situation.
Follow the instructions, then rerun the command until it tells you not to worry.

# Planing

<!-- researcher should begin early to enjoy most benefits -->
Reproducibility is invaluable for effective collaboration---including with your "past self".
While you might be tempted to consider reproducibility only when you about to finish a project, upfront investments pay off through your increased productivity.
Therefore, we suggest starting early, ideally from the moment you decide to pursue a project to incorporate tools to help you archive reproducibility gradually.
The first step is to bundle all required files in one folder.
This way, you never lose anything, and you can easily share the files.
<!-- Hands-on: Introducing RProjects-->
RStudio's RProjects facilitate this concept of a project folder.
To create one, click on:

> File → New Project... → New Directory → New Project

## Markdown

<!--Keep notes.-->
<!-- Hands on: Introduce Markdown -->
A great tool to capture ideas or meeting notes is Markdown.
To create a Markdown document, click on: 

> File → New File → Markdown File

Markdown is simply text, where some characters let you add a minimal amount of structure:

```{r, results='asis', echo=FALSE}
# in case we want to reuse this example later
markdown_example <- "
<!--this is a Markdown file -->
# Header

Normal text.
Important **word**.

To do list:

* do research
* do more research
* spend time with family
"
cat("```markdown", markdown_example, "```", sep = "")
```

We will later see how this simplicity enables us to create many different document formats (i.e. Word, PDF, HTML) and incorporate code (i.e. R, Python, Julia).

## Git

Another advantage of Markdown is that you can easily version-control it with Git.
You may be familiar with Microsoft Words' "Track Changes"-Feature.
Version control is similar in its basic idea but more potent in its features.
Instead of tracking only one file, you can keep track of the whole project directory, and instead of losing all changes when you agree to them, the entire history is preserved transparently.
For example, this paper has been going through `r length(git2r::commits())` `r link("https://github.com/aaronpeikert/repro-tutorial/commits/master", "iterations")`.
To activate Git in a given project, you can call:
<!-- Hands-on: Introduce Git -->

```r
# `package::function()` → use function from package without `library(package)`
usethis::use_git()
```

Recording each change you made helps you to iterate more quickly because you know that you can effortlessly go back to previous versions.
For example, you can write down a rough project idea, knowing your collaborators and you will iterate and improve.
<!-- This is maybe the place to introduce the example-->
First, you could create a new Markdown file named `idea.md` that looks like this:

```markdown
<!--this is a Markdown file -->

# Hypothesis

Machiavellianism is higher in male persons.

# Analytic Strategy

t-test

# Sample

Could we use openpsychometrics.org data?
```

And you could commit it in Git:
<!--I am leaning towards doing everything in R, not sure though-->
<!--For Git we maybe should use RStudio's Git pane?-->

```r
git2r::add(".", "idea.md")
git2r::commit(".", "add a first concept")
```

But then you realize that the new literature suggests that there could be a bias in measurement:

```markdown
<!--this is a Markdown file -->

# Hypothesis

Machiavellianism is higher in male persons.

# Analytic Strategy

Multigroup CFA + Measurement Invariance

# Sample

Could we use openpsychometrics.org data?
```

Commit this version:

```r
# -- type this on the R console --
git2r::add(".", "idea.md")
git2r::commit(".", "exclude bias in measurement as possible confounder")
```

And GitHub can show you what has changed, like in Figure \@ref(fig:idea-change).
GitHub is, in its essence, a web interface to Git, and you may learn more about how to use it under the section on [GitHub].

```{r idea-change-screenshot, include=FALSE}
# unfortunately css selectors won't work for pdf so we'll use png instead
idea_change <- here::here("images", "idea-change.png")
if(!file.exists(idea_change)){
  webshot2::webshot("https://github.com/aaronpeikert/repro-tutorial/commit/5b3f4641ff542158184f85d458880c35e8f09f3c?diff=split",
                  file = idea_change,
                  selector = "div#diff-2f8beaa39e5c98706d8abb3361a8383776f8df8a195a21c0e6eeb25e5aa48f45", # only select file change
                  zoom = 2)# higher resolution
}
```

```{r idea-change, echo=FALSE, fig.cap="A screenshot of how a change tracked in Git is represented by GitHub.  ", out.width='100%'}
knitr::include_graphics(idea_change)
```

<!--I am troubled by how we should recommend to learn Git-->
Git can be challenging to learn because we are used to learning alone, and Git is best learned with others.
We recommend finding a partner to attend an online lecture or workshop to learn the ins and outs of Git.

## Simulation (optional)

<!-- impress with neatness → sample size planning, preregistration and analysis in one 😎-->
<!-- but explain that good science is often not as neat, strive for the ideal -->

```{r, include=FALSE}
source(here::here("R", "simulation.R"))
```

```{r, include=FALSE}
loadings_jones_paulhus <- c(38, 31, 40, 52, 59, 71, 62, 46, 51)/100
cohend_jones_paulhus <- c(24, 29, 35)/100
```

There are probably few things as frustrating as realizing that the data that you gathered cannot answer the research question you had in mind.
A step to prevent unpleasant surprises is to simulate data.
Simulated data lets you verify that the data you expect to gather and the analysis you plan fit together.
No matter which analytic strategy you plan, it comes with assumptions and a simulation forces you to address these assumptions more directly than simply thinking about them.
If you identify unrealistic assumptions, you can either violate these assumptions and see if the analytic strategy is robust or modify the analytical approach to address them.
We will show an example using structural equation modeling (SEM) for our Machiavellism example.
However, the general process applies to any statistical model, but the simulation here is specific to our hypothetical research question.
One has to make many educated guesses to simulate data that closely matches your expectation.
These guesses strongly depend on the study you are planning, and there cannot be a one fits all solution.
The analytic strategy (i.e. SEM) often gives you a hint about how to simulate data.

If you are unfamiliar with SEM, you may skim over the next sections.
To simulate data for SEM, one could specify a model and then sample from a multivariate normal distribution following its implied covariance matrix.
The model parameters should express our expectations for the Machiavellism subscale of the short dark triad (SD3).
As a starting point, we can use already published information.
@Jones2013 report loadings seen in Table \@ref(tab:loadings) and report a gender difference (at the manifest level) depending on the sample between `r min(cohend_jones_paulhus)` and `r max(cohend_jones_paulhus)`.

```{r loadings, echo=FALSE}
knitr::kable(data.frame(Item = 1:9,
                        Loading = loadings_jones_paulhus),
             caption = "Factor loadings from an Exploratory Structrural Equation Model as reported in Jones \\& Paulhus (2013)")
```

It is probably better to assume more pessimistic values.
This way, we make sure that our analytic strategy is viable even in non-optimal circumstances.
Therefore, we will assume that the loadings are 30% lower than reported and that the standardized mean difference is only 0.2.
These assumption can be simply translated into [`lavaan` [@R-lavaan]](https://lavaan.ugent.be)[^lavaan] model syntax:

[^lavaan]: https://lavaan.ugent.be

```{r, results='asis', echo=FALSE}
model_truth <- combine(
  measurement(
    "MACH",
    items("x", 9),
    loadings = round(loadings_jones_paulhus * 0.7, 2)
    ),
  intercepts("MACH", list(0, 0.2))
  )
cat("```r",
    model_truth,
    "```",
    sep = "\n")
model <- combine(measurement(
  "MACH",
  items("x", 9)),
  intercepts("MACH", list(0, "NA")))
```

From which we can simulate data:

```{r, results='hide'}
simulated <- lavaan::simulateData(
  model_truth,
  sample.nobs = c(1000, 1000),
  meanstructure = TRUE,
  standardized = TRUE
)
```

And then fit the statistical model:

```{r}
fit <-
  lavaan::cfa(
    model,
    data = simulated,
    group = "group",
    meanstructure = TRUE,
    std.lv = TRUE
  ) 
```

But wait, `lavaan` warns us that this model may not be identified.
We overlooked a crucial assumption of this model, that is, measurement invariance.
This assumption is also baked into our simulation of the data.
There is a good chance that this assumption is violated.
We, therefore, adept our analysis to plan for this contingency.
After fitting the model with strict measurement invariance, we automatically free equality constraints, first on loadings, and then on intercepts based on the Lagrange Multiplier test.
The model is iteratively refit and improved until at most three loadings, or three intercepts are freed, or the Lagrange Multiplier test is not significant for the constraints on loadings or intercepts.
While reconsidering our assumptions, it is also unlikely that the data is normally distributed.
Not only would the actual items be Likert-like and therefore categorical, but also are the in @Jones2013 reported means not in the middle of the scale, thus suggesting some skewness to the items.
For simplicity, we do not simulate these likely data problems (but doing so would undoubtedly make the simulation more realistic).
Anyhow, remember that what we consider realistic for our case could be unfit for your case.

If you are satisfied with your assumptions, it helps to write a function that expresses them, like this one:
<!--Hands-on: build simple functions tailored for your analysis-->

```{r}
generate_data
```

Note that this function is again composed of several functions, which you may inspect on `r link("https://github.com/aaronpeikert/repro-tutorial/blob/master/R/simulation.R", "GitHub")`.

With its help we can simulate 500 cases and fit our analysis.

```{r}
true_diff <- 0.2
model_truth <- combine(
  measurement(
    "MACH",
    items("x", 9),
    loadings = round(loadings_jones_paulhus * 0.7, 2)
  ),
  intercepts("MACH", list(0, true_diff)),
  variances("MACH", list(1, 1))
)

model_same <- 
  "MACH =~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9
  MACH ~ c(0,0)*1
  MACH ~~ c(1, NA)*MACH"

model_differ <- 
  "MACH =~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9
  MACH ~ c(0, NA)*1 + c(zero, diff)*1
  MACH ~~ c(1, NA)*MACH"
generate_data(500,
              model_truth,
              model_same,
              model_differ,
              list(std_estimate = get_std_estimate),
              group.equal = c("loadings", "intercepts"),
              mod_load = 3,
              mod_int = 3,
              std.lv = TRUE)
```

Note, that this certainly results in different numbers each time we run it.

```{r}
generate_data(500,
              model_truth,
              model_same,
              model_differ,
              list(std_estimate = get_std_estimate),
              group.equal = c("loadings", "intercepts"),
              mod_load = 3,
              mod_int = 3,
              std.lv = TRUE)
```

To consistently get the same results, it is good practice to set a seed with `set.seed(1235)`.
It is a little more complicated to archive uncorrelated random numbers when you have parallelized your code like we did, but the future package handles this for us:

```{r}
set.seed(1235)
runif(1)
set.seed(1235)
runif(1)
```

```{r}
future::value(future::future(runif(1), seed = 1235))
future::value(future::future(runif(1), seed = 1235))
```

## Preregestration (optional)

<!--Hands-on: build simple functions tailored for your analysis-->

Preregistration serves as a tool that may help to increase the credibility of empirical results.
Without getting lost in the meta-scientific arguments for preregistration, we want to note two advantages.
First, a preregistration is an opportunity to plan a study carefully.
Gathering empirical data requires many resources, and better planning increases the chances that these efforts are worthwhile.
For us, a good preregistration is the same as a good plan; the only difference is that a preregistration is made public beforehand.

The second advantage is transparency.
A preregistration offers the interested reader more insight into the research process than a paper alone could.
Some [citation!] argue that preregistration can not fulfil its promise to reduce the false discovery rate of science if we allow deviating from them.
While such an argument has some merits, we believe that transparency outweighs these concerns.
If researchers deviate from their preregistration, they probably have good reason to do so, which means they have learned something from the process of conducting the study.
A preregistration, together with version control, allows their readers to inform themselves about these lessons learned.

### Preregistration as Code

The likelihood that you have to diverge from your preregistration increases the more specific and concrete the preregistration is.
To nudge the researcher to still be specific enough, there are many preregistrations templates.
These templates formalize somewhat the research questions and the planned analysis.
An alternative to such protocol is to submit preregistration as code.
Therefore, instead of describing the analysis, you plan to turn into code, you skip a step and write the code beforehand.
Such preregistration as code is challenging because it leaves no place to hide behind the additional layer of abstraction that verbal descriptions provide.
It may well be that for any given project, this is impossible without deviating from the preregistration.
However, preregistered and version-controlled code lets anyone see what was changed very concretely.

One way to ensure that you can expect your analysis to work with real data is to simulate data.
A data simulation allows you to verify that the code runs error-free and lets you form more realistic expectations about the estimates' correctness and precision.
We, for example, simulated data for the Machiavellism subscale of the short dark triad (SD3) and analyzed this with structural equation modelling (SEM).
The simulation showed that we have to gather at least `missing_rcode_simulation` observations per group to archive enough precision and that our solution to a data problem we expect (insufficient measurement invariance) introduces a bias of `missing_rcode_simulation`%.
If you went to the trouble to simulate data, we argue that you get a preregistration almost for free.
A preregistration in code describes sufficiently how you want to analyze the data, but ideally, you also convey the why.
Dynamic document generation combines code and text, enabling you to explain your reasoning.
See the section on [R Markdown] for an example of a dynamic document generation paradigm.

### R Markdown

<!--Hands on: Introduce R Markdown-->
As @knuthCWEBSystemStructured states: "The main idea is to regard a program as a communication to human beings rather than as a set of instructions to a computer."
We already learned about [Markdown], to write text.
R Markdown allows you to include R code (in fact, arbitrary computer code; see @riedererChapter15Other) into a Markdown.
To create an R Markdown document, click on: 

> File → New File → R Markdown...

The code is separated from the text by three backticks, like here:

````md
```{r}`r ''`
set.seed(1235)
runif(1)
```
````

When you include code into an R Markdown, this code is run, and its results are included in the rendered document.
With R Markdown, you can write whole articles (`r link("https://github.com/aaronpeikert/repro-tutorial/blob/master/manuscript.Rmd", "like the one you are reading")`).
You could, in principle, even write large parts of the manuscript as part of your preregistration, i.e. the introduction, theoretical background, the method section and parts of the discussion.
If you have simulated data, you could even include fake results.
Such a dynamic document is arguably the most concrete preregistration possible.
One advantage is that writing the manuscript and writing the preregistration are no longer two distinct tasks.
Therefore a preregistration can be thought of as simply rearranging tasks that you have to do anyway.

<!--Hands on: gitignore resulting .html/pdf-->
<!-- A preregistration needs to be public -->

### Preregistration on GitHub

An essential aspect of a preregistration is that you have to make it public.
Of course, if you commit the preregistration to Git and upload it into a public repository, it is also public.
However, simply using GitHub as a preregistration site is not ideal.
First, you can delete your GitHub repository whenever you want.
Second, while Git meticulously records when you made which change, committing and publishing are two different steps.
Therefore, you can choose to upload to GitHub after conducting the study while still making it look like you have made the preregistration public before conducting the study. 
Third, any person with some experience in Git can retroactively change the history.
You can alleviate these downsides by using zenodo.org to give a digital object identifier (DOI) to your preregistration because zenondo.org saves an unchangeable copy of your repository.
You can learn more about this in [Creating a public release].

## Prepare public release (optional)

An open research project usually entails that you publish content to the public web, without an intermediate publisher.
Since you are directly responsible for such content, you have to prepare a few things that are usually done by the publisher for you.

### README (optional)

One of the most significant benefits of open research is that other people can freely reuse materials.
To use your materials, they first have to find them and learn how to use them.
Interested users find out about your project via the readme.
The readme is the first thing they see when they visit your project on [GitHub].
It should explain to them what your project is about and how to reproduce it.
You could, for example, include the abstract of your (planned) paper here.

To add a readme to a repository run:

<!--Hands on: Add a readme-->
```r
usethis::use_readme_rmd()
```

This command will create a file with the name `README.Rmd`.
<!--knit/rmarkdown will be explained in section above-->
When you knit this file, it, in turn, creates a `README.md`.
That means you can include R code or plots etc., within the readme.
The `README.md` file will look pleasant in the browser, just like in Figure \@ref(fig:readme).

```{r readme-screenshot, include=FALSE}
# unfortunately css selectors won't work for pdf so we'll use png instead
readme <- here::here("images", "readme.png")
webshot2::webshot("https://github.com/aaronpeikert/repro-tutorial/",
                  file = readme,
                  vwidth = 1920, # height & width are not resolution
                  vheight = 1080, # but what website thinks the browser size is
                  selector = "div#readme",
                  zoom = 2) # higher resolution
```

```{r readme, echo=FALSE, fig.cap="A screenshot of how a README.md is rendered/presented on GitHub.  ", out.width='100%'}
knitr::include_graphics(readme)
```

### License

Even when other researchers are excited to use your published code or materials, they are usually legally inhibited from doing so.
In many countries (i.e. the USA, European Union), the creator of something has exclusive right to use their creation.
A license allows other people to use what you have created.
<!---exact copy from our previous manuscript, do we need to change? -->
In our experience, the [Creative Commons - Attribution license (CC-BY)](https://creativecommons.org/licenses/by/4.0/) is often appropriate for sharing texts, R Markdown files, generated figures, and other media.
In contrast, scripts and any other computer code are often best shared under the [MIT license](https://opensource.org/licenses/MIT) (or similar permissive licenses).
Both licenses assure maximal freedom for future users while requiring the attribution of the original authors in derivative work.
These licenses are also in line with the recommendations by the Reproducible Research Standard [@stoddenEnhancingReproducibilityComputational2016; @stoddenEnablingReproducibleResearch2009].
A great resource to choose a license is [choosealicense.com](https://choosealicense.com).
However, no resource, including our recommendation, replaces legal advice.
<!-- plagiary end -->
It is important to get written consent from collaborators when you change/add the license.
However, if their first contribution occurs after adding a license, they give their consent by contributing.
To add a CC-BY license to your project, simply run (there are similar commands for other licenses):

<!--Hands on: Add a license-->
```r
usethis::use_ccby_license()
```

### Code of Conduct

We strongly suggest that you strive for respectful interactions with anyone who might want to contribute.
We hope it comes naturally to you.
To communicate that contributors can expect a welcoming and respectful environment, you can add the `r link( "https://www.contributor-covenant.org", "Contributor Covenant")` to your repository.
Contributing to an open project is daunting, and a public pledge to value any contributions (even when you do not incorporate them) can help others to take the first step.
The Contributor Covenant is a code of conduct used by many open source project.

To add the Contributor Covenant to your project use:

<!--Hands on: Add a code of conduct-->
```r
usethis::use_code_of_conduct()
```

However, there is some controversy around how to handle violations of a code of conduct.
For that or any other reason, you may want to change the Contributor Covenant to fit your project's needs (which you are allowed to do because it is published with a CC-BY license).
You can also consider alternative Codes of Conduct, like the `r link("https://opensource.microsoft.com/codeofconduct/", "Microsoft Open Source Code of Conduct")` or the `r link("https://berlincodeofconduct.org", "Berlin Code of Conduct")`.

If you have considered what you want to tell the public about the project with a readme, have chosen a license and thought about how to make collaboration welcoming, you may publish your Git repository.
See the section on [GitHub] on how to publish your project.

## Collaboration

Often, reproducibility is reduced to the merits it has for the reader.
However, we argue that reproducibility is vital for collaboration, not only in the sense that the consumer of a scientific product builds upon it but also that the creators work together.
The time that you invest into ensuring reproducibility is directed at ensuring effective collaboration.
Therefore, reproducibility is at its core about collaboration.

### GitHub

On your local machine, you use Git to track changes.
Online services like GitHub allow you to share these changes with others and collaborate with them.
GitHub is not the only solution; there is also [GitLab](gitlab.com) or [BitBucket](https://bitbucket.org), which are as feature-rich as GitHub.
GitLab has the advantage that your institution can host it themselves; therefore, you are not relying on commercial service providers.
Anyhow, we focus on GitHub because it is the most popular service.
To upload your changes to GitHub, you need to create an account under https://github.com/join.
As a researcher, you are eligible for the `r link("https://help.github.com/en/articles/applying-for-an-educator-or-researcher-discount", "Researcher/Educator discount")`.
The following command creates a new GitHub repository under your GitHub account (beware this is public):
<!--Hands-on: Introduce Github-->

```r
usethis::use_github()
```

To create a private repository (only you and people you invite have access), use:

```r
usethis::use_github(private = TRUE)
```

Only from this moment on files are uploaded to the internet and only those you have explicitly added and committed in Git.

### GitHub Issues

<!--Hands on: Introduce GitHub Issues-->
Effective collaboration needs effective organization.
GitHub offers a ticket system called "Issues".
Here you and your collaborators can discuss and distribute tasks.

To take a look at the issue page of your project, you may use:

```r
usethis::browse_github_issues()
```

```{r issue-screenshot, include=FALSE}
# unfortunately css selectors won't work for pdf so we'll use png instead
issue <- here::here("images", "issue.png")
if(!file.exists(issue)){
  webshot2::webshot("https://github.com/aaronpeikert/repro-tutorial/issues/20",
                  file = issue,
                  selector = "div#repo-content-pjax-container", # only select file change
                  zoom = 2)# higher resolution
}
```

```{r issue, echo=FALSE, fig.cap="A screenshot of an example issue.  ", out.width='100%'}
knitr::include_graphics(issue)
```

One advantage of using the issue system of GitHub is that it enables you to link changes you made to their tasks by including the issue number (i.e. #20) in the commit message:

```r
# do not run:
gert::git_add(c("manuscript.Rmd", "references.bib"))
gert::git_commit("explain the basics of issues #20")
```

You see the linkage in Figure \@ref(fig:issue).

While it is a valuable device for working with direct collaborators, it is also great to facilitate review [@rougierSustainableComputationalScience2017].
Even if a journal does not support this, it enables post-publication review.
Readers of your manuscript can ask questions or point out how to improve it after you have published it.

### GitHub Pull Requests

<!--Hands on: Introduce GitHub PR and review-->

Pull requests allow anyone to contribute to a project, even though they do not have write access, while the authors still retain all control.
If someone wants to contribute, but they have no permission to write to a repository, they can create a copy of the project, called "fork", where they can push their changes.
For example if you found an error in this manuscript, you can not simply change it, because you have no write access.
However, you can create a fork in your account, where you have write permission:

```r
usethis::create_from_github("aaronpeikert/repro-tutorial", fork = TRUE)
```
In this copy you then make any changes you want.
You could then request that we (or anyone with write permissions) "pull" these changes.
You can view new pull request for any project with:

```r
usethis::browse_github_pulls()
```

Before incorporating changes from someone else, they can be reviewed and adepted.
This review step can also be useful for collaboration, because you can invite a collaborator to take a look at changes you want to do.
It is therefore sometimes useful to create a pull request even though you have permission to push, because it invites your collaborators to comment or change your proposal.

You can initiate a new pull request with:

```r
usethis::pr_init("myrevision")
```

Then you make and commit your changes and open a new pull request with:

```r
usethis::pr_push()
```

After you or someone else has merged the pull request on GitHub, you can wrap it up with:

```r
usethis::pr_finish()
```

This process is especially useful if you want the feedback of a more senior researcher, because they do not need to know how to use Git but have a convenient web interface to provide comments or changes.

### GitHub Project Management

<!--Hands on: Introduce GitHub Projects-->
<!--I love it, but maybe we should remove it-->
While issues and pull requests already provide much structure to organize your work, GitHub projects allow you to categorize and prioritize issues and pull request, while informing you on the progress you made.

# Analyzing Data

While you already know how to write dynamic documents with [Markdown] and [R Markdown], and track changes with [Git] and [GitHub], two components to archive long-term reproducibility are still missing, namely internal and external dependencies.

## Internal Dependencies

<!--I am not sure this is the right place, should we introduce this earlier? -->

A reader of your reproducible manuscript should be able to derive how they are to reproduce it (ideally by a statement in your [README]).
To do that, it is often crucial to understand how different project components relate to each other.
You cannot run an analysis without data; you cannot render a dynamic document without an analysis, and so forth.
Dependency tracking means that you capture the interrelations between the components of your analysis.
The "components" of an analysis are often loosely mapped onto files.
For a simple project, it might be enough to have a single R Markdown file.
However, while it is easy to understand how to reproduce the end result, this approach quickly gets unwieldy.
To better navigate the data analysis, a researcher often splits it across several scripts.
For example, they create a script to load the data and another to run the analysis.
This modularisation facilitates reuse because the loading script may be used by other projects building on the same data, while the analysis script may be employed on different data and so forth.
The downside is that reproducibility is complicated because instead of reproducing everything in one step, multiple steps are required.

Of course, there a several ways to solve this problem.
One could build an R package to separate analysis and data or use code externalization in the RMarkdown to facilitating reuse or create a "master" script that combines all steps.
We found Make to be particularly useful for two reasons.
First, it is language agnostic, therefore fusing steps that are carried out in different programming languages.
Second, it does implement a simple yet powerful caching mechanism of results which is welcome in computing-intensive analysis often found in neuroimaging or machine learning.

Make is build around the concept of computational recipes.
Like cooking recipes, the dish you want to create is at the top, followed by the necessary ingredients with the procedure at last.
In Make the dish is usually a file and is called "target".
The ingredients are called "requirements" and are usually also files.
The procedures are formulated as shell commands.
One (hypothetical) example is this:

```
spaghetti_arrabiata.pdf: spaghetti_arrabiata.Rmd arrabiata_sauce.csv pasta.csv
  Rscript -e 'rmarkdown::render("spagetti_arrabiata.Rmd")'
```

An important property is that each requirement can be itself be a target, like here:

```
spaghetti_arrabiata.pdf: spagetti_arrabiata.Rmd arrabiata_sauce.csv pasta.csv
  Rscript -e 'rmarkdown::render("spaghetti_arrabiata.Rmd")'

pasta.csv: cook_pasta.R
  Rscript -e 'source("cook_pasta.R")'

arrabiata_sauce.csv: cook_sauce.R canned_tomatoes.csv
  Rscript -e 'source("cook_sauce.R")'
```

When you cook something, and you have the necessary ingredients already at hand, would you throw it away just to prepare it again?
Unlikely.
Make behaves the same.
If a file already exists, it does not recreate it unnecessarily.
However, if a requirement is newer than its target, make recreates the target to reflect the updated dependency.
Therefore, when you update the data file, the analysis is rerun.
Because Make knows the whole dependency tree, its caching is smarter than many alternatives.

One alternative that is even smarter than Make is the R package [`targets` [@targets2021]](https://books.ropensci.org/targets/)[^targets].
`targets` can infer more about your project than Make, because it analysis the R code you write to build the dependency tree.
For this inference to work well, you have to write everything in R, and you have to write everything strictly functional.

[^targets]: https://books.ropensci.org/targets/

<!--Hands on: Introduce Make-->
<!--Hands on: Add targets for README.Rmd, preregistration.Rmd -->

The `repro` package can automatically write Makefiles for simple scenarios.
For this purpose, `repro` utilizes the YAML metadata:

```
---
title: "Preregistration"
author: "Aaron Peikert"
date: "4/12/2021"
output: html_document
repro:
  scripts:
    - R/simulation.R
---
```

This command will go through all R Markdown files, and their YAML metadata, in your repository and infer a `Makefile` from them:

```r
automate_make()
```

The generated `Makefile` automatically contains targets for all R Markdown documents.
To, for example, generate the `README.md` from the `README.Rmd` you can run:

```bash
# this should be run in the terminal
make README.md
```

It is common practice to have a target all, that encompasses all important files of the project as the first target.
If you do not specify a target, Make defaults to the first target, which is usually `all`.
`repro` reminds you of that and suggests which file you might want to add to the target `all`.
After you have done so, you can then run in the terminal:

```bash
# this should be run in the terminal
make
```

And it will fully reproduce the project, if you have all required software installed.

## External Dependencies

Modern data analysis is only feasible because we rely on much external software.
One downside is that a considerable number of external dependencies may hinder collaboration and long term reproducibility.
The critical problem is the fragility of a software setup.
Many components have to fit perfectly together, and each change, e.g. an update, risks that balance.
Updates are, of course, very much necessary.
Another problem is that the software environment is also variable and what works on one machine might not work on any other.
Docker circumvents such issues.
Instead of leaving it to the user which software should be installed how on their computer, Docker provides all software used in an analysis, down to the operating system.
Docker does this without interfering with the already installed software by using a virtual software environment that is independent of the host software environment.
Such a snapshot of the software stack is called "image".
Of course, packaging all needed software in such an image requires considerable amounts of storage.
However, two strategies help to keep the storage requirements manageable.
First, there is a community that maintains images for particular purposes.
For example, using R [rocker-citation].
Such images only contain what is necessary for a specific purpose.
Users can then install whatever they additionally need on top of these purpose-built images.
The image that was used for this article uses `r round(as.numeric(fs::file_size("reprotutorial.sif"))/1024^3, 2)`Gb of disc space.
It does include Ubuntu (a Linux operating system) R, RStudio, LaTeX as well as many R packages like the tidyverse (amounting to `r installed.packages() %>% as.data.frame() %>% filter(is.na(Priority)) %>% nrow()` packages.)
Second, instead of saving a binary image of the software, one can also save a textual description of the programmatic steps to install the software in a `Dockerfile`.
Such files are tiny (the `r link("https://github.com/aaronpeikert/repro-tutorial/blob/master/Dockerfile", "Dockerfile")` for this project is `r round(as.numeric(fs::file_size("Dockerfile"))/1024^1, 2)`Kb big), but they rely on the assumption that all software repositories are available and contain the correct software version.
For archiving, you therefore have to store the image, as explained in [Archiving Docker].
The `repro`-package lets you specify required packages as part of the metadata of the RMarkdown:

```
---
title: "Preregistration"
author: "Aaron Peikert"
date: "4/12/2021"
output: html_document
repro:
  scripts:
    - R/simulation.R
  packages:
    - tidyverse
    - here
    - lavaan
    - furrr
---
```

With the R version that you currently use, this specification is then translated into a `Dockerfile`:

<!-- Hands on: Introduce Docker -->

```r
automate_docker()
```

From the `Dockerfile` you can then build an image.
While you can do so manually we prefer to use Make, see [Internal Dependencies].
The `Makefile` build by `repro` does contain a recipe for creating the `Dockerfile`.

```bash
make docker
```

The `Makefile` from `repro` does also include a mechanism to send commands conditionally through the Docker container.
This mechanism is triggered by `DOCKER=TRUE` in the make call:

```bash
# this should be run in the terminal
make DOCKER=TRUE
```

<!-- after this everything is optional? -->

### renv (alternative)

Docker can be a bit unfriendly, because it requires a complicated mental model and some knowledge about how to install software on Linux.
An R-focused alternative is the R package [`renv`](https://rstudio.github.io/renv/articles/renv.html)[^renv][@R-renv].
`renv` isolates all R packages used in a project and records there version as well as the source from where they where installed.
This mechanism is faster and simpler to use than Docker, at the expense of only tracking R packages.
`renv` therefore is already of great use for reproducibility.
However, even when your project relies solely on R, external dependencies can still change the outcome of computations, like the operating system, the compilers used, the BLAS system in use and so forth.
Most importantly the R version itself has to match.

[^renv]: https://rstudio.github.io/renv/articles/renv.html


### Singularity (optional)

One usecase in which Docker sometimes can not be used is in high performance computing (HPC).
The simulation we did is computationally expensive enough to warrant the use of a HPC cluster.
To run the simulation on a cluster we require the exact same software, not only on one or two computers, but potentially hundreds.
Docker is a natural solution for this problem and is the defacto standard for cloud computing.
However, Docker requires admin rights, while this is no issue on cloud computing infrastructure it is an issue in classical HPC environments.
Singularity was designed to run without admin rights, while maintaining full compatibility with Docker images.
If you already have a Docker image, you can choose if you want to run this image with Docker or with Singularity.
Anyway, from within the container i.e. for your analysis it does not make any difference.
Like with Docker, the default `Makefile` of `repro` has a mechanism to automatically send all commands to Singularity.
It is triggered by `SINGULARITY=TRUE`:

```bash
make SINGULARITY=TRUE
```

<!-- Hands on: add data via automate() -->
<!-- Hands on: automatically download data via manually editing the Makefile -->

## Reproduce a Project

A reproducible project should not only be theoretically reproducible, but it should be simple to reproduce.
Ideally, you have a statement in the README that contains a few steps to reproduce the project, where most steps are automatic and standardized.
This statement should also note the required software.
E.g. you can download any git repository with these commands:

```bash
# this should be run in the terminal
git clone https://github.com/aaronpeikert/repro-tutorial.git
cd repro-tutorial
```

Projects that combine Make and Docker then can usually be reproduced with:

```bash
# this should be run in the terminal
make docker &&
make -B DOCKER=TRUE 
```

<!-- maybe we add here the real data? #25 -->

## If you can not make the data public (optional)

<!-- Hands-on: check changed hash -->

To reproduce an analysis, you need to have access to the data.
There are several reasons which may prohibit you from making your data available, like legal or regulatory or privacy concerns.
Often you find yourself in a situation where you can make the data only availabe on request.
A possibility for irreproducibility is that the data has changed in the meantime.
To at least check that the data was not changed you can provide a so called "hash".
This is a cryptographic fingerprint.
If the data changes, so does its fingerprint or hash.
The user can therefore compare if the data they have received is the same that was used in the original publication, as long as the original publication does contain the hash of the data.

To verify the data integrity you can include a line of code after reading the data that looks like this:

```{r}
# create a dummy data.frame with two columns
x <- data.frame(VAR1 = c(1, 2, 3, 4), VAR2 = c(0, 4, 6, 9))
# compute hash using md5
hash <- digest::digest(x, "md5")
if(hash != "5ba412f5a26f43842971dd74954fcdeb"){
  warning("Mismatch between original and current data!\nHash now is:\n    '", hash, "'")
}
```

We employed the same mechanism for our simulation to verify that the simulation is really reproducible[^simhash].

[^simhash]: https://github.com/aaronpeikert/repro-tutorial/blob/master/R/hash.R

An compromise between making the data publicly available and keeping it closed is to publish synthetic data.
Such approach tries to recreate the marginal distribution and bivariate relationships in simulated data, e.g. with the R package `synthpop`[@nowokSynthpopBespokeCreation2016].
The results of such data should generally not be interpreted.
However, synthetic data provides an opportunity for readers to test if a project is in principle reproducible, even though they have no access to the data. 

# Archiving and Dissemination

The open nature of the here proposed workflow does blur the lines between published and unpublished.
However, even when your Git repository is public, you may want to signify important stages of your project, like preregistration, preprint, submission and acceptance to the journal.
These milestones deserve to be archived.
A project is only then properly archived when it is stored at least at two independent storage locations and reproducible from all locations.
Therefore, you should make extra sure that the project at these points is reproducible.
To ensure reproducible, you could ask a coworker or collaborator to reproduce the project independently from you.
The sections [Creating a DOI] and [Creating a Release] describe how you can easily create three different openly accessible longterm archived copies of your project.

When you disseminate the paper across more locations than just GitHub it can be ambiguous which version of code and data have created the document (e.g. the manuscript.pdf).
To create a clear link between the repository version and some end result you can automatically include the hash of the commit.
Either you use Git from within R:

```{r}
system2("git", c("rev-parse",  "--short",  "HEAD"), stdout = TRUE)
```

Or you can use the `gert` package:

```{r}
strtrim(gert::git_commit_id(), 7)
```

## Creating a DOI

<!-- public releases like preregistration, preprint -->
zenodo.org [@zenodo] is a publicly funded service provider that archives digital artifacts for research and provides digital object identifiers (DOI) for these archives.
While the service is independent of GitHub---in terms of storage facilities and financing---you can link GitHub and zenodo.org.
Please note that you can only link public GitHub repositories to zenodo.
You may log into zenodo.org through your GitHub account.

> Log into zenodo.org → Account → [GitHub[^zenodo-github]](https://zenodo.org/account/settings/github/)

[^zenodo-github]: https://zenodo.org/account/settings/github/

After you have linked a GitHub repository you trigger the archivation by creating a GitHub release, as described in the next section.

## Creating a Release

GitHub has a "release" function, which creates a link to a specific version of your project, where everything is available as a zip file.
You have the opportunity to describe the release in the release notes and to upload other additional files (e.g. the rendered pdf).

## Archiving Docker

## Archiving Data

<!-- Hands on: add commit hash to results-->
<!-- Hands on: git tag + github release-->
<!-- Hands on: explain how to archive data-->
<!-- Hands on: explain how to archive docker images-->

# Checklist

<!-- Should we design a checklist? -->

```{r, results='asis', echo=FALSE}
link_index()
```

# References

```{r, include=FALSE}
knitr::write_bib(c(.packages(), "bookdown", "lavaan", "knitr", "targets", "renv"),
                 "packages.bib")
```

