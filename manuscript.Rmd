---
title: A Hitchhiker's Guide to Reproducible Research in R
author:
  - name: Aaron Peikert
    affil: 1, *
    orcid: 0000-0001-7813-818X
  - name: Caspar J. Van Lissa
    affil: 2, 3
    orcid: 0000-0002-0808-5024
  - name: Andreas M. Brandmaier
    affil: 1, 4
    orcid: 0000-0001-8765-6982
affiliation:
  - num: 1
    address: |
      Center for Lifespan Psychology---Max Planck Institute for Human Development
      Lentzeallee 94, 14195 Berlin, Germany
  - num: 2
    address: |
      Department of Methodology & Statistics---Utrecht University faculty of Social and Behavioral Sciences, Utrecht, Netherlands
  - num: 3
    address: |
      Open Science Community Utrecht, Utrecht, Netherlands
  - num: 4
    address: |
      Max Planck UCL Centre for Computational Psychiatry and Ageing Research
      Berlin, Germany and London, UK
# firstnote to eighthnote
correspondence: |
  peikert@mpib-berlin.mpg.de
journal: psych
type: tutorial
status: submit
bibliography: temp.bib
simplesummary: |
  Reproducibility has long been considered integral to the scientific method.
  Something is called reproducible when independend researcher arrive at the same results from the same data.
  Until recently, detailed descriptions of methods and analyses were the primary instrument for ensuring scientific reproducibility.
  Technological advancements now enable scientists to achieve a much higher standard;
  one in which any individual can be granted access to a digital research repository, and reproduce the analyses from raw data to final manuscript with a single command.
  This method has far-reaching implications for scientific archiving,
  reproducibility and replication,
  scientific productivity,
  and the credibility and reliability of scientific findings.
  One obstacle preventing the widespread adoption of this method is that the underlying technological advancements
  are complicated to use.
  This paper introduces `repro`; an R-package that guides researchers in the installation and use of the tools required for computational reproducibility.
  It includes a tutorial on how to make a research project reproducible with the aid of `repro`.
  Finally, it introduces novel applications of reproducibility tools for scientific research;
  including the preregistration of study plans as reproducible computer code.
  This circumvents the shortcomings of ambiguous preregistrations that allow for researcher degrees of freedom,
  because computer code describes study plans more precisely than prose.
  Reproducibility made convenient with automation has hence a wide range of applications to accelerate scientific progress.
abstract: "`r tryCatch(trimws(readr::read_file(here::here('abstract.Rmd'))))`"
keywords: |
  keyword 1; keyword 2; keyword 3 (list three to ten pertinent keywords specific 
  to the article, yet reasonably common within the subject discipline.).
acknowledgement: |
  The authors received no financial support for the research, authorship, and/or publication of this article.
authorcontributions: |
  Aaron Peikert took the lead in writing and provided the initial draft of the manuscript.
  Andreas Brandmaier and Caspar J. Van Lissa contributed further ideas, critical feedback, and revisions of the original manuscript.
  Furthermore, we would like to thank Maximilian Stefan Ernst (not an author) for his contributions to the code for the simulation study.
conflictsofinterest: |
  The authors declare no conflict of interest.
abbreviations:
  - short: PAC
    long: Preregistration as 
  - short: Gb
    long: Gigabyte
  - short: Kb
    long: Kilobyte
  - short: CRAN
    long: Comprehensive R Archive Network
repro:
  packages:
    - tidyverse
    - usethis
    - gert
    - aaronpeikert/repro@c3cae1f
    - here
    - rstudio/webshot2@f62e743
    - targets
    - renv
    - slider
    - patchwork
    - knitr
    - pander
    - lavaan
    - furrr
    - future.batchtools
    - rticles
    - moments
    - report
  scripts:
    - R/simulation.R
    - R/simulation_funs.R
    - R/link.R
  data:
    - data/simulation_results.csv
  images:
    - images/nutshell.svg
  files:
    - references.bib
output:
  rticles::mdpi_article
header-includes:
   - \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}, numbers=left}
---

```{r setup, include=FALSE}
library(repro)
automate_load_packages()
automate_load_scripts()
source(here::here("R", "link.R"))
```

```{r echo=FALSE}
# define a print method for objects of the class data.frame
knit_print.data.frame = function(x, ...) {
  res = paste(c('', '', kable(x)), collapse = '\n')
  asis_output(res)
}
# register the method
registerS3method("knit_print", "data.frame", knit_print.data.frame)
```

# Introduction

Scientists increasingly strive to make research data, materials, and analysis code openly available.
Sharing these types of digital research products has the potential to increase scientific efficiency by enabling researchers to learn from each other and reuse materials, and to increase scientific reliability by facilitating the review and replication of published research results.
To some extent these potential benefits are contingent on whether these digital research products are reproducible. 
Reproducibility can be defined as the ability of anyone to obtain identical results from the *same* data with the *same* computer code [see @Peikert2019 for details].
Reproducibility has long been considered integral to empirical research, whose credibility hinges on its objectivity, meaning "in principle it can be tested and understood by anybody." [@popperLogicScientificDiscovery2002, p. 22][^popper]
Unfortunately, despite increasing commitment to open science practices and good will, many projects can not yet be reproduced by other research teams [@obels2020].
This is because making a research project with all code, data, and materials reproducible comes with a variety of challenges, and there is a dire need for reproducibility standards.
We now have the technology to make all digital products related to a research project available so that a research project can be reproduced with minimal effort.

This tutorial is aimed at researchers who intend to ensure the reproducibility of a research project and are willing to make relevant code, data, and materials available, whether publicly or on request.
We pursue two aims.
First, the present paper introduces the reader to a workflow that maximizes the potential reproducibility of a research project.
This workflow builds on several technical solutions originating in the software engineering community. 
We discuss how these solution address four common threats to reproducibility [see @Peikert2019 for details] and form a coherent workflow.
We further demonstrate how the R package `repro` supports researchers in adopting this workflow.
The second objective is to discuss opportunities of reproducible workflows to improve not only how research outputs are created and recreated, but also how research itself is conducted.
One such opportunity is a strictly reproducible and unambiguous form of preregistration [@NosekRevolution2018] that builds upon a reproducible workflow, the so-called *preregistration as code* (PAC).
PAC involves preregistering the intended analysis code and the bulk of the manuscript as dynamic document, including the Introduction, Methods, and Results sections.
The resulting dynamic document closely resembles the final manuscript, but uses preliminary ("mock") results, based on the analysis of simulated data, as a placeholder for figures, tables, and statistics. 
After the empirical data are analyzed, the Results are updated, and the Discussion is written.

[^popper]: In fact, @popperLogicScientificDiscovery2002's famous criterion of demarcation is built around "inter-subjective testing", a concept which he later generalized to inter-subjective criticism (Footnote *1, p. 22). Somewhat confusingly, @popperLogicScientificDiscovery2002 uses the term "Reproducibility" for what we would call "Replication".

Scientific organizations and funding bodies increasingly demand transparent sharing of digital research products, and researchers are increasingly willing to do so.
However, although the sharing of such digital research products is a necessary condition for reproducibility, it is not a sufficient one.
This was illustrated by an attempt to reproduce results from open materials in the journal *Cognition* [@hardwicke2018]. 
Out of 35 published articles with open code and data, results of 22 articles could be reproduced but in 11 of these cases, further assistance from the original authors was required.
For 13 articles, at least one outcome could not be reproducedâ€”--even with the original authors' assistance.
Another study of 62 Registered Reports found that only 41 had data available, and 37 had analysis scripts available [@obels2020].
The authors could execute only 31 of the scripts without error and reproduce the results of only 21 articles (within a reasonable time).
These failed attempts to reproduce highlight the need for widely accepted reproducibility standards, because open repositories do not routinely provide sufficient information to reproduce relevant computational and statistical results.
If digital research products are available but not reproducible, their added value is limited.

This tutorial therefore demonstrates how to make R-based research projects reproducible, while striking a balance between rigor and ease-of-use.
A rigorous standard increases the likelihood that a project will remain reproducible as long as possible.
An easy-to-use standard, on the other hand, is more likely to be adopted.
Our approach is to promote broad adoption of easy-to-use practices, i.e., a "low threshold", and ensure that these practices are compatible with more complex rigorous solutions, i.e., a "high ceiling".
As researchers become more proficient with the tools involved, they can thus improve the reproducibility of their work.
The first part of this tutorial outlines a reproducibility standard, and introduces the underlying theoretical concepts and software solutions.
The second part is a tutorial that demonstrates how this standard can be easily adopted with the help of the R package `repro`.

We have structured the tutorial with a *learning by doing* approach in mind, such that readers can follow along on their own computers.
We explicitly encourage readers to try out all R commands for themselves.
Unless stated otherwise, all code blocks are meant to be run in the statistical programming language R (tested with version `r with(R.version, paste0(major, ".", minor))`).

# Concepts and Software Solutions<!--CJ Some inconsistencies:
1) From the end of the previous paragraph, I would expect a description of the "rigorous reproducibility standard" here.
2) The paragraph is labeled "Concepts and Software SOlutions, but it begins with a list of threats to reproducibility. Threats are not concepts nor software solutions.
3) After the list of threats, we get a description of the "rigorous reproducibility standard". That is what I would expect based on the preceding paragraph. Note that the "rigorous reproducibility standard" is also not a concept, nor a software solution. Consider re-naming this section.-->

From our own experience with various research projects, we identified the following common threats to reproducibility: <!-- CJ This still requires attention! How did you identify these causes? Is this exhaustive?-->

1.	**Multiple inconsistent versions of code, data, or both**; for example, the data set could have changed over time because outliers were removed at a later stage or an item was later recoded; or the analysis code could have been modified during writing of a paper because a bug was removed at some point in time; it may then be unclear, which version of code and data was used to produce some reported set of results.
2.	**Copy-and-paste errors**; for example, often, results are manually copied from a statistical computing language into a text processor; if a given analysis is re-run and results are manually updated in the text processor, this may inadvertently lead to inconsistencies between the reported result and the reproduced result;
3.	**Undocumented or ambiguous order of code execution**; for example, with multiple data and code files, it may be unclear which scripts should be executed in what order;  or, some of the computational steps are documented (e.g., final analysis), but other steps were conducted manually and not documented (e.g., running a scripts/commands by hand; copy-pasting results from one program to another)
4.	**Ambiguous dependencies**; for example, a given analysis may depend on a specific version of a specific software package, or rely on software that might not be available on a different computer or no longer exist at all; or, a different version of the same software may produce different results.
<!-- CJ 5. Missing steps: Some of the steps are documented (e.g., final analysis), but other steps were conducted manually and not documented (e.g., copy-pasting results from one program to another; reverse coding items on the fly).-->
<!--AB Caspar's 5th point is important. In my mind, this best fits to point #3, which adressed an insufficient level of coding/automation/documentation of the necessary steps taken. Can we merge this into one new point #3? I will commit a suggested change of point #3 here (see above);.-->

We have developed a workflow that leverages established tools and practices from software engineering to achieve long-term and cross-platform computational reproducibility of scientific data analyses resting on four pillars that address the aforementioned causes of non-reproducibility [@Peikert2019]: <!-- CJ In fact, this is the central message of your paper: "We have developed this new workflow, based on four pillars." I would suggest mentioning this very clearly in the opening paragraph. The opening paragraph should at least:
1) Introduce the topic area
2) Introduce the problem/knowledge gap
3) Introduce how you will fill this knowledge gap-->

1. Version control
2. Dynamic document generation
3. Dependency tracking
4. Software management

The remainder of this section briefly explains why each of these four building blocks is needed and their role in ensuring reproducibility.

**Version control** prevents the ambiguity that arises when multiple versions of code and data are created in parallel during the development phase of a research project.
Version control allows a clear link between which results were generated by which version of code and data.
We recommend using **Git** for version control, because of its widespread adoption in the R community.

Git tracks changes to all project-related files (e.g., materials, data, and code) over time.
At any stage, individual files or the entire project can be compared to, or reverted to, an earlier version.
A version-controlled project makes the loss of files unlikely and also supports remote collaboration.
Git is built around snapshots that represent the project state at a given point in time.
Those snapshots are called "commits" and work like a "save" action.
Changes to files are collected in a _commit_.
Ideally, each commit has a message that succinctly describes these changes.
It is good practice to make commits for concrete milestones, e.g., "Commented on Introduction", "Added SES as a covariate", "Address Reviewer 2's comment 3".
This makes it easier to revert specific changes than when multiple milestones are joined in one commit, e.g., "Changes made on 19/07/2021".
Each commit "knows" its ancestor, and they form a timeline together.
The entirety of commits (i.e. the version-controlled project) is called a repository.
In Git, specific snapshots of a repository can be tagged, such that one can clearly label which version of the project was used to create a preregistration, preprint, or final version of the manuscript as accepted by a journal.
Git has additional features beyond basic version control, such as branches to facilitate collaboration.
@vuorreCuratingResearchAssets2018 provide a more extensive treatment how Git functions and how to use Git for research.
@bryanExcuseMeYou2018 provides additional information on how to track R Markdown documents.
Collaborating via Git requires the repository to be uploaded somewhere.
We recommend GitHub to host Git repositories because of its popularity among R users.
GitHub has many tools that ease project management and collaboration, and these tools provide much value in our everyday work, but they are not central to achieving reproducibility.

Second, we rely on **dynamic document generation**.
The traditional way of writing a scientific report based on a statistical data analysis uses two separate steps conducted in two different programs.
In a word processor, the researcher writes the text, and in another software, they conduct the analysis.
Typically, results are manually copyied and pasted from one software to the other, a process that often produces inconsistencies [@nuijtenPrevalenceStatisticalReporting2016].

Dynamic document generation integrates both steps.
Through dynamic document generation, code becomes an integral, although usually hidden, part of the manuscript, complementing the verbal description and allows interested readers deeper understanding [@knuthCWEBSystemStructured; @claerboutElectronicDocumentsGive1992].
**R Markdown** uses Markdown for writing the text and R (or other programming languages) for the analysis.
Markdown is a lightweight text format in plain text with a minimal set of reserved symbols for formatting instructions.
This way, Markdown does not need any specialized software for editing.
It is highly fool-proof [unlike, for example, LaTeX @lamportLATEXDocumentPreparation1994], works well with version control systems, and can be ported to various document formats, such as HTML Websites, a Microsoft Word document, a typeset PDF file (for example, via LaTeX journal templates), or a Powerpoint presentation.
We suggest using Markdown for all sorts of documents that are created in the academic context, starting from simple sketches of your ideas to your scientific manuscripts[@R-rticles], and presentations[@revealjs] or even your rÃ©sumÃ© [@vitae].
R Markdown extends regular Markdown by allowing users to include R code chunks [in fact, arbitrary computer code @riedererChapter15Other] into a Markdown document.
Upon rendering the document, the code blocks are executed, and their output is dynamically inserted into the document.
This allows the creation of (conditionally) formatted text, statistical results, or figures that are guaranteed to be up-to-date because they are created every time anew as the document is rendered to its output format (e.g., presentation slides or a journal article).
@xieMarkdownCookbook2020 provides an extensive yet practical introduction to most features of R Markdown.

One would hope that sharing a dynamic document would allow individuals (including one's future self) to simply download and reproduce a given analysis.
However, while version control and dynamic document generation are becoming more common, this approach fails to guarantee reproducibility in many instances [@Peikert2019].
In practice, dependencies between project files (for example, the information what script uses which data file and what script needs to be run first) or on external software (such as system libraries or components of the programming language, such as other R packages) are frequently unmentioned or not exhaustively and unambigously documented.

To automatically resolve dependencies between project files, we rely on **dependency tracking**.
In essence, researchers provide a collection of computational recipes that describe how ingredients are processed to create intermediate products and, in the end, a final product.
Similar to a collection of cooking recipes, we can have multiple products (_targets_) with different ingredients (_requirements_) and different steps of preparation (_recipes_).
In the context of scientific data analysis, the targets are typically the final scientific report (e.g., the one to be submitted to a journal) and possibly intermediate results (such as preprocessed data files, simulation results, analysis results).

We recommend using **Make** for dependency tracking because it is language independent.
The following hypothesical example illustrates the utility of Make.
Consider a research project that contains a script to simulate data (`simulate.R`) and a scientific report of the simulation results written in R Markdown (`manuscript.Rmd`).
A Makefile for this project could look like this:

```{bash, eval=FALSE}
manuscript.pdf: manuscript.Rmd simulated_data.csv
  Rscript -e 'rmarkdown::render("manuscript.Rmd")'

simulated_data.csv: simulate.R
  Rscript -e 'source("simulate.R")'
```

There are two targets, the final rendered report (`manuscript.pdf`, l.1) and the simulation results (`simulation_results.csv`, l.4).
Each target is followed by a colon and a list of requirements.
If a requirement is newer than the target, the recipe will be executed to rebuild the target.
If a requirement does not exist, Make uses a recipe to build the requirement first before building the target.
Here, if one were to build the final `manuscript.pdf` by rendering the R Markdown with the command shown in l.2, Make would check whether the file `simulation_results.csv` exists; if not, it would issue the command in l.5 to run the simulation first before rendering the manuscript.
This ensures that the simulation is always run before the manuscript is built, and that the manuscript is rebuilt if the simulation code was changed.
Make therefore offers a standardized process to reproduce projects, regardless of the complexity or configuration of the project.

```{r, include=FALSE}
container_size <- as.numeric(fs::file_size("reprotutorial.sif"))/1024^3
```

A version controlled dynamic document with dependency tracking may still rely on external software.
Troubleshooting issues specific to a particular programming language or dependent tool typically requires considerable expertise and pose a threat to reproducibility.
One solution to avoid problems caused by missing or inconsistent external software dependencies is to provide not only the analysis script but all dependent software packages and system libraries via **software management**.
One comprehensive approach to software management is containerization.
"packaging the key elements of the computational  environment  needed  to  run  the  desired  software [makes] the software much easier to use, and the results easier to reproduce" [@silverSoftwareSimplified2017, p. 174].

**Docker** is an example of containerization.
It provides all software used in an analysis, including the operating system itself.
The operating system level is important because some functionality may be passed to software layers beneath the programming language, such as calls to random number generators, linear algebra libraries and such.
Docker does this without interfering with the already installed software by using a virtual software environment independent of the host software environment.
Researchers therefore have the opportunity to use the exact same software setup that was used by the original authors.
Such a snapshot of the software stack is called an "image".
Packaging all required software in such an image requires considerable amounts of storage space.
Two major strategies help to keep the storage requirements reasonable.
One strategy is to rely on pre-made images, which are maintained by a community for particular purposes.
For example, there are pre-made images that only include what is necessary for R, based on Ubuntu (a Linux operating system) containers [@boettigerIntroductionRockerDocker2017].
Users can then install whatever they need in addition to what is provided by these pre-compiled images.
`r ifelse(is.na(container_size), "", glue::glue("The image that was used for this article uses {round(container_size, 2)}Gb of disk space."))`
The image for this project includes Ubuntu, R, R Studio, LaTeX as well as a variety of R packages like the tidyverse [@tidyverse] and all their dependent packages, amounting to `r installed.packages() %>% as.data.frame() %>% filter(is.na(Priority)) %>% nrow()` R packages.
A second strategy is to save a container recipe; a so-called  `Dockerfile` containing a textual description of all commands that need to be executed to recreate the software environment, instead of the software environment itself.
Dockerfiles are tiny (the [Dockerfile](https://github.com/aaronpeikert/repro-tutorial/blob/main/Dockerfile) for this project has a size of only `r round(as.numeric(fs::file_size("Dockerfile"))/1024^1, 2)`Kb).
However, they rely on the assumption that all software repositories that provide the dependent operating systems, pieces of software, and R packages will remain accessible in future and will continue to make available historic software versions.
For proper archival, we therefore recommend to store a complete image of the software environment, in addition to the Dockerfile.
A more comprehensive overview of the use of containarization in research projects is given by @wiebelsLeveragingContainersReproducible2021.

To summarize, the workflow by @Peikert2019 requires four components (see Figure \@ref(fig:schematic)), dynamic document generation (using R Markdown), version control (using Git), internal dependency management (using Make), and containerization (using Docker).    
While R Markdown and Git are well integrated into the R environment through R Studio, Make and Docker require a level of expertise that is often beyond the training of scholars outside the field of information technology, which can represent a considerable hurdle in the acceptance and implementation of the workflow.
To remove this hurdle, we have developed the R package `repro` that supports scholars in setting up, maintaining, and reproducing research projects in R.
In the remainder, we will walk you through the creation of a reproducible research project with the package `repro`.

```{r schematic, eval = TRUE, echo = FALSE, fig.cap="Schematic illustration of the interplay of the four components (in dashed boxes) central to the reproducible workflow: version control (Git), dependency tracking (Make), software management (Docker), and dynamic document generation (R Markdown). Git tracks changes to the project over time. Make manages dependencies among the files. Docker provides a container in which the final report is built using dynamic document generation in R Markdown. Reproduced from @Peikert2019.  "}
# file gets downloaded in Makefile
knitr::include_graphics(here::here("images/nutshell.svg"), auto_pdf = TRUE)
```

# Creating reproducible research projects

One impediment to the widespread adoption of a standard for reproducible research is that learning to use the required tools can be time-intensive.
To lower the threshold to adoption, the R-package `repro` introduces helper functions that wrap more complicated (and powerful) tools.
These so-called wrappers guide end-users in the use of reproducibility tools, provide feedback about what the computer does, and suggest what the user should do in layman's terms.
We hope this makes reproducibility tools more accessible by enabling beginner-level users to detect their system's state accurately and act correspondingly [@parasuramanAutomationHumanPerformance2018, Chapter 8: "Automation and Situation Awareness"].
These wrappers are merely an assistance system, and as users get more and more comfortable, they can use the underlying tools directly to solve more complex problems.

This tutorial assumes that the user is working predominantly in R, with the help of RStudio.
It describes basic steps that we expect to apply to all users. Your specific situation might involve additional, more specialized steps. After completing the tutorial, you should be able to customize your workflow accordingly.
The first step is to install the required software.
Start R Studio<!--CJ I am concerned that new users may not install R if you only mention RStudio.--> and install the package [`repro`](https://github.com/aaronpeikert)[@R-repro]. 
It will assist you while you follow the tutorial.

```{r, eval=FALSE}
# repro is not on CRAN yet
options(
  repos = c(aaronpeikert = 'https://aaronpeikert.r-universe.dev',
            CRAN = 'https://cloud.r-project.org')
)
install.packages('repro')
```

An example of the assistance that `repro` provides are the check functions.
These verify that you indeed have installed and setup the required pieces of software for this workflow (here, we use the double-colon operator to access functions from the `repro` namespace; we chose to do so to make clear which functions are native `repro` functions) :

```{r}
# `package::function()` â†’ use function from package without `library(package)`
repro::check_git()
repro::check_make()
repro::check_docker()
```

These functions check whether specific dependencies are available on the users' system, and if not, explain what further action is needed to obtain it.
Sometimes they ask the user to do something, for example, the following happens if you were a Windows user who does not have Git installed:

```{r, include=FALSE}
opts <- options()
options(repro.os = "windows",
        repro.git = FALSE)
```

```{r}
check_git()
```

```{r, include=FALSE}
options(opts)
```

The messages from `repro` try to take the user by the hand to help them solve problems.
These messages are adjusted to your specific operating system and installed dependencies.
Before you continue, we ask you to run the above commands to check Git, Make, and Docker - both to become familiar with the functionality of the `check_*()` functions, and to prepare your system for creating reproducible projects.
If necessary, follow any instructions presented until all checks pass.


## Implementing version control

Now that your system is set up for this tutorial, we will introduce you to version control with Git.
Git tracks files within one project folder.
We start by creating such a project folder with RStudio by clicking the menu item:

> File â†’ New Project... â†’ New Directory â†’ Example Repro Template

This creates a project containing an example analysis, but you may use any other template or existing R Project to turn into a reproducible research project using the functionality of `repro`.

Git tracks any files that you manually add to the repository.
To make sure you do not accidentally add files that you do not wish to share (e.g., privacy sensitive data),
you can list specific files that you do not want to track in the `.gitignore` file.
You can add something to the `.gitignore` file directly or with this command:

```{r, eval=FALSE}
usethis::use_git_ignore("private.md")
```
Now the file `private.md` will not be committed to Git and will not be made public.

Especially for new users, it is recommended to follow this procedure and exclude sensitive files before proceeding.
When you are ready, you can begin tracking your files using Git by running:

```{r, eval=FALSE}
usethis::use_git()
```

For Git to recognize changes to a given file, you have to stage and then commit these changes (this is the basic save action for a project snapshot).
One way to do this is through the visual user interface in the RStudio Git pane.
Click on the empty box next to the file you want to stage. A check mark appears to indicate that the file is staged.
After you have staged all files you want, click on the commit button, explain in the commit message why you made those changes, and then click on commit.
This stores a snapshot of the current state of the project.

The files you created and the changes you made still have not left your computer.
All snapshots are stored in a local repository in your project folder.
To back up and/or share the files online, you can push your local repository to a remote repository.
While you can choose any Git service (like GitLab or BitBucket), we will use GitHub in this tutorial.
Before you upload your project to Git, you need to decide if you would like the project publically accessable (viewable by anyone, editable by selected collaborators) or if you want to keep it private (only viewable and editable by selected collaborators).
To upload the project publically to GitHub use:

```{r, eval=FALSE}
usethis::use_github()
```

To upload it privately:

```{r, eval=FALSE}
usethis::use_github(private = TRUE)
```

Depending on your computer configuration, this may ask to set up a secure connection to GitHub. In this case first follow the suggestions shown on the R console.

## Dynamic document generation

Now that you have created a version controlled project, we will proceed with dynamic document generation.
A dynamic document has three elements:

1. Text in natural language (prose; e.g., a scientific paper or presentation)
2. Executable code (e.g., analyses)
3. Metadata (e.g. title, authors, document format)

R Markdown is a type of dynamic document well-suited to the RStudio user interface.
The natural language text in an R Markdown file can be formatted using Markdown (see @xieMarkdownDefinitiveGuide2019 for technical details and @xieMarkdownCookbook2020 for practical guidance).
The code mostly consists of R-code (although other programming languages are supported, like Python, C++, Fortran, etc).
The following example serves to illustrate the Markdown syntax. 
It shows how to create a heading, a word in bold font, and a list of several items in Markdown:

```markdown
<!--this is a Markdown file -->
# Heading (level 1)

Normal text.
Important **word** in bold.

## Subheading (level 2)

To do list:

* do research
* do more research
* spend time with family and friends
```

One advantage of this type of markup for formatting is that it can be rendered to many different output formats - both in terms of file types, like `.docx`, `.html`, `.pdf`, and in terms of style, e.g. specific journal requirements.
Another advantage is that, because R Markdown files are in plain text, they are more suitable for version control using Git.
Some users might find it easier to activate the "Visual Editor" of R Studio (Ctrl + Shift + F4 or click on the icon that resembles drawing materials or a compass <!--CJ Add picture-->in the upper right corner of the R Markdown document) which features more graphical elements like a traditional word processor but still creates an R Markdown underneath with all of its flexibility.
The visual editor has some additional benefits, such as promoting best practices (for example, each sentence should be written on a newline, which eases tracking changes across versions)
and easing the generation of citations and references to Tables and Figures.

Now that you are familiar with the basics of Markdown formatting, we turn our attention to including code into the text.
Code is separated by three backticks (and the programming language in curly braces) like here:

````markdown
This is normal text, written in Markdown.

`r ''````{r}
# this is R code
1 + 1
```
````
The hotkey [Control]+[Alt]+[i] inserts a block of code in the file.
The results of code enclosed in such backticks will be dynamically inserted into the document (depending on specific settings).
This means that, whenever you render the R Markdown to its intended output format, the code will be executed and the results updated.
The resulting output document will be static, e.g. a pdf document, and can be shared wherever you like, e.g on a preprint server.

Once the R Markdown file has been rendered to a static document, the resulting file is decoupled from the R Markdown and the code that created it.
This introduces a risk that multiple versions of the static document are disseminated, each with slightly different results.
To avoid ambiguity, we therefore recommend referencing the identifier of the Git commit at the time of rendering in the static document.
Simply put, a static document should link to the version of the code that was used to create it.
The `repro` package comes with the function `repro::current_hash()` for this purpose.
This document was created from the commit with the hash ``r repro::current_hash()``.

Now that you know how to write text and R code in an R Markdown, you need to know about metadata (also called: YAML front matter).
These metadate contain information about the document, like the title and the output format.
Metadata are placed at the beginning of the document and are separated by three dashes from the document body; the following example is a full markdown document where the header is in lines 1--6.

````markdown
---
title: A Hitchhiker's Guide to Reproducible Research in R
author: Aaron Peikert, Caspar J. van Lissa and Andreas M. Brandmaier
abstract: A tutorial about doing the same thing more than once.
output: html_document
---

# Introduction

Important for reproducibility:

1. Version Control
2. Dynamic Document Creation
3. Dependency Tracking
4. Software Management

`r ''````{r}
# this is R code
t.test(extra ~ group, data = sleep)
```
````

Some metadata fields are self-explanatory (like the author field), and exist across all output formats (like the title field).
However, R Markdown has been extended to hundreds of document formats which are often highly configurable by their metadata.
Some R packages add additional metadata fields.
The `repro` package adds fields to the metadata to help make the research project reproducible.
These fields are used to list all dependencies of the research project.
This includes R scripts, data files, and external packages.
The format is as follows (see everything below the line *repro:*):

```markdown
---
title: A Hitchhiker's Guide to Reproducible Research in R
author: Aaron Peikert, Caspar J. Van Lissa and Andreas M. Brandmaier
abstract: A paper about recalculating the same thing more than once.
output: html_document
repro:
  scripts:
    - R/load.R
  data:
    - data/mtcars.csv
  packages:
    - tidyverse
    - usethis
    - gert
---
```

This information makes explicitly declares the project's dependencies on files and R packages.
`repro` uses this information to infer a Makefile for the dependencies on other files and a Dockerfile that includes all required packages.
These two files together form the basis for consistency within a research project and consistency across different systems.<!--CJ Consider explaining this in the theoretical intro-->
At this point, we would like to emphasize that a preqrequisite for reproducibility is that every step taken in an analysis can be automatically reproduced.<!--CJ This comes way to late. Shouldbe in the opening paragraph or something.-->
This means there must be no manual processing such as copy and pasting, manual modification of data files (e.g., manually removing observations or manually recoding variables). 
Ideally, all data preprocessing is well-documented and gathered in a separate R file, which is then called before a Markdown report is generated (that is, the preprocessing script is added in the metadata header under _scripts:_).<!--CJ These points describe the formal definition of reproducibility used in this paper, and the goal of the workflow. That needs to be explained in the introduction. If you only find out what you are working toward halfway through the tutorial - that's too late!-->

The function `repro::automate()` converts the metadata from all R Markdown files in the project (all files with the ending `.Rmd`) to a Makefile and a Dockerfile.
Together, these files allow users (including your future self) to reproduce every step in the analysis automatically.
It is important to re-run `repro::automate()` whenever you change the `repro`-metadata, change the output format, or add a new R Markdown file to the project, to keep the Makefile and Dockerfile up to date.
The function `repro::automate()` does not overwrite manual changes<!--CJ What does that mean?-->, and there is no harm in running it too often.<!-- CJ Can that not be automated? Or at least provide a suggestion when to run it?-->
Other than the Makefile and the Dockerfile, which are created in the document root path, repro generates a few more files in the `.repro` directory (which we will explain in detail later), all of which you should add and commit to Git. <!--AB: why is this addition not automated? Feels strange given that this is our recommended best practice?! CJ: Agreed.-->

<!-- CJ What i'm missing here is something to wrap up the tutorial. Do users now have a project that could be reproduced? The tutorial should not end until that is the case.-->

## Reproducing a project

<!--CJ Suggestion: Instead of explaining reproduction right in the middle of the tutorial for setting up a project, add a later section called "reproducing a project", and provide an example project for users to reproduce.-->
When you or someone else who received your project want to reproduce it, they can ask repro to explain how this should be done:

```{r}
repro::reproduce()
```

If you uncomfortable with using the terminal directly, you can sent the command to the terminal from within R:

```{r, eval=FALSE}
system(reproduce())
```

Importantly, a reproducible research project created with `repro` does not have the `repro` package itself as a dependency.<!--CJ This belongs in the intro, not the tutorial. It is part of explaining the benefits of repro.-->
These projects will remain reproducible irrespective of whether `repro` will be maintained and remain accessible in future.
Users do not have to have repro installed to reproduce a project, in fact, they do not even need to have R installed because the entire project can be rebuilt inside of a container that has R installed.
The only required software for reproducing a project is Docker, assuming users know how to build a Docker image and run Make within the container.
However, if they have installed Make additionally to Docker, they do not need to know how to use Docker and can simply rely on the two Make commands `make docker` and `make -B DOCKER=TRUE`.
The commands can be pasted into a terminal and will reproduce the project in many cases.

# Advanced Features

This section is only for advanced users who want to overcome some limitations of `repro`.
If you read this paper the first time you probably want to skip this section.
As explained above, `repro` is merely a simplified interface that enables a high level of reproducibility for simple projects.
The simplification of Docker and Make are possible because of two restrictions.
Users who ask themselves either "How can I install software dependencies outside of R in the Docker image?" or "How can I express complex dependencies between files, e.g. hundreds of data files are preprocessed and combined?") need to be aware of these restriction and require a deeper understanding of the inner workings of `repro`.
Other users may safely skip this section, or return to it if they encounter such challenges.

The metadata the `repro::automate()` function relies on can only express dependencies R packages for the Dockerfile and only trivial dependencies (in the form of "file must exist") for the Makefile. 
The first limitation means that users must rely on software that is either already provided by the base Dockerimage "rocker/verse" or the R packages they list in the metadata.
Other software that users might need like other programming languages, not already installed LaTeX packages etc. have to be added manually.
We plan to add support for commonly used ways to install software beyond R packages via the metadata and `repro::automate()` e.g. for system libraries (via `apt` the Ubuntu package manager), LaTeX packages (via `tlmgr` the Tex Live package manager), python packages (via `pip` the python package manager).
The second limitation is about dependencies.
Make can represent complex dependencies like in this example: A depends on B, which in turn depends on C and D.
If B is missing in this example, Make would know how to recreate it from C and D.
These dependencies and especially how these dependencies should be resolved are difficult to represent in the metadata.
Users therefore have to either "flatten" the dependency structure, by simply stating that A depends on B, C and D therefore leaving out important information or express the dependencies directly within the Makefile.

The following section explains how to overcome these limitation while still relying on the automatons afforded by `repro`.
Lifting these restrictions requires the user to interact more directly with Make or Docker.
To satisfy more complicated requirements users need to understand how repro utilizes Make and Docker internally.

Let us have a closer look at the command for reproducing a repro project: `make docker && make -B DOCKER=TRUE`; it consists of two processing steps.
First it recreates the virtual software environment (Docker) and then it executes computational recipes in the virtual software environment (Make).
The first step is done by the command `make docker`.
The command `make docker` will trigger Make to build the target called `docker`.
The recipe for this target builds an image from the Dockerfile of the repository.
The `&&` concatenates both commands and only runs the second command if the first was successful.
Therefore, the computational steps are only executed when the software environment is set up.
The second step executes the actual reproduction and is again a call to Make in the form of `make -B DOCKER=TRUE` with three noteworthy parts.
First, a call to `make` without any explicit target will build the Make target `all`.
Second, the flag `-B` means that Make will consider all dependencies as outdated and hence will rebuild everything.
Third, repro constructs Make targets so that if you supply `DOCKER=TRUE` they are executed within the Docker image of the project.

This interplay between Docker and Make is relatively complicated because it resembles a chicken or egg problem.
We have computational steps (Make) which depend on the software environment (Docker) for which we again have computational steps that create it.
Users only require a deeper understanding of this interdependence when they either want to have more complex computational recipes than rendering an R Markdown or require other software than R packages.

Users can have full control over the software installed within the image of the project.
`repro` creates three Dockerfiles inside the `.repro` directory.
The file `.repro/Dockerfile_base` contains information about the base image on which all the remaining software is installed.
By default we rely on the "verse" images provided by the Rocker project [@boettigerIntroductionRockerDocker2017].
These contain (among other software) the packages `tidyverse`, `rmarkdown` and a complete LaTeX installation, which makes these images ideal for the creation of scientific manuscripts.
Users can choose which R version they want to have inside the container by changing the version number in line 1 to the desired R version number.
By default the R version corresponds to the locally installed version on which `repro::automate()` was called the first time.
The build date is used to install packages in the version which was available on CRAN on this specific date and can also be changed, by default this date is set to the date on which `repro::automate()` was called the first time. 
This way, the call to the automate function virtually freezes the R environment to the state it was called the first time inside the container. 
Below, you see the Docker base file we used to create this manuscript:

```{r, results='asis', echo=FALSE}
cat(
  "```bash",
  readLines(here::here(".repro", "Dockerfile_base"))[1:3],
  "```",
  sep = "\n"
)
```

The file `.repro/Dockerfile_manual` is empty by default and can be used to add further dependencies outside of R, like system libraries or external software. 
Remember, the software environment of the Docker image is completely separate from software installed on your computer.
This separation is excellent for reproducibility, but requires you to install software on an operating system that may not be familiar to you.
The images supplied by [@boettigerIntroductionRockerDocker2017] are based on Ubuntu, and hence software has to be installed in a way that is compatible with Ubuntu.
The most convenient way to install software on Ubuntu is through its package manager `apt`.
The following snipped installs Python.
If added to `.repro/Dockerfile_manual` the Docker image will have Python installed.

```bash
RUN apt-get update && apt-get install -y python3
```

The last Dockerfile, `.repro/Dockerfile_packages`, is automatically generated from the metadata provided and must not be manually changed.
Whenever `repro::automate()` is called, repro gathers all R packages from all `.Rmd` files and figures out whether they should be installed from CRAN or GitHub fixed to the date specified in `Dockerfile_base`.

The separation into several Dockerfiles is a design choice to provide a clear separation between automatically generated parts based on metadata provided and manually altered parts of the Dockerfile.
Docker eventually requires a single Dockerfile to run, so `repro::automate()` simply concatenates the three Dockerfiles and saves the result into the main `Dockerfile` at the top level of the R project.

With this approach, users of `repro` can not only build complex software environments but they can also implement complex file dependencies.
The standard `repro` metadata only makes sure that all dependencies are available but does not allow you to specify custom recipes for them in the metadata.
If you can formulate the creation of dependencies in terms of computational steps, e.g. the file `data/clean.csv` is created from `data/raw.csv` by script `R/preprocess.R`, you should include these in the Makefile.
The Makefile that `repro` creates is only an template and you are free to change it.
However, make sure to never remove the following two lines:

```bash
include .repro/Makefile_Rmds
include .repro/Makefile_Docker
```

The file `.repro/Makefile_Rmds` contains the automatically generated targets from `repro::automate()` for the R Markdown files.
This file should not be altered manually.
If you are not satisfied with the automatically generated target than simply provide an alternative target in the main `Makefile`.
Targets in the main `Makefile` take precedent.

The file `.repro/Makefile_Docker` does again contain a rather complicated template which you could but usually should not modify.
This Makefile coordinates the interplay between Make and Docker and contains targets for building (with `make docker`) and saving (with `make save-docker`) the Docker image.
Additionally, it provides facilities to execute commands within the container.
If you write computational recipe for a target it will be by default evaluated using the locally installed software.
To instead evaluate commands inside the Docker image, you should wrap them in `$(RUN1) command $(RUN2)`, like in this example, which is identical to the first Make example we gave above except for the addition of `$(RUN1)` and `$(RUN2)` in l. 2:
```bash
simulated_data.csv: R/simulate.R
  $(RUN1) Rscript -e 'source("R/simulate.R")' $(RUN1)
```

If users execute this in the terminal:

```bash
make simulated_data.csv
```
It behaves exactly as the in the first Make example, the script `R/simulate.R` is run using the locally installed R.
Because this translates simply to:

```{r, echo=FALSE, results='asis'}
cat("```bash",
  stringi::stri_wrap(system2("make", " --dry-run simulated_data.csv", stdout = TRUE)),
  "```", sep = "\n")
```

But if users use

```bash
make DOCKER=TRUE simulated_data.csv
```

it is evaluated within the Docker container using the software within it and not the locally installed R version:

```{r, echo=FALSE, results='asis'}
cat("```bash",
  stringi::stri_wrap(system2("make", " --dry-run DOCKER=TRUE simulated_data.csv", stdout = TRUE)),
  "```", sep = "\n")
```

To summarise, `repro` provides researchers with an easy to use solution for dependency tracking (in form of Make) and software management (using Docker) without the necessity to learn both languages.
Users with more advanced requirements can manually change all aspects of both programs while still profiting from `repro`'s automations.

# Preregistration as Code 

Defining the research questions and planning data analysis before observing the research outcomes is called preregistration [@NosekRevolution2018].
Preregistration increases the credibility of empirical results by separating a priori planned analyses from those conducted unplanned and post hoc.
For a priori planned analysis, preregistration should reduce "researcher degrees of freedom" therefore the number of decisions researcher can make that could unduly influence the results.
Preregistrations, however, have a problem.
Even when several researchers describe their analysis with the same terms, use the same data, and investigate the same hypothesis the results vary considerably [@silberzahnManyAnalystsOne2018].
The current best practice to ensure that the preregistration are comprehensive and specific enough is to use preregistration templates [like @bowmanOSFPreregTemplate2020].
The structure these templates provides can more effectively reduce researchers degrees of freedom than an unstructured approach [@bakkerEnsuringQualitySpecificity2020].
But these templates can not remove researchers degrees of freedoms completely, because it is impossible to verbally describe every detail of an analysis for all but the simplest analysis.

Our proposed solution is simple:
Instead of describing the analysis, you write the code to conduct the analysis before you gather data.
We term this approach of writing and publishing code before data collection *preregistration as code (PAC)*.
PAC has the potential to all but eliminate researchers degrees of freedom that standard preregistration despite the best efforts afford.

The idea of submitting code as part of a preregistration is not new.
A prominent preregistration platform, [The Open Science Framework](https://osf.io/), suggests submitting scripts alongside the preregistration of methods.
In an informal literature search (we skimmed the first 300 results with the keywords `("pre registration"|"pre-registration"|preregistration)&(code|script|matlab|python|"R")`) we have found close to a dozen published articles which did included some form of script as part of their preregistration.

We believe, however, that combining ideas from reproducibility and preregistration can turn this "neat" idea into a tool with tangible benefits for the research community.

## Why are PAC and reproducibility related? 

First, preregistrations are plagued by non-reproducibility like any other research outcome.
@bakkerRecommendationsPreregistrationsInternal2020 found that, among the 210 preregistrations they investigated, 174 (67%) included a formal power analysis but from these only 34 (20%) provided enough information to reproduce the power analysis.
Even researcher who have gone to great length in submitting a script in their preregistration sometimes inexplicably fail to reproduce their own results.
For example, @steegenMeasuringCrowdAgain2014 realized after publication that part of their preregistered code results in different test statistic than they reported initially (see Footnote 7).

Second, although it has been almost a decade since @steegenMeasuringCrowdAgain2014 have preregistered their code, the idea of publishing code before analyzing data has gained not much traction.
This could change when combining the here proposed workflow and PAC.
We want to extend PAC from preregistering code to preregistering *the whole manuscript as a dynamic document*.
The idea is as follows:
For a PAC, researchers write a reproducible, dynamically generated manuscript including the sections Introduction, Methods, and Results (or similar) _before_ they gather empirical data.
In this document, the results are initially based on simulated data that mimics the expected output.

Writing a preregistration in form of a research article has the advantage that researchers already have great expertise in writing a scientific manuscript.
It follows that for preregistrations they should stick to widely accepted standards of scientific writing [e.g. @apa7].
We have already explained the need for reproducibility standards in the process of writing such manuscript and it is therefore only natural to extend these arguments to preregistration.

## Advantages of PAC over traditional preregistration

We believe that pairing PAC with the here presented workflow offers five advantages over classical preregistration.
First, a PAC removes any ambiguity regarding the translation of an verbal analysis plan into code.
Second, while being rigorous, it offers the flexibility to incorporate data-dependent decisions if they can be formulated as code.
Researchers can, for example, formulate conditions under which they prefer one analysis type over the other.
For example, if distributional assumptions are not met, the code may branch out to employ robust methods; or, an analysis may perform automated variable selection mechanisms before running the final model.
Another example of data dependent decisions are more explorative analyses, i.e. explorative factor analysis or machine learning.
With PAC researchers can precisely calibrate how much they want to explore.
When communicating a finding as explorative, it may still make a difference if tens or hundreds of variables were considered.
Third, a PAC is more comprehensive by design because its completeness can be empirically checked with simulated data.
We believe it is easier with PAC to detect missing steps or ambiguous decisions.
Because of its strict structure, PAC can therefore more effectively minimize researchers degrees of freedom than a standard preregistration [@bakkerEnsuringQualitySpecificity2020].
Fourth, deviations from the preregistration are visible because they are reflected in changes to the code, which are managed and tracked with version control.
Fifth, the preregistration is merely a development stage of the final manuscript, thus sparing authors from writing and editors and reviewers from evaluating two separate documents.

## Benefits of PAC for study planning

Leaving meta-scientific benefits aside, a preregistration is a tool for planning a study.
To use PAC for study planing, we suggest that researchers write a dynamically generated manuscript including the sections Introduction, Methods, and Results (or similar) _before_ they gather empirical data.
To that end, they should borrow from widely employed standards for writing an empirical manuscript [e.g. @apa7].
Then they should translate their method section into executable computer code and simulate data that has the same structure as the data they expect to obtain from their experiments.

We consider three domains where this procedure can help planning a study.
First, the plan should be comprehensive, that is, researchers need to state the research question and describe the study design and analysis in detail.
Second, the plan should be effective, meaning that the researchers can reasonably expect it to answer the research question at hand.
Third, the plan should be efficient; the plan should only require the necessary amount of resources.

### Comprehensiveness

Describing the preregistration in form of a journal article requires authors to follow the same standards that also ensure that research articles are comprehensive enough to replicate the described study.
One particular advantage is the strong foundation in existing literature where the introduction and theoretical background provide the basis for the studies' design.
Nevertheless it can be fruitful to consult preregistration templates, because of there strong focus on formulating a comprehensive methods section.

Another factor that ensures a comprehensive plan is writing the analysis based on simulated data.
While rather uncommon for an empirical research project, we argue that simulating data offers various benefits in the planning phase of a study.
Simulating data forces the researcher to think about and address data analytic problems on a conceptual level before they make the effort to acquire data [@axelrodAdvancingArtSimulation1997].
It also affords researchers with a simple technical check whether the analysis code runs without failure [@braiekTestingMachineLearning2020] (for example, does the code work with missing values, extreme outliers, etc.).
Writing incomplete code is technically impossible or at least researchers would notice if they e.g. forgot to aggregate the data etc.

### Effectiveness

With simulated data researchers can check if the analysis is technically complete and can also recover the true parameters of the simulation.
When an analysis fails to recover the correct values in idealized conditions with big samples researchers should not expect this analysis to work under realistic conditions.
Because the simulation captures the researchers expectations about the real world conditions it is the best way to assess the effectiveness of the analysis without actually gathering the data.

### Efficiency

A single simulated or synthesized dataset already helps to ensure that the plan is viable under certain circumstances.
However, repeated simulation with varying parameters, a so-called Monte Carlo simulation, lets researchers play through several scenarios.
Monte Carlo simulation lets researcher navigate several trade offs regarding their resources.
In the simplest case they can assess the power (the probability to detect an effect of a given size) for varying numbers of observations in the simulation to determine the required sample size for their study [@brandmaier2018precision]. @bakkerRecommendationsPreregistrationsInternal2020 found that a formal power analysis does not necessarily increase the planned sample size. You may also evaluate how the data analysis copes with expected data complications, e.g. non-normal data [@harrisonIntroductionMonteCarlo2010; @raychaudhuriIntroductionMonteCarlo2008].

## What you need to know before writing a PAC

To write a PAC researchers should start by drafting a manuscript in form of an R Markdown.
When they have completed the method section, they should begin writing code.
We suggest writing three functions for each hypothesis, one for the planned analysis, one for the simulating the data and one for reporting the results.
At the heart of a PAC lies the planned analysis so researchers should first draft a function called `planned_analysis()` that captures the description in the method section.
This function should receive the data and derive relevant results from it.

```{r}
planned_analyis <- function(data){
  # 1. preprocess e.g. with `rowMeans(data)`
  # 2. conduct analysis e.g. with `t.test()`
  # 3. `return(results)`
}
```

Because it is difficult to draft such function without interactively test it, researcher should than proceed to write a function for simulating data.

```{r}
simulate_data <- function(n, effect_size){
  # 1. warn users that the results are "fake"
  # 2. draw `n` samples with `effect_size`
  # 3. format and return in expected data format
}
```

As soon as they have written `planned_anaylsis()` and `simulate_data()` they can iteratively improve both functions, e.g. till `planned_analysis()` runs without error and recovers the correct parameters from `simulate_data()`.
The goal is that the output of `simulate_data()` works as input to the function `planned_analysis()`.

This process can be tedious and complicated.
If it proofs infeasible to simulate data, researcher can delay their preregistration till they have gathered data and repurpose the actual data.
Of course researcher must not analyze the data before they write the preregistration but they can obfuscate the real data by shuffling the dependent variable or otherwise create synthetic data that allows them to test the function `planned_analysis()` without actually revealing the research outcomes.

When the researchers are satisfied with the function `planned_analysis()` they can think about how they would like to report the results of the analysis, via tables, plots and text.
The implementation of this reporting should be in the function `report_analysis()`.

```{r}
report_results <- function(results){
  # 1. create markdown tables from results
  # 2. conditionally interpret results e.g. if(p < .025)"Result is significant."
  # (optional) visualize results
  # 3. return results section formatted in markdown
}
```

These function again should be accept the output of `planned_analysis()` as input.
The output of this function should be formatted in Markdown.
A variety of packages automatically generate well formatted outputs of statistical reports or even entire tables of estimates or figures directly from R objects.
Packages like [`pander`](https://github.com/Rapporter/pander) [@pander], [`stargazer`](https://cran.r-project.org/web/packages/stargazer/vignettes/stargazer.pdf) [@stargazer], [`apaTables`](https://dstanley4.github.io/apaTables/articles/apaTables.html) [@apaTables] and [`papaja`](http://frederikaust.com/papaja_man/) [@papaja] help you to create dynamically generated professional looking results.
The package [`report`](https://github.com/easystats/report) [@report] is particularly noteworthy because it not only generates tables, but also a simple interpretation of the effects as actual prose (e.g it quantifies the size of an effect).

Ideally, these three functions can be composed to create a "fake" results section, e.g. when composed to `report_analysis(planned_analysis(simulate_data()))` or `simulate_data() %>% planned_analysis() %>% report_analysis()` outputs a results section.

To summarize researchers write three functions, `planned_analysis()`, `simulate_data()`, and `report_analysis()` and embed these into a manuscript that serves as preregistration.
After they gathered the actual data, they replace the simulated data, run `planned_analysis()` and write the discussion.

# Preregistration as Code --- a Tutorial 

To guide the reader through a preregistration as code (PAC), we will use an exemplary research question that was based on openly available data and merely chosen for simplicity of illustration: 

> "Is there a mean difference in the personality trait 'Machiavellism' between self identified females and males?" 

This example serves only didactic purposes and not to derive substantive claims.
Nevertheless, everything else is kept as close to a real research question as possible.

At the heart of a PAC lies the planned analysis.
We suggest writing a main analysis function that captures the full process of testing a research question.
This function receives a dataset as input and performs all statistical processing. <!--AB say anything about its output? is the Markdown the output?-->
For the hypothetical research question, we could for example plan to conduct a simple Student's t-test [@studentProbableErrorMean1908] with Welch's correction [@welchGeneralizationStudentProblem1947] an the average of machiavellism items of the Short Dard Triad [SD3, @Jones2013] between the binary-coded gender groups.
As an example of a data-dependent decision, we included a rank transformation if the skewness was greater than 1, converting the t-test to a supposedly more robust Mannâ€“Whitneyâ€“Wilcoxon test.[^caution-mww]

[^caution-mww]: The Mannâ€“Whitneyâ€“Wilcoxon is in fact **not** more robust than the Welch corrected t-test for reasonably large sample sizes. The interested reader can use the provided code for the simulation to verify that the t-test provides unbiased effect sizes but the Mannâ€“Whitneyâ€“Wilcoxon overestimates effect size with increasing sample size and skewness.

This function is one possible translation of the verbal description above:

```{r planned_analysis}
```

Importantly, natural language descriptions do not map unambiguously onto computer code implementations. 
The same textual descriptions may be implemented differently by different persons depending on their statistical and programming knowledge but also based on their own expectations and beliefs.
One example would be using the function `wilcox.test` instead of the combinations of `rank` and `t.test`.
Both are valid implementation of the Mannâ€“Whitneyâ€“Wilcoxon test but the first assumes equal variance whereas the second applies Welch's correction and hence is robust even with unequal variances across groups [@zimmermanRankTransformationsPower1993].
Mentioning every such minute implementation detail is almost impossible and would result in overly verbose preregistrations.
Still, these details can make a difference and provide unwanted researcher degrees-of-freedom.

<!--AB here is kind of a break in the text-->
To use this function one needs a compatible dataset. <!--AB what is "this" function? What does "compatible" mean?-->
At the time of preregistration there is of course no actual data available.
We therefore simulate a dataset first that resembles the expected structure of the empirical data that we will use for the final analysis.
Using the simulated dataset as placeholder, we can now write a scientific report including all analysis code blocks and the presentation of results, which can serve as PAC.
The following code simulates a dataset with one observed variable in two groups.

```{r simulate_data}
```
<!--AB it is odd that the return variable is called effect because it is a dataset-->

Here, we decided to randomly draw non-normal data to test how well our procedure performed under non-optimal situations.
Specifically, we draw from a $\chi^2$-distribution, where in one group we add difference that conforms to a given standardized effect size (a Cohen's d of .2).
This simulated dataset fits perfectly into our planned analysis with a sample size of 400 and 10 items: <!--AB in what way? please explain-->

***

```{r}
set.seed(112358) # fix state of RNG
results <- 
  planned_analysis(
    simulate_data(
      n = 400,# Sample size
      df = 8, # Chisq with DF = 8 has skewness = 1
      d = .2, # Cohen's D
      i = 10  # Number of items (should not effect anything)
    )
  )
```

Next, we add dynamic code blocks to our Markdown report that implement the planned reporting of the final results.
A variety of packages automatically generate well formatted outputs of statistical reports or even entire tables of estimates or figures directly from R objects.
Packages like [`pander`](https://github.com/Rapporter/pander) [@pander], [`stargazer`](https://cran.r-project.org/web/packages/stargazer/vignettes/stargazer.pdf) [@stargazer], [`apaTables`](https://dstanley4.github.io/apaTables/articles/apaTables.html) [@apaTables] and [`papaja`](http://frederikaust.com/papaja_man/) [@papaja] help you to create dynamically generated professional looking results.
The package [`report`](https://github.com/easystats/report) [@report] is particularly noteworthy because it not only generates tables, but also a simple interpretation of the effects as actual prose (e.g it quantifies the size of an effect).
Here we use the `report` package to create a preview of how our reporting will look like based on simulated data:<!--AB add citation to the report package-->

```{r, results='asis', eval=FALSE}
report::report(results$test)
```

This call to the `report()` function will yield the following automatically generated output:

```{r, results='asis', echo=FALSE}
report::report(results$test)
```
<!--AB it is difficult to discern the automatically generated text and out manuscript text even though there is the horizontal line. Not sure how to improve this.-->
***

```{r, echo=FALSE}
res <-
  read_csv(here::here("data", "simulation_results.csv"), col_types = "ddddddd")
choosen_power <- .8
choosen_d <- .2
minn <- filter(res, d == choosen_d, power > choosen_power) %>% 
  filter(n == min(n))
```

To determine a minimal sample size for this study we repeat this simulation across several combination of effect sizes and sample sizes.
One standard recomendations for choosing a sample size is to have an expected power of 80% based on the expected effect size.
@Jones2013 found in three studies gender differences of .24, .29, and .35 for their machiavilism subscale.
If we want to detect an effect of size `r choosen_d` with an `r choosen_power*100`% probability, our simulation shows that we require at least `r minn$n` participants.

<!--AB please add a figure number and a proper citation to the figure. Also, tell people where they can find the code to run the power analysis themselves-->

```{r, echo=FALSE}
res %>%
  ggplot(aes(n, power, color = d, group = d)) +
  geom_line() +
  geom_point(
    data = minn,
    size = 3,
    color = "black",
    shape = 3
  ) +
  theme_minimal() +
  scale_color_viridis_c() +
  theme(legend.title = element_text()) +
  labs(y = "Power\n(Proportion of significant results)",
       x = "Sample size",
       color = "Cohen's D") +
  NULL
```

# Discussion

## Summary

We demonstrated how the R package `repro` supports researchers in creating reproducible repositories and reproducible manuscripts.
These are important building blocks for transparent and cumulative science because they enable others to reproduce statistical and computational results and reports later in time and on different computers.
The workflow we present here rests on four software solutions, version control, dynamic document generation, dependency tracking and software management to all but guarantee reproducibility.

We then proceeded to show preregistration as code (PAC) as one example of how such rigorous and automated reproducibility workflow may enable other innovations.
In a PAC researcher write all analysis code and a reproducible manuscript that already renders the entire reporting of statistical and computational results before they gather data.
These results are based on simulated data, so that there is no danger of peeking at the real data while writing the scripts.
This way, all researcher's degrees of freedom are eliminated.
Once real data is gathered, the reproducible manuscript is (re-)created with the real data.
Because the same dynamic document serves as preregistration and final manuscript, researcher, reviewers and editors only have to revise one document.
To editors, reviewers and readers it is obvious how the researchers deviated from the preregistration because everything is under version control.
The likelihood of those deviations, while unavoidable, is decreased by using simulated data to verify the suitability of the analysis plan.
This analysis plan may contain data depended decisions to further decrease the likelihood that researcher are unable stick to their preregistration

## Limitations of repro

`repro`'s primary goal is to increase the user-friendliness of reproducibility tools for researchers.
While we have invested much effort to ensure this goal, the package is still in early development and undoubtedly has rough edges and even bugs that went undetected.
We hope that despite the early development stage, `repro` can support researchers to work more reproducible.

The workflow we present here introduces several software solutions which in itself presents a danger to reproducibility.
To profit from automatic and convenient reproduction researcher must install Git, Make, and Docker.
However, Git and Make are itself included in the Docker image created by `repro`.
Researchers can therefore employ the Docker image manually to download the Git repository and execute Make for full reproduction.
In other words the only hard requirement for reproduction and therefore Achilles' heel for this workflow is Docker.
The Docker approach has two vulnerabilities.
First, and more important, the Docker image for the project has to remain available.
The Dockerfile (the plain text description to build a Docker image) on the other hand is insufficient because it relies on to many service providers (e.g. Microsoft R Application Network, Ubuntu Package Archive).
The Docker image can be saved to disk and treated like any other file that requires careful archivation.
However, if the existence of the Docker image is guaranteed, researchers still require software to run the image.
To that end they can either rely on Docker itself or Docker-compatible alternatives (e.g. CoreOS rkt, Mesos Containerizer, Singularity).
The only way to remove the reliance on such external software is to turn the Docker image into a full operating system that subsequently can be installed and run on almost any modern computer.
This process is technically possible and would guarantee reproducibility for decades without any software dependency.
However, this process requires much technical knowledge and is currently not supported by `repro`.
In any case and without such precautions do the R Markdown, the Makefile and the Dockerfile provide information that allow researchers to manually trace the computational steps and recreate the computational environment.
The Makefile, for example, is written in a way that researchers can manually trace the dependencies and execute commands in the right order, in case they are, for whatever reason, unable to run Make.
Similarly, even when Docker does not work, the Dockerfile still serves as unambiguous documentation of how the original system was set up and may help future users to create a software environment that closely resembles the original.

# Limitations

We realize that PACs are challenging.
Not every research question or project is suitable for preregistration in general, and many analytical strategies could prove unwieldy for a PAC.
Therefore, PAC should not be seen as an attempt to invalidate classical preregistration or not preregistered research but as a complement.
The rigidness of PAC is only than to much of a good thing, if one holds the view that researchers can for no reason deviate from their preregistration.
However, we are convinced that many reasons justify changing the preregistered code, but are equally convinced that readers should judge themselves if they find these deviations from the preregistration meaningful.
PAC enables just that.
Researcher can change their plans and possibly defend their decision, e.g. in the discussion, while readers of the manuscript can clearly see what was changed through version control.

Another challenge is that PAC requires more programming expertise than a usual analysis.
In contrast to a reproducible script, which should work on the same data, a PAC must work on different data.
We call the property of code that can run on different inputs from a similar context and produce valid outputs "reusability".
This property is based on reproducibility but requires the researcher to more carefully write the software [@lanerganSoftwareEngineeringReusable1989] such that it is _build-for-reuse_ [@al-badareenReusableSoftwareComponents2010].
The reproducible workflow we present here is heavily automated and hence promotes reusability.
Increased automation is more and more recognized as a means to improve the research process [@rouderMinimizingMistakesPsychological2019], and therefore this workflow fits well together with other innovations that employ automation, like machine-readable hypothesis tests [@lakensImprovingTransparencyFalsifiability2021] or automated data documentation [@arslanHowAutomaticallyDocument2019].
Automated and reusable research projects promise a wide range of applications, among them PAC [possibly submitted as a registered report @nosekRegisteredReports2014; @chambersWhatNextRegistered2019], Direct Replication[@simonsValueDirectReplication2014], fully automated living metanalysis [@elliottLivingSystematicReviews2014], Executable Research Articles [@elifesciencespublicationsELifeLaunchesExecutable2020], and other innovations like the live analysis of born open data [@rouderWhatWhyHow2016; @kekecsRaisingValueResearch2019].<!--AB very nice!!-->

Therefore reproducibility facilitates traditional good scientific practices and provides the foundation for promising innovations.

Please note that the analytical procedure in the PAC example of running different test procedures based on a fixed skewness threshold was created for didactic purposes, and it is not necessarily optimal for addressing your research question.
The t-test and Mannâ€“Whitneyâ€“Wilcoxon test are arguably the most often used hypothesis test [@fagerlandTtestsNonparametricTests2012; @hortonStatisticalMethodsJournal2005 reports that 26% of all studies employ a t-test and 27% employ a rank based alternative in the New England Journal of Medicine in 2005].
The analytical strategy presented here is, in fact, suboptimal concerning several criteria (untested assumption of measurement invariance [@putnickMeasurementInvarianceConventions2016], underestimation of effect size in the presence of measurement error [@frostCorrectingRegressionDilution2000], overestimation of effect size for highly skewed distributions [@stonehouseRobustnessTestsCombined1998]). 

# Outlook

Open science practices are a continually evolving field where technical innovations foster changes in research practice.
Open data is much more widespread thanks to online storage facilities; preregistration are possible because there are preregistration platforms and so forth.
Similarly, we hope that fully automatic reproduction, e.g. with `repro` as technical innovation, increase scientific rigour and efficiency.

This ideal of a fully automatic reproduction of research projects meets in practice a wide range of demands on how user-friendly and how powerful the software solutions are.
Some may find Make too complicated or that Docker requires too much storage.
Yet others may find that they require other programming languages or scale their computation across hundreds of computers, e.g. via high-performance computing clusters or cloud computing.

`repro` was designed modularly to meet such demands.
At the moment, `repro` only supports the combination of RMarkdown, Git, Make, and Docker.
However, there are alternatives for each of these elements that may fit better into an individual research project.
RMarkdown could be complemented or replaced by a dynamic Microsoft Word document with the help of `officer`[@officer] or `officedown`[@officedown] to accommodate a wider arrange of journal submission standards.
Instead of using formal version control with Git, `repro` could automatically save snapshots for increasing user-friendliness.
Make could be replaced by the more R-centered alternative `targets` for more convenience.
Docker could be combined with `renv` [@R-renv] to control the package versions precisely (our approach fixes the date, `renv` the exact package version).
Alternatively, Docker could be replaced by the more lightweight `renv` if no dependencies outside of R are considered crucial.
Docker does not satisfy the requirements of many HPC environments, but Singularity was designed to avoid this limitation while still being compatible with Docker images.

`repro`'s modular structure allows such alternative workflow, though they have not been implemented.
Depending on the demand by users, we will implement some of these alternative workflows in `repro`.

# References

```{r, include=FALSE}
knitr::write_bib(
  c(
    .packages(),
    "repro",
    "here",
    "rticles",
    "gert",
    "bookdown",
    "lavaan",
    "knitr",
    "targets",
    "renv",
    "tidyverse"
  ),
  here("packages.bib")
)
cat(c(readLines(here::here("packages.bib")), "\n", readLines(here::here("references.bib"))), sep = "\n", file = "temp.bib")
```

