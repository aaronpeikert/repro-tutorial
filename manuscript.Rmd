---
title: A Hitchhiker's Guide to Reproducible Research in R
author:
  - name: Aaron Peikert
    affil: 1, *
    orcid: 0000-0001-7813-818X
  - name: Caspar J. Van Lissa
    affil: 2, 3
    orcid: 0000-0002-0808-5024
  - name: Andreas M. Brandmaier
    affil: 1, 4
    orcid: 0000-0001-8765-6982
affiliation:
  - num: 1
    address: |
      Center for Lifespan Psychology---Max Planck Institute for Human Development
      Lentzeallee 94, 14195 Berlin, Germany
  - num: 2
    address: |
      Department of Methodology & Statistics---Utrecht University faculty of Social and Behavioral Sciences, Utrecht, Netherlands
  - num: 3
    address: |
      Open Science Community Utrecht, Utrecht, Netherlands
  - num: 4
    address: |
      Max Planck UCL Centre for Computational Psychiatry and Ageing Research
      Berlin, Germany and London, UK
# firstnote to eighthnote
correspondence: |
  peikert@mpib-berlin.mpg.de
journal: psych
type: tutorial
status: submit
bibliography: temp.bib
appendix: appendix.tex
simplesummary: |
  Reproducibility has long been considered integral to the scientific method.
  Until recently, detailed descriptions of methods and analyses were the primary instrument for ensuring scientific reproducibility.
  Technological advancements now enable scientists to achieve a much higher standard of reproducibility;
  one in which any individual can be granted access to a digital research repository, and reproduce the analyses from raw data to final manuscript with a single command.
  This method has far-reaching implications for scientific archiving,
  reproducibility and replication,
  scientific productivity,
  and the credibility and reliability of scientific findings.
  One obstacle preventing the widespread adoption of this method is that the underlying technological advancements
  are complicated to use.
  This paper introduces `repro`; an R-package that guides researchers in the installation and use of the tools required for computational reproducibility.
  It includes a tutorial on how to make a research project reproducible with the aid of `repro`.
  Finally, it introduces novel applications of reproducibility tools for scientific research;
  including the preregistration of study plans as reproducible computer code.
  This circumvents the shortcomings of ambiguous preregistrations that allow for researcher degrees of freedom,
  because computer code describes study plans more precisely than prose.
  <!-- CJ Add concluding sentence(s) -->
abstract: "`r tryCatch(trimws(readr::read_file(here::here('abstract.Rmd'))))`"
keywords: |
  keyword 1; keyword 2; keyword 3 (list three to ten pertinent keywords specific 
  to the article, yet reasonably common within the subject discipline.).
acknowledgement: |
  All sources of funding of the study should be disclosed. Please clearly 
  indicate grants that you have received in support of your research work. 
  Clearly state if you received funds for covering the costs to publish in open 
  access.
authorcontributions: |
  For research articles with several authors, a short paragraph specifying their 
  individual contributions must be provided. The following statements should be 
  used ``X.X. and Y.Y. conceive and designed the experiments; X.X. performed the 
  experiments; X.X. and Y.Y. analyzed the data; W.W. contributed 
  reagents/materials/analysis tools; Y.Y. wrote the paper.'' Authorship must be
  limited to those who have contributed substantially to the work reported.
conflictsofinterest: |
  The authors declare no conflict of interest.
abbreviations:
  - short: GUI
    long: Graphical User Interface
  - short: CLI
    long: Comand Line Interface
repro:
  packages:
    - tidyverse
    - usethis
    - gert
    - aaronpeikert/repro@7bfaf98
    - here
    - rstudio/webshot2@f62e743
    - targets
    - renv
    - slider
    - patchwork
    - knitr
    - pander
    - lavaan
    - furrr
    - future.batchtools
    - rticles
    - moments
    - report
  scripts:
    - R/simulation.R
    - R/simulation_funs.R
    - R/link.R
  data:
    - data/simulation_results.csv
output:
  bookdown::markdown_document2:
    base_format: rticles::mdpi_article
header-includes:
   - \usepackage[labelformat=empty]{caption}
   - \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}, numbers=left}
---


```{r setup, include=FALSE}
library(repro)
automate_load_packages()
automate_load_scripts()
source(here::here("R", "link.R"))
```

```{r echo=FALSE}
# define a print method for objects of the class data.frame
knit_print.data.frame = function(x, ...) {
  res = paste(c('', '', kable(x)), collapse = '\n')
  asis_output(res)
}
# register the method
registerS3method("knit_print", "data.frame", knit_print.data.frame)
```

# Introduction

Increasingly, scientists openly share their data, materials, and analysis code.
Sharing the digital research output increases scientific efficiency by enabling researchers to learn from each other, reuse materials, or review published research results, but only if these artifacts are reproducible.
The technology is now available to make all artefacts related to a research project available so that the project can be reproduced with minimal effort.
Reproducibility has long been considered integral to empirical research, whose credibility hinges on its objectivity, meaning "in principle it can be tested and understood by anybody." [@popperLogicScientificDiscovery2002, p. 22][^popper]
In contemporary research, reproducibility can be defined as the ability of anyone to obtain identical results from the *same* data with the *same* computer code [see @Peikert2019 for details].
Unfortunately, despite increasing commitment to open science practices, many projects can not yet be reproduced by other research teams.
This is because making a research project with all code, data, and materials reproducible comes with a variety of challenges, and there is a dire need for reproducibility standards.

This tutorial is aimed at researchers who intend to ensure the reproducibility of a research project and are willing to make relevant code, data, and materials available, whether publicly or on request.
We pursue two aims.
First, the present paper introduces the reader to a workflow that ensures reproducibility by building on several technical solutions originating in the software engineering community that eliminate four common threats to reproducibility [see @Peikert2019 for details].
Specifically, we demonstrate how the R package `repro` supports researchers in implementing these solutions.
The second objective is to discuss opportunities of such workflow to improve not only how research outputs are created and recreated but also how research itself is conducted.
One such opportunity is an alternative preregistration scheme that builds upon a reproducible workflow, the so-called preregistration as code (PAC), where the researchers write the complete analysis as code and the bulk of the manuscript as dynamic document, including the sections Introduction, Methods, and Results---initially based on simulated data, before they gather data.
This dynamic document closely resembles an actual journal article but with "fake" results and without discussion of results, then serves as preregistration.

[^popper]: In fact, @popperLogicScientificDiscovery2002's famous criterion of demarcation is built around "inter-subjective testing", a concept which he later generalised to inter-subjective criticism (Footnote *1, p. 22). Somewhat confusingly, @popperLogicScientificDiscovery2002 uses the term "Reproducibility" for what we would call "Replication".

While the willingness to share materials is required for reproducing a project, it is not sufficient.
For example, @hardwicke2018 attempted to reproduce results from open materials in the journal Cognition. 
Out of 35 published articles, results of 22 articles could be reproduced but in 11 of these cases, assistance from the original authors was required.
For 13 articles, at least one outcome could not be reproducedâ€”--even with the original authors' assistance.
@obels2020 showed that in 62 Registered Reports, 41 had data available, and 37 had analysis scripts available.
The authors could execute only 31 of the scripts without error and reproduce the results of only 21 articles (within a reasonable time).
These failed attempts to reproduce highlight the need for widely accepted reproducibility standards.
The results further illustrate that open repositories do not routinely provide sufficient information to reproduce relevant computational and statistical results.

We argue that the benefits associated with transparency increase if research can additionally be reproduced.
This paper tutorial demonstrates how to make research projects reproducible, while striking a balance between rigor and ease-of-use.
A rigorous standard increases the likelihood of any individual project to be actually reproducible.
An easy to use standard, on the other hand, is more likely to be adopted.
The remainder of this paper is structured along these lines.
The first part introduces theoretical concepts and software solutions that together form a rigorous reproducibility standard.
The second part is a tutorial that shows how these software solutions can be easily employed by researchers with the help of the R package `repro`.

We have structured the tutorial with a _learning by doing_ approach in mind, such that readers can follow along at their own computers.
We explicitly encourage readers to try out all R commands for themselves.
Unless stated otherwise, all code blocks are meant to be run in the statistical programming language R (tested with version `r with(R.version, paste0(major, ".", minor))`).

# Concepts and Software Solutions

We identified the following common threat to reproducibility: <!-- CJ How did you identify these causes? Is this exhaustive?-->

1.	**Multiple inconsistent versions of code, data, or both**; for example, the dataset could have changed over time because outliers were removed at a later stage, or the analysis code could have been modified during writing of a paper because a bug was removed at some point in time; it may then be unclear, which version of code and data was used to produce some reported set of results.
2.	**Copy-and-paste**; for example, often, results are manually copied from a statistical computing language into a text processor; if a given analysis is re-run and results are manually updated in the text processor, this may inadvertently lead to inconsistencies between the reported result and the reproduced result;
3.	**Ambiguous order of code execution**; for example, with multiple data and code files, it may be unclear which scripts should be executed in what order; 
4.	**External code dependencies**; for example, a given analysis may depend on a specific version of a specific software package that might not be available on a different computer or no longer exist; or, a different version of the same software may produce different results.
<!-- CJ 5. Missing steps: Some of the steps are documented (e.g., final analysis), but other steps were conducted manually and not documented (e.g., copy-pasting results from one program to another; reverse coding items on the fly).-->

We have developed a workflow that leverages established tools and practices from software engineering to achieve long-term and cross-platform computational reproducibility of scientific data analyses resting on four pillars that address the aforementioned causes of non-reproducibility [@Peikert2019]: <!-- CJ In fact, this is the central message of your paper: "We have developed this new workflow, based on four pillars." I would suggest mentioning this very clearly in the opening paragraph. The opening paragraph should at least:
1) Introduce the topic area
2) Introduce the problem/knowledge gap
3) Introduce how you will fill this knowledge gap-->

1. Version control
2. Dynamic document creation
3. Dependency tracking
4. Software management

The remainder of this section briefly explains why each of these four building blocks is needed and their role in ensuring reproducibility.

First, to resolve ambiguity across multiple versions of code and data that come into existence during the development phase of a research project, we recommend using a **version control** system.
Version control allows a clear link between which results were generated by which version of code and data.
A version control system tracks changes to all project-related files (e.g., materials, data, and code) over time.
At a later stage, single files or the entire project can be compared to or reverted to an earlier version.
A version-controlled project makes the loss of files unlikely and also supports remote collaboration.

We recommend using **Git** for version control, because of its widespread adoption in the R community.
Git is built around snapshots that represent the entire project state at a given point in time.
Those snapshots are called "commits".
Each file that was changed has to be added to a commit, and each commit requires a commit message that succinctly describes the changes made.
Each commit "knows" its ancestor, and they form a timeline together.
The conglomerate of commits (i.e. the version-controlled project) is called a repository.
In Git, specific snapshots of a repository can be tagged, such that one can clearly label a given project status as the one that created a given preregistration, a given preprint, or the final authors' version accepted at a journal.
Git has additional features beyond basic version control, such as branches to facilitate collaboration.
To collaborate via Git,<!--dangling modify, cant help it--> the repository has to be uploaded somewhere.
We recommend GitHub to host Git repositories because of its popularity among R users.
GitHub has many tools that ease project management and collaboration, and these tools provide much value in our everyday work, but they are not central to archiving reproducibility.

Second, we rely on **dynamic document generation**.
The traditional way of writing a scientific report based on a statistical data analysis uses two separate steps conducted in two different programs.
In a word processor, the researcher writes the text, and in another software, they conduct the analysis.
Both steps are linked by manually copying and pasting results from one software to the other, a process that often produces inconsistencies [@nuijtenPrevalenceStatisticalReporting2016].

Dynamic document generation fuses both steps and can be traced back to @knuthCWEBSystemStructured, who put it this way: "The main idea is to regard a program as a communication to human beings rather than as a set of instructions to a computer."
**R Markdown** uses Markdown for writing the text and R (or other programming languages) for the analysis.
Markdown is a lightweight text format in plain text with minimal reserved symbols for formatting instructions.
This way, Markdown does not need any specialized software for editing.
It is highly fool-proof (unlike, for example, LaTeX), works well with version control systems, and can be ported to various document formats, such as HTML Websites, a Microsoft Word document, a typeset PDF file (for example, via LaTeX journal templates), or a Powerpoint presentation.
We suggest using Markdown for everything, starting from simple sketches of your ideas to your scientific manuscripts[@R-rticles], and presentations[@revealjs] or even your CV [@vitae].
R Markdown extends regular Markdown by allowing users to include R code chunks (in fact, arbitrary computer code; see @riedererChapter15Other) into a Markdown document.
Upon rendering the document, the code blocks are executed, and their output is dynamically inserted into the document.
This allows the creation of (conditionally) formatted text, statistical results, or figures that are guaranteed to be up-to-date because they are created every time anew as the document is rendered to its output format (e.g., presentation slides or a journal article).

One would hope that sharing a literate statistical analysis would allow others (including one own's future self) to simply download and run (ie., reproduce) a given analysis.
However, while version control and dynamic document generation are becoming more common @, this approach fails to guarantee reproducibility in most instances.
In practice, dependencies between project files (e.g. the raw data was preprocessed) or on external software (such as system libraries or components of the programming language) are frequently unmentioned.

To automatically resolve dependencies between project files, we rely on **dependency tracking**.
In essence, researchers provide a collection of computational recipes that describe how ingredients are processed to create intermediate products and, in the end, a final product.
Similar to a collection of cooking recipes, we can have multiple products (_targets_) with different ingredients (_requirements_) and different steps of preparation (_recipes_).
In the context of scientific data analysis, the targets are typically the final scientific report (e.g., the one to be submitted to a journal) and possibly intermediate results (such as preprocessed data files, simulation results, analysis results).

We recommend using **Make** for dependency tracking because it is language independent.
To make this more concrete, consider the following hypothetical example, in which a research project contains a script to simulate data (`simulate.R`) and a scientific report of the simulation results written in R Markdown (`manuscript.Rmd`).
A Makefile could then look like this:

```{bash, eval=FALSE}
manuscript.pdf: manuscript.Rmd simulation_results.csv 
  Rscript -e 'rmarkdown::render("manuscript.Rmd")'

simulation_results.csv: simulate.R
  Rscript -e 'source("simulate.R")'
```

There are two targets, the final rendered report (`manuscript.pdf`, l.1) and the simulation results (`simulation_results.csv`, l.4).
Each target is followed by a colon and a list of requirements.
If a requirement is newer than the target, the recipe will be executed to rebuild the target.
If a requirement does not exist, Make uses a recipe to build the requirement first before building the target.
Here, if one were to build the final `manuscript.pdf` by rendering the R Markdown with the command shown in l.2, Make would make sure that the file `simulation_results.csv` exists; if not, it would find that this is also a target and issue the command in l.5 to run the simulation first before rendering the manuscript.
This way, we ensure that the simulation is always run before the manuscript is built and that the manuscript is rebuilt if the simulation code was changed.

A version controlled dynamic document with dependency tracking still relies on external software.
Troubleshooting issues specific to a particular programming language or dependent tool typically requires considerable expertise and hinder reproducibility.

```{r, include=FALSE}
container_size <- as.numeric(fs::file_size("reprotutorial.sif"))/1024^3
```

One solution to the problem of external software dependencies is to provide not only the analysis script but all dependent software packages and system libraries via **software management**.
One comprehensive approach to software management is containerization.
"By packaging the key elements of the computational  environment  needed  to  run  the  desired  software, [...] they  make the software much easier to use, and the results easier to reproduce [@silverSoftwareSimplified2017].
Instead of leaving it to the user which software should be installed how on their computer, **Docker** provides all software used in an analysis, including the operating system itself.
The operating system level is important because some functionality may be passed to software layers beneath the programming language, such as calls to random number generators, linear algebra libraries and such.
Docker does this without interfering with the already installed software by using a virtual software environment independent of the host software environment.
Such a snapshot of the software stack is called an "image".
Packaging all needed software in such an image requires considerable amounts of storage space.
Two major strategies are helping to keep the storage requirements reasonable.
First, there is a community that maintains images for particular purposes.
For example, there are ready-made images that only include what is necessary for R, based on Ubuntu (a Linux operating system) containers [@boettigerIntroductionRockerDocker2017].
Users can then install whatever they need in addition to what is provided by these pre-compiled images.
`r ifelse(is.na(container_size), "", glue::glue("The image that was used for this article uses {round(container_size, 2)}Gb of disc space."))`
Our image includes Ubuntu, R, R Studio, LaTeX as well as a variety of R packages like the tidyverse [@tidyverse], amounting to `r installed.packages() %>% as.data.frame() %>% filter(is.na(Priority)) %>% nrow()` packages.
Second, instead of saving a binary image of the software, one can also save a container recipe, a so-called  `Dockerfile`.
This recipe is a textual description of all commands that need to be executed to recreate the image.
Such files are tiny (the `r link("https://github.com/aaronpeikert/repro-tutorial/blob/main/Dockerfile", "Dockerfile")` for this project is `r round(as.numeric(fs::file_size("Dockerfile"))/1024^1, 2)`Kb big) and are an unambiguous description of how to recreate a given container image.
However, they rely on the assumption that all software repositories will remain accessible and will continue to make available historic software versions.
For archiving, we therefore recommend that a full image is always kept (see Section on [Archiving Docker]).

To summarize, the workflow by @Peikert2019 requires four components (see Figure \@ref(fig:schematic)), dynamic document generation (using R Markdown), version control (using Git), internal dependency management (using Make), containerization (using Docker).
While R Markdown and git are well integrated into the R environment through R Studio, Make and Docker require a level of expertise that is often beyond the training of scholars outside studies of information technology, which represents a considerable hurdle in the acceptance and implementation of the workflow.
To remove this hurdle, we have developed a new R package `repro` that supports scholars in setting up, maintaining, and reproducing research projects in R.
In the remainder, we will walk you through a complete research project, from sketching the very first idea over preregistration, data analysis, and submission of a preprint while focusing on how the `repro`-package enables users to maintain reproducibility of all steps.

```{r schematic, eval = TRUE, echo = FALSE, fig.cap="Schematic illustration of the interplay of the four components (in dashed boxes) central to the reproducible workflow: version control (Git), dependency tracking (Make), software management (Docker), and dynamic document generation (R Markdown). Git tracks changes to the project over time. Make manages dependencies among the files. Docker provides a container in which the final report is built using dynamic document generation in R Markdown. Reproduced from @Peikert2019.  "}
# file gets downloaded in Makefile
knitr::include_graphics(here::here("images/nutshell.svg"), auto_pdf = TRUE)
```

# Tutorial

A significant hurdle for adopting the before-mentioned tools for researchers is that they are typically used through a command-line interface (CLI).
The CLI predates and coexists with graphical user interfaces (GUIs) which many users (including the authors) prefer over the text-based interactions of a CLI for most tasks.
A GUI has the apparent disadvantages for reproducibility that it requires a user who clicks on the correct buttons in the correct order.
Depending on the number of steps, this is not only much effort but also error-prone.
This tutorial seeks a compromise.
We use simpler alternatives, including a GUI where we can, to create a reproducible workflow, but the reproduction of such workflow does not rely on manual steps.
At several points, we rely on or have self-developed a CLI that wraps a more complicated (and powerful) CLI.
These wrappers use standard CLI tools but provide feedback for the user about what the computer does and what the user should do in layman's terms and are accessible from within R.
We hope this makes reproducibility tools more accessible by enabling untrained users to detect their systems state accurately and act correspondingly [@parasuramanAutomationHumanPerformance2018, Chapter 8: "Automation and Situation Awareness"].
These alternative tools are merely an assistance system, and as users get more and more comfortable, they can use the underlying tools directly to solve more complex problems.
Please install the package [`gert`](https://github.com/r-lib/gert)[@R-gert] which wraps Git commands and [`repro`](https://github.com/aaronpeikert)[@R-repro] so they can act as assistance systems while you follow the tutorial:

```{r, eval=FALSE}
install.packages("gert")
# repro is not on CRAN yet
install.packages('repro', repos = 'https://aaronpeikert.r-universe.dev')
```

One example for such assistance are the check functions from `repro`.
They verify that you indeed have installed and setup the required software for this workflow:

```{r}
# `package::function()` â†’ use function from package without `library(package)`
repro::check_git()
repro::check_make()
repro::check_docker()
```

They detect the users' system state and explain what this means to the user.
Sometimes they ask the user to do something, e.g. the following happens if you were a windows user who does not have installed Git:

```{r, include=FALSE}
opts <- options()
options(repro.os = "windows",
        repro.git = FALSE)
```

```{r}
check_git()
```

```{r, include=FALSE}
options(opts)
```

The messages emitted by `repro` and other packages we present try to take the user by the hand to help them solve problems.
This tutorial describes steps that we expect to apply to all users; the messages from the packages fill the gap to your specific situation and extend the tutorial in this way.
Before you continue, we ask you to run the above commands to check Git, Make, and Docker.
If necessary, follow the instructions of repro to install them.

The first concept we want to apply practically is version control with Git.
Git tracks files within one project folder.
We start by creating such project folder with RStudio:

> File â†’ New Project... â†’ New Directory â†’ Example Repro Template

This creates a project containing an example analysis, but you may use any other template or existing R Project.

You can prevent Git from accidentally committing files that you do not want to track with the `.gitignore` file.
You can add something to the `.gitignore` file directly or with this command:

```{r, eval=FALSE}
usethis::use_git_ignore("private.md")
```

Now the file `private.md` will not be committed to Git and will not be made public.

After you have ignored sensitive files (like not yet anonymized data), you can activate Git with:

```{r, eval=FALSE}
usethis::use_git()
```

If you made changes to a file, you have to stage and then commit these changes.
The easiest way is to use the RStudio Git pane.
You click on the empty box next to the file you want to stage, and when a tick appears, the file is staged.
After you have staged all files you want, click on the commit button, explain curtly why you made those changes in a commit message and then click on commit.

The files you created and the changes you made still have not left your computer.
To back up and share the files, you can publicly upload those changes to GitHub with:

```{r, eval=FALSE}
usethis::use_github()
```

Depending on your computer configuration, this may ask to set up a secure connection to GitHub. In this case first follow its suggestions.

If you are not yet ready to share your project publicly use:

```{r, eval=FALSE}
usethis::use_github(private = TRUE)
```

Now that you have created a project that is under version control, we turn to dynamic document generation.
Three aspects are essential to understand a dynamic document:

1. Text
2. Code
3. Metadata

The text of an R Markdown is written in Markdown.
The following example shows how to signify a header, a bold word and a list in Markdown:

```markdown
<!--this is a Markdown file -->
# Header

Normal text.
Important **word** in bold.

To do list:

* do research
* do more research
* spend time with family
```

Code is separated by three backticks (and the programming language in curly braces) like here:

````markdown
This is normal text, written in Markdown.

`r ''````{r}
# this is R code
1 + 1
```
````

Now that you know how to write text and R code in an RMarkdown, you need to know about metadata, like the title or the output format. Metadata are placed at the beginning of the document and are separated by three dashes; the following example is a full markdown document where the header is in lines 1-6.

````markdown
---
title: A Hitchhiker's Guide to Reproducible Research in R
author: Aaron Peikert, Caspar J. Van Lissa and Andreas M. Brandmaier
abstract: A paper about recalculating the same thing more than once.
output: html_document
---

# Introduction

Important for reproducibility:

1. Version Control
2. Dynamic Document Creation
3. Dependency Tracking
4. Software Management

`r ''````{r}
# this is R code
t.test(extra ~ group, data = sleep)
```
````

Most metadata fields are self-explanatory (like the author field), but any package may use the metadata for their purposes.
Often, for example, does a specific output format require more metadata, i.e. the `rticles`-package for authoring scientific manuscripts.
RMarkdown was extended to hundreds of document formats which are often highly configurable by their metadata.
The `repro` package uses the metadata, not to change the output, but to make the document more reproducible, simply by listing resources, i.e., scripts, data, and packages, your document depends on, in the following format:

```markdown
---
title: A Hitchhiker's Guide to Reproducible Research in R
author: Aaron Peikert, Caspar J. Van Lissa and Andreas M. Brandmaier
abstract: A paper about recalculating the same thing more than once.
output: html_document
repro:
  scripts:
    - R/load.R
  data:
    - data/mtcars.csv
  packages:
    - tidyverse
    - usethis
    - gert
---
```

This information makes internal dependencies to other files in the project and external dependencies to R packages explicit.
`repro` can use this information to infer a Makefile for the dependencies on other files and a Dockerfile that includes all required packages.
In day-to-day work the simplest solution is to use the function `repro::automate()` which both infers a Makefile and a Dockerfile from all RMarkdown  (all files with the ending `.Rmd`) and there respective repro metadata.
Every time you change the `repro`-metadata, change the output format, or add a new RMarkdown, the Makefile and Dockerfile are potentially outdated, and you should run `repro::automate()` to keep them up to date.
The function `repro::automate()` does not overwrite manual changes, and there is no harm in running it too often.
Besides the main Makefile and the Dockerfile repro generates a few more files in the `.repro` directory (which we will explain in detail later), all of which you should add and commit to Git.

When you or people who received your project from GitHub want to reproduce the project, they can ask repro to explain how this should be done:

```{r}
repro::rerun()
```

However, users do not have to have repro installed, in fact, they do not even need R installed. 
The two Make commands `make docker` and `make -B DOCKER=TRUE` together with an installation of Make and Docker are sufficient.
They can be pasted into a terminal and will reproduce the project in many cases.
Some projects, however, are not as easy to make reproducible.
As explained above, repro is merely a simplified interface.
These simplifications are possible because of two restrictions.
First, the project can only rely on R packages for the Dockerfile and second, it can have no multilayered preprocessing for the Makefile.
Lifting these restrictions requires the user to interact more directly with Make or Docker.
To satisfy more complicated requirements users need to understand how repro utilizes Make and Docker internally.

Let us dissect the command for reproducing a repro project: `make docker && make -B DOCKER=TRUE`; it consists of two steps.
First it virtually recreates the software environment and then it executes computational recipes in the virtual software environment.
The first step is represented by the command `make docker`.
The command `make docker` will trigger Make to build the target `docker`.
The recipe for this target builds an image from your Dockerfile.
The `&&` only runs the second command if the first was successful.
Therefore, the computational steps are only executed, when the software environment is set up.
The second step and actual reproduction is again a call to Make in the form of `make -B DOCKER=TRUE` with three noteworthy properties.
First, a call to `make` without any explicit target will build the Make target `all`.
Second, the flag `-B` means that Make will consider all prerequisites as outdated and hence will rebuild everything.
Third, repro constructs Make targets so that if you supply `DOCKER=TRUE` they are executed within the Docker image of the project.

This interplay between Docker and Make is so complicated because it resembles a chicken or egg problem.
We have computational steps (Make) which depend on the software environment (Docker) for which we again have computational steps that create it.
Users only require a deeper understanding when they either want to have more complex computational recipes than rendering an R Markdown or require other software than R packages.

Users can have full control over the software installed within the image of the project.
`repro` creates three Dockerfiles inside the `.repro` directory.
The file `.repro/Dockerfile_base` contains information about the base image on which all the rest is build.
By default we rely on the "verse" images provided by the Rocker project.
These contain (among other) the packages `tidyverse`, `rmarkdown` and a Latex installation, which make them ideal for writing scientific manuscripts.
Users can choose which R version they want by changing the version number in line 1 to the desired R version number.
By default the R version corresponds to the version on which `repro::automate()` was called the first time.
The build date is used to install packages in the version which was available on CRAN on this specific date and can also be changed, by default this date is set to the date on which `repro::automate()` was called the first time.

```{r, results='asis', echo=FALSE}
cat(
  "```bash",
  readLines(here::here(".repro", "Dockerfile_base"))[1:3],
  "```",
  sep = "\n"
)
```

The file `.repro/Dockerfile_manual` is by default empty and is supposed to allow dependencies outside of R, like system libraries or external software.
The most convinient way to install software on Ubuntu is through its package manager `apt`.
This snipped installs python:

```bash
RUN apt-get update && apt-get install -y python3
```

The the last Docker, `.repro/Dockerfile_packages`, should not be manually changed.
`repro::automate()` gathers all packages from all `.Rmd` files and figures out if they should be installed from CRAN or GitHub.
`repro::automate()` also simply combines the three Dockerfiles and saves the result into the main `Dockerfile` at the top level of the project.
The seperation into several Dockerfiles is only to provide a clear seperation between automatically generated parts and manually altered parts of the Dockerfile.
However, Docker requires in the end the one Dockerfile that `repro::automate()` creates from them.

# Preregistration as Code 

We argue that a preregistration as code (PAC) is an excellent planning tool that offers several benefits over traditional preregistration.
For a PAC, researchers write a reproducible, dynamically generated manuscript including the sections Introduction, Methods, and Results before they gather data using simulated data.

We consider three criteria when planning a study.
First, the plan should be comprehensive, e.g. state the research question and describe the study design and analysis.
Second, the plan should be effective, meaning that the researchers can reasonably expect it to answer the research question at hand.
Third, the plan should be efficient; therefore, it only employs the necessary amount of resources.

To ensure a comprehensive plan, we suggest that researchers borrow from widely employed standards for writing an empirical manuscript.
The introduction and theoretical background provide the basis for the studies design and can be written before gathering data.
We argue that researchers can plan more effectively if they formulate the planned analysis as a method section that again follows best practices for describing empirical research.
To ensure that the analysis is technically feasible, they can translate their method section into executable computer code for the data analysis using simulated data.
The researchers can include a "faked " results section based on the simulated data, employing dynamic document generation.
Repeated simulation with varying parameters, a so-called Monte Carlo simulation, can then be employed to plan resources to assure efficiency.
For example, researchers can assess the power (the probability to detect an effect of a given size) for varying numbers of observations in the simulation to determine the required sample size for their study.

Defining the research questions and planning data analysis before observing the research outcomes is called preregistration [https://www.pnas.org/content/115/11/2600].
Preregistration increases the credibility of empirical results in three ways [preregistration is hard and worthwhile, Nosek].

> First, preregistration of analysis plans makes clear which analyses were planned a priori and which were conducted post hoc. This improves calibration of uncertainty for unplanned analyses, and diagnosticity of statistical inferences for planned analyses. [...] Doing so has the benefit of strengthening statistical inferences as compared with unplanned analyses. [...] Second, preregistration enables detection of questionable research practices such as selective outcome reporting (John et al. 2012) or Hypothesizing After the Results are Known (HARKing; Kerr, 1998). Third, preregistration of studies can reduce the impact of publication biasâ€”particularly the prioritization of publishing positive over negative resultsâ€”by making all planned studies discoverable whether or not they are ultimately published.

A preregistration as code offers four advantages over classical preregistration.
First, a PAC removes any ambiguity regarding the translation of an analysis plan into code.
Second, despite being rigorous, it offers the flexibility to incorporate data-dependent decisions if they can be formulated as code.
Researchers can, for example, formulate conditions under which they prefer one analysis over the other, i.e. if distributional assumptions are not met, employ robust methods or automated variable selection mechanisms.
Third, deviations from the preregistration are more explicit because they are reflected in changes to the codes a property facilitated by version control.
Fourth, the preregistration is merely a development stage of the final manuscript, thus saving the researchers from writing and the reviewer from evaluating two separate documents.

# Preregistration as Code --- an example

To guide the reader through a preregistration as code (PAC), we will use an exemplary research question:

> "Is there a mean difference in the personality trait 'Machiavellism' between self identified females and males?"

This example serves only didactic purposes and not to derive substantive claims.
Nevertheless, everything else is kept as close to a real research question as possible.

At the heart of a PAC lies the planned analysis.
We suggest writing a function that captures the full process of testing a research question that receives a dataset as input.
For the hypothetical research question, we could for example plan to conduct a simple Student's t-test [@studentProbableErrorMean1908] with Welch's correction [@welchGeneralizationStudentProblem1947] an the average of machiavillism items of the Short Dard Triad [SD3, @Jones2013] between the gender groups.
As an example of a data dependent decisions we included a rank transformation if the skewness greater than 1, converting the t-test to a supposedly more robust Mannâ€“Whitneyâ€“Wilcoxon test.[^caution-mww]
This analyitical strategie is suboptimal with regard to several criteria (untested assumption of measurement invariance [@putnickMeasurementInvarianceConventions2016], underestimation of effect size in presence of measurement error [@frostCorrectingRegressionDilution2000], overestimation of effect size for highly skewed distributions [@stonehouseRobustnessTestsCombined1998]); we merely choose it for didactic purposes because similar procedures a widely used [@fagerlandTtestsNonparametricTests2012; @hortonStatisticalMethodsJournal2005 reports that 26% of all studies employ a t-test and 27% employ a rank based alternative in the New England Journal of Medicine in 2005].

[^caution-mww]: The Mannâ€“Whitneyâ€“Wilcoxon is **not** more robust than the Welch corrected t-test for reasonably large sample sizes. The interested reader can use the provided code for the simulation to verify that the t-test provides unbiased effect sizes but the Mannâ€“Whitneyâ€“Wilcoxon overestimates effect size with increasing sample size and skewness.

```{r planned_analysis}
```

To use this function one needs a compatible dataset.
At the time of preregistration there is of course no actual data available.
We therefore simulate a dataset.

```{r simulate_data}
```

We decided to draw non-normal data to test how well our procedure performed under non-optimal situations.
Specifically, we draw from a $\chi^2$-distribution, where in one group we add difference that conforms to a standardized effect size.
This simulated dataset fits perfectly into our planned analysis:

***

These results are based on simulated data.

```{r, results='asis'}
set.seed(112358) # fix state of RNG
report::report_text(
  planned_analysis(
    simulate_data(
      n = 400,# Sample size
      df = 8, # Chisq with DF = 8 has skewness = 1
      d = .2, # Cohen's D
      i = 10  # Number of items (should not effect anything)
    )
  )$test
)
```

***

```{r, echo=FALSE}
res <-
  read_csv(here::here("data", "simulation_results.csv"), col_types = "ddddddd")
choosen_power <- .8
choosen_d <- .2
minn <- filter(res, d == choosen_d, power > choosen_power) %>% 
  filter(n == min(n))
```

To determine a minimal sample size for this study we repeat this simulation across several combination of effect sizes and sample sizes.
One standard recomendations for choosing a sample size is to have an expected power of 80% based on the expected effect size.
@Jones2013 found in three studies gender differences of .24, .29, and .35 for their machiavilism subscale.
If we want to detect an effect of size .2 with an 80% probability, our simulation shows that we require at least `r minn$n` participants.

```{r, echo=FALSE}
res %>%
  ggplot(aes(n, power, color = d, group = d)) +
  geom_line() +
  geom_point(
    data = minn,
    size = 3,
    color = "black",
    shape = 3
  ) +
  theme_minimal() +
  scale_color_viridis_c() +
  theme(legend.title = element_text()) +
  labs(y = "Power\n(Proportion of significant results)",
       x = "Sample size",
       color = "Cohen's D") +
  NULL
```


# References

```{r, include=FALSE}
knitr::write_bib(
  c(
    .packages(),
    "repro",
    "here",
    "rticles",
    "gert",
    "bookdown",
    "lavaan",
    "knitr",
    "targets",
    "renv",
    "tidyverse"
  ),
  here("packages.bib")
)
cat(c(readLines(here::here("packages.bib")), "\n", readLines(here::here("references.bib"))), sep = "\n", file = "temp.bib")
```
