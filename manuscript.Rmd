---
title: "A Hitchhiker's Guide to Reproducible Research" # replace with something serious
output: bookdown::pdf_document2
repro:
  packages:
    - usethis
    - repro
    - here
csl: apa7.csl
bibliography: references.bib
---

<!-- the HTML comments, like this one, are meta comments, mainly describing the intent --->
<!-- each sentence below a heading summarizes what I want to say there --->
<!-- "Hands-on:" means concrete practical application, they roughly proceed from easy/familiar to hard/unfamiliar-->

```{r setup, include=FALSE}
library(repro)
source(here::here("R", "link.R"))
```

# Why you should care about reproducibility

<!-- define reproducibility -->
<!-- is a direct quote from my master thesis -->
<!-- I wouldn't want to cite it. @brandmaier, would you please rewrite? -->
A reproducible scientific product allows other researchers to obtain the same results from the same dataset in a way that enables substantive criticism and therefore facilitates replication.
<!-- lure the reader in -->
This notion of independent verifiability has long been part of "good scientific practice", but has not reached its full potential to benefit the scientific community.
<!-- expand: who is calling it a requirement, maybe metascientific arguments-->
Reproducing an empirical study is often tedious and frustrating work for the researcher.
However, if reproduction would require mere minutes instead of hours, it could greatly facilitate collaboration within and across research projects.
<!-- Guiding principle: spending machine compute time instead of human research time -->
To archive this boost, we must place the burden of reproducing something upon computers instead of human researchers.
A scientific document should therefore be understood by researchers but reproduced by computers.

This tutorial walks through the creation of such a document that can be reproduced automatically.
Though reproducibility is but a small part of the research process, it can serve as the mortar of open science building blocks (e.g. preregistration, open data, postpublication review).
Therefore, we show the whole lifecycle of an open research project, where some parts are essential to archive reproducibility.
Still, the reader can safely skip sections marked as optional, when unfit for their research question or methods.
However, any empirical data analysis will suffer from the following threats to reproducibility:
<!-- Which problems are solved by which tool (like in Andreas talk)-->
<!--stolen from: https://brandmaier.github.io/reproducible-data-analysis-materials/KULeuvenQuantPsy2020.html#11 -->

1. Multiple versions of scripts/data (e.g., the dataset has changed over time, i.e., was further cleaned or extended)
2. Copy&paste errors (e.g., inconsistency between reported result and reproduced result)
3. unclear which scripts should be executed in which order
4. Broken software dependencies (e.g., analysis broken after an update, missing package)

It is common to rely on the craftiness of the researcher to debug these problems, but we could prevent them altogether with:

1. Version control
2. Dynamic document creation
3. Dependency tracking
4. Software management

We deem each concept necessary to archive longterm reproducibility.
However, which tools you use to implement these concepts is a matter of taste and project requirements.
This tutorial will follow the recommendations of @Peikert2019 and utilizes Git for version control, RMarkdown for dynamic document creation, Make for dependency tracking, and Docker for software management.
Their interplay is shown in Figure \@ref(fig:schematic).
At several points, we suggest other viable alternatives.

```{r schematic, eval = TRUE, echo = FALSE, fig.cap="Schematic illustration of the interplay of the four components (in dashed boxes) central to the reproducible workflow: version control (Git), dependency management (Make), software management (Docker), and dynamic document generation (R Markdown). Git tracks changes to the project over time. Make manages dependencies among the files. Docker provides a container in which the final report is built using dynamic document generation in R Markdown. Reproduced from @Peikert2019.  "}
# file gets downloaded in Makefile
knitr::include_graphics("images/nutshell.svg", auto_pdf = TRUE)
```

# Setup

<!-- keep it brief, let `repro` do the work -->

We assume that you have already installed R and RStudio (if not check the `r link("https://github.com/aaronpeikert/repro-tutorial/blob/master/install.md", "installation guide")`).
Additionally, you'll need the [`repro`-package](https://github.com/aaronpeikert/repro)[^repropackage]:

[^repropackage]: https://github.com/aaronpeikert/repro

```r
# -- type this on the R console --
if(!requireNamespace("remotes"))install.packages("remotes")
remotes::install_github("aaronpeikert/repro")
library("repro")
```

The [`repro`-package](https://github.com/aaronpeikert/repro) contains several helpers to work with reproducibility tools.
For example, it can check if your computer is set up to use the tools we rely on:

```{r}
# -- type this on the R console --
check_git()
```

```{r}
# -- type this on the R console --
check_make()
```

```{r}
# -- type this on the R console --
check_docker()
```

If these commands detect that something is not installed or set up, they will give you detailed instructions (tailored to your operating system) to remedy the situation.
Follow the instructions, then rerun the command, until it tells you not to worry.

# Planing

<!-- researcher should begin early to enjoy most benefits -->
It can be tempting to leave considerations of reproducibility for when you finish a project.
<!-- weird sentence, I'll leave it as a placeholder-->
However, reproducibility is invaluable for collaboration---including with the version of yourself from a few weeks ago.
Therefore, we suggest starting early, ideally from the moment you decide to pursue a project to incorporate tools to help you archive reproducibility gradually.
The first step is to organize all files in one folder.
This way, you never lose anything, and you can easily share the files.
<!-- Hands-on: Introducing RProjects-->
RStudio's RProjects facilitate this concept of a project folder.
To create one, click on:

> File â†’ New Project... â†’ New Directory â†’ New Project

<!--Keep notes.-->
<!-- Hands on: Introduce Markdown -->
A great tool to capture ideas or meeting notes is Markdown.
To create a Markdown document click on: 

> File â†’ New File â†’ Markdown File

Markdown is simply text, where some characters let you add a minimal amount of structure:

```{r, results='asis', echo=FALSE}
# in case we want to reuse this example later
markdown_example <- "
<!--this is a Markdown file -->
# Header
Normal text.
Important **word**.

To do list:

* do research
* do more research
* spend time with family
"
cat("``` markdown", markdown_example, "```", sep = "")
```

We will later see how this simplicity enables us to create many different document formats (i.e. Word, PDF, HTML) and incorporate code (i.e. R, Python, Julia).
Another advantage of Markdown is that you can easily version-control it with Git.
You may be familiar with Microsoft Words' "Track Changes"-Feature and version control is similar in its basic idea but more potent in its features.
Instead of tracking only one file, you can keep track of the whole project directory, and instead of losing all changes when you agree to them, the entire history is preserved transparently.
For example this paper has been going through `r length(git2r::commits())` `r link("https://github.com/aaronpeikert/repro-tutorial/commits/master", "iterations")`.
To activate Git in a given project, you can call:
<!-- Hands-on: Introduce Git -->

```r
# `package::function()` â†’ use function from package without `library(package)`
usethis::use_git()
```

Recording each change you made helps you to iterate more quickly because you know that you can effortlessly go back to previous versions.
For example, you can write down a rough project idea, knowing your collaborators and you will iterate and improve.
<!-- This is maybe the place to introduce the example-->
First, you could create a new markdown file named `idea.md` that looks like this:

```markdown
<!--this is a Markdown file -->

# Hypothesis

Machiavellianism is higher in male persons.

# Analytic Strategy

t-test

# Sample

Could we use openpsychometrics.org data?
```

And you could commit it in Git:
<!--I am leaning towards doing everything in R, not sure though-->
<!--For Git we maybe should use RStudio's Git pane-->

```r
git2r::add(".", "idea.md")
git2r::commit(".", "add a first concept")
```

But then you realize that the new literature suggests that there could be a bias in measurement:

``` markdown
<!--this is a Markdown file -->

# Hypothesis

Machiavellianism is higher in male persons.

# Analytic Strategy

Multigroup CFA + Measurement Invariance

# Sample

Could we use openpsychometrics.org data?
```

Commit this version:

```r
# -- type this on the R console --
git2r::add(".", "idea.md")
git2r::commit(".", "exclude bias in measurement as possible confounder")
```

And Git can show you what has changed, like in Figure \@ref(fig:idea-change).

```{r idea-change-screenshot, include=FALSE}
# unfortunately css selectors won't work for pdf so we'll use png instead
idea_change <- here::here("images", "idea-change.png")
webshot2::webshot("https://github.com/aaronpeikert/repro-tutorial/commit/5b3f4641ff542158184f85d458880c35e8f09f3c?diff=split",
                  file = idea_change,
                  selector = "div#diff-2f8beaa39e5c98706d8abb3361a8383776f8df8a195a21c0e6eeb25e5aa48f45", # only select file change
                  zoom = 2)# higher resolution
```


```{r idea-change, echo=FALSE, fig.cap="A screenshot of how a change tracked in Git is represented by GitHub.  ", out.width='100%'}
knitr::include_graphics(idea_change)
```

<!--I am troubled by how we should recomend to learn git-->
We recommend to attend an online lecture or workshop to learn the ins and outs of Git.

### Simulation (optional)

<!-- impress with neatness â†’ sample size planning, preregistration and analysis in one ðŸ˜Ž-->
<!-- but explain that good science is often not as neat, strive for the ideal -->

```{r, include=FALSE}
source(here::here("R", "simulation.R"))
```

There are probably few things as frustrating as realizing that the data that you gathered cannot answer the research question you had in mind.
A step to prevent unpleasant surprises is to simulate data.
Simulated data lets you verify that the data you expect to gather and the analysis that you plan fit together.
Of course one has to make a lot of educated guesses.
Generally there are of course hundreds of different equal possibilities, but we just care about the worst reasonable settings.
Sticking with our example about machiavilism, we simulate multivariate normal data representing the machiavilism subscale of the short dark triad (SD3).
As a starting point we can use already publishes information.
@Jones2013 report loadings seen in Table \@ref(tab:loadings) and report a gender difference (at the manifest level) depending on the sample between `r min(cohend_jones_paulhus)` and `r max(cohend_jones_paulhus)`.

```{r loadings, echo=FALSE}
knitr::kable(data.frame(Item = 1:9,
                        Loading = loadings_jones_paulhus),
             caption = "Factor loadings from an Exploratory Structrural Equation Model as reported in Jones \\& Paulhus (2013)")
```

It is probably better to assume values that are more pessimistic.
Instead of taking these values as they are we will therefore assume that the loadings are 30% lower than reported and that the standardized mean difference is only 0.1.
These assumption can be simply translated into `lavaan` model syntax:

```{r, results='asis', echo=FALSE}
model_truth <- paste(measurement(
  "MACH",
  paste0("x", 1:9),
  loadings = round(loadings_jones_paulhus * 0.7, 2)
),
intercepts("MACH", list(0, 0.1)),
collapse = "\n", sep = "\n")
cat("``` r",
    model_truth,
    "```",
    sep = "\n")
model <- paste(measurement(
  "MACH",
  paste0("x", 1:9)),
  intercepts("MACH", list(0, "NA")),
  sep = "\n")
```

From which we can simulate data and fit a model to it:

```{r, results='hide'}
data <- lavaan::simulateData(model_truth, "cfa", sample.nobs = c(20000, 20000), meanstructure = TRUE, standardized = TRUE)
fit <- lavaan::cfa(model, data = data, group = "group", meanstructure = TRUE, std.lv = TRUE, group.equal = c("loadings", "intercepts")) 
lavaan::summary(fit)
```


<!--Hands-on: build simple functions tailored for your analysis-->

### Preregestration (optional)

A preregistration is a tool that may help to increase the credibility of empirical results.

<!--Hands on: Introduce RMarkdown-->
<!--Hands on: gitignore resulting .html/pdf-->
<!-- A preregistration needs to be public -->

## Prepare public release (optional)

<!--Hands on: Add a readme-->
```r
usethis::use_readme_rmd()
```

<!--Hands on: Add a license-->
```r
usethis::use_cc0_license()
```

<!--Hands on: Add a code of conduct-->
```r
usethis::use_code_of_conduct()
```

## Collaboration

### GitHub

GitHub/GitLab are great as a shared information basis: sharing and editing files, discussing and distributing tasks via issues and documenting the project via wikis even before data collection starts.

<!--Hands-on: Introduce Github-->

```r
usethis::use_github()
```

<!--Hands on: Introduce GitHub Issues-->
<!--Hands on: Introduce GitHub Projects-->

### Internal Dependencies

A well defined "entry point" that automatically reproduces everything eases collaboration.

<!--I am not sure this is the right place, should we introduce this earlier? -->

<!--Hands on: Introduce Make-->
<!--Hands on: Add targets for README.Rmd, preregistration.Rmd -->

``` r
automate_make()
```

### External Dependencies

A considerable number of external dependencies may hinder collaboration and long term reproducibility, but automated solutions can "bundle" and fix dependencies.

<!-- Hands on: Introduce Docker -->

``` r
automate_docker()
```

<!-- after this everything is optional? -->

#### renv (alternative)

#### Singularity (optional)

## Analyzing Data

<!-- Hands on: add data via automate() -->
<!-- Hands on: automatically download data via manually editing the Makefile -->

### Data Integrity (optional)

<!-- Hands-on: check changed hash -->

### Anonymising Data (optional)

<!-- Should we add a section on synthetic data generation? -->
<!-- Hands on: Create synthetic data -->

## Releasing a Preprint (optional)

<!-- Hands on: add commit hash to results-->
<!-- Hands on: git tag + github release-->

## Peer Review (optional)

## Post Publication Review (optional)

## Archiving

<!-- Hands on: explain how to archive data-->
<!-- Hands on: explain how to archive docker images-->

## Checklist

<!-- Should we design a checklist? -->

```{r, results='asis', echo=FALSE}
link_index()
```

# References

