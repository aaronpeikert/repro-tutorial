@article{Peikert2019,
  doi = {10.31234/osf.io/8xzqy},
  url = {https://doi.org/10.31234/osf.io/8xzqy},
  year = {2019},
  month = nov,
  publisher = {Center for Open Science},
  author = {Aaron Peikert and Andreas Markus Brandmaier},
  title = {A Reproducible Data Analysis Workflow with R Markdown,  Git,  Make,  and Docker}
}

@article{Jones2013,
  doi = {10.1177/1073191113514105},
  url = {https://doi.org/10.1177/1073191113514105},
  year = {2013},
  month = dec,
  publisher = {{SAGE} Publications},
  volume = {21},
  number = {1},
  pages = {28--41},
  author = {Daniel N. Jones and Delroy L. Paulhus},
  title = {Introducing the Short Dark Triad ({SD}3)},
  journal = {Assessment}
}

@article{stoddenEnhancingReproducibilityComputational2016,
  Author = {Stodden, Victoria and McNutt, Marcia and Bailey, David H. and Deelman, Ewa and Gil, Yolanda and Hanson, Brooks and Heroux, Michael A. and Ioannidis, John P. A. and Taufer, Michela},
  Date = {2016-12-09},
  Doi = {https://doi.org/10.1126/science.aah6168},
  Eprint = {27940837},
  Eprinttype = {pmid},
  Issn = {0036-8075, 1095-9203},
  Journaltitle = {Science},
  Langid = {english},
  Number = {6317},
  Pages = {1240-1241},
  Title = {Enhancing Reproducibility for Computational Methods},
  Url = {https://science.sciencemag.org/content/354/6317/1240},
  Urldate = {2019-08-23},
  Volume = {354},
  Bdsk-Url-1 = {https://science.sciencemag.org/content/354/6317/1240},
  Bdsk-Url-2 = {https://doi.org/10.1126/science.aah6168}
}

@report{stoddenEnablingReproducibleResearch2009,
  Author = {Stodden, Victoria},
  Date = {2009-03-03},
  Institution = {{Social Science Research Network}},
  Keywords = {Enabling Reproducible Research: Open Licensing for Scientific Innovation,SSRN,Victoria Stodden},
  Langid = {english},
  Location = {{Rochester, NY}},
  Number = {ID 1362040},
  Shorttitle = {Enabling {{Reproducible Research}}},
  Title = {Enabling {{Reproducible Research}}: {{Open Licensing}} for {{Scientific Innovation}}},
  Type = {SSRN Scholarly Paper},
  Url = {https://papers.ssrn.com/abstract=1362040},
  Urldate = {2019-08-29},
  Bdsk-Url-1 = {https://papers.ssrn.com/abstract=1362040}
}

@online{knuthCWEBSystemStructured,
  title = {The {{CWEB System}} of {{Structured Documentation}}},
  author = {Knuth, Donald E. and Levy, Silvio},
  url = {https://www-cs-faculty.stanford.edu/~knuth/cweb.html},
  urldate = {2021-04-21},
  date = {2011}
}

@book{riedererChapter15Other,
  title = {Chapter 15 {{Other Languages}} | {{R Markdown Cookbook}}},
  author = {Riederer, Christophe Dervieux, Emily, Yihui Xie},
  url = {https://bookdown.org/yihui/rmarkdown-cookbook/},
  urldate = {2021-04-21},
  isbn = {978-0-367-56383-7},
  date = {2020}
}


@article{nowokSynthpopBespokeCreation2016,
  title = {Synthpop: {{Bespoke Creation}} of {{Synthetic Data}} in {{R}}},
  shorttitle = {Synthpop},
  author = {Nowok, Beata and Raab, Gillian M. and Dibben, Chris},
  date = {2016-10-28},
  journaltitle = {Journal of Statistical Software},
  volume = {74},
  pages = {1--26},
  issn = {1548-7660},
  doi = {10.18637/jss.v074.i11},
  url = {https://www.jstatsoft.org/index.php/jss/article/view/v074i11},
  urldate = {2021-04-26},
  issue = {1},
  keywords = {CART,disclosure control,R,synthetic data,UK longitudinal studies},
  langid = {english},
  number = {1}
}

@misc{zenodo,
  doi = {10.25495/7GXK-RD71},
  url = {https://www.zenodo.org/},
  author = {{European Organization For Nuclear Research} and {OpenAIRE}},
  keywords = {FOS: Physical sciences, Publication, Dataset},
  language = {en},
  title = {Zenodo},
  publisher = {CERN},
  year = {2013}
}

@article{rougierSustainableComputationalScience2017,
  title = {Sustainable Computational Science: The {{ReScience}} Initiative},
  shorttitle = {Sustainable Computational Science},
  author = {Rougier, Nicolas P. and Hinsen, Konrad and Alexandre, Frédéric and Arildsen, Thomas and Barba, Lorena A. and Benureau, Fabien C. Y. and Brown, C. Titus and de Buyl, Pierre and Caglayan, Ozan and Davison, Andrew P. and Delsuc, Marc-André and Detorakis, Georgios and Diem, Alexandra K. and Drix, Damien and Enel, Pierre and Girard, Benoît and Guest, Olivia and Hall, Matt G. and Henriques, Rafael N. and Hinaut, Xavier and Jaron, Kamil S. and Khamassi, Mehdi and Klein, Almar and Manninen, Tiina and Marchesi, Pietro and McGlinn, Daniel and Metzner, Christoph and Petchey, Owen and Plesser, Hans Ekkehard and Poisot, Timothée and Ram, Karthik and Ram, Yoav and Roesch, Etienne and Rossant, Cyrille and Rostami, Vahid and Shifman, Aaron and Stachelek, Joseph and Stimberg, Marcel and Stollmeier, Frank and Vaggi, Federico and Viejo, Guillaume and Vitay, Julien and Vostinar, Anya E. and Yurchak, Roman and Zito, Tiziano},
  date = {2017-12-18},
  journaltitle = {PeerJ Computer Science},
  shortjournal = {PeerJ Comput. Sci.},
  volume = {3},
  pages = {e142},
  publisher = {{PeerJ Inc.}},
  issn = {2376-5992},
  doi = {10.7717/peerj-cs.142},
  url = {https://peerj.com/articles/cs-142},
  urldate = {2021-05-03},
  langid = {english}
}





@article{obels2020,
  title={Analysis of open data and computational reproducibility in registered reports in psychology},
  author={Obels, Pepijn and Lakens, Daniel and Coles, Nicholas A and Gottfried, Jaroslav and Green, Seth A},
  journal={Advances in Methods and Practices in Psychological Science},
  volume={3},
  number={2},
  pages={229--237},
  year={2020},
  publisher={SAGE Publications Sage CA: Los Angeles, CA}
}


@article{hardwicke2018,
author = {Hardwicke, Tom E.  and Mathur, Maya B.  and MacDonald, Kyle  and Nilsonne, Gustav  and Banks, George C.  and Kidwell, Mallory C.  and Hofelich Mohr, Alicia  and Clayton, Elizabeth  and Yoon, Erica J.  and Henry Tessler, Michael  and Lenne, Richie L.  and Altman, Sara  and Long, Bria  and Frank, Michael C. },
title = {Data availability, reusability, and analytic reproducibility: evaluating the impact of a mandatory open data policy at the journal <i>Cognition</i>},
journal = {Royal Society Open Science},
volume = {5},
number = {8},
pages = {180448},
year = {2018},
doi = {10.1098/rsos.180448},

URL = {https://royalsocietypublishing.org/doi/abs/10.1098/rsos.180448},
eprint = {https://royalsocietypublishing.org/doi/pdf/10.1098/rsos.180448}
,
    abstract = { Access to data is a critical feature of an efficient, progressive and ultimately self-correcting scientific ecosystem. But the extent to which in-principle benefits of data sharing are realized in practice is unclear. Crucially, it is largely unknown whether published findings can be reproduced by repeating reported analyses upon shared data (‘analytic reproducibility’). To investigate this, we conducted an observational evaluation of a mandatory open data policy introduced at the journal Cognition. Interrupted time-series analyses indicated a substantial post-policy increase in data available statements (104/417, 25\% pre-policy to 136/174, 78\% post-policy), although not all data appeared reusable (23/104, 22\% pre-policy to 85/136, 62\%, post-policy). For 35 of the articles determined to have reusable data, we attempted to reproduce 1324 target values. Ultimately, 64 values could not be reproduced within a 10\% margin of error. For 22 articles all target values were reproduced, but 11 of these required author assistance. For 13 articles at least one value could not be reproduced despite author assistance. Importantly, there were no clear indications that original conclusions were seriously impacted. Mandatory open data policies can increase the frequency and quality of data sharing. However, suboptimal data curation, unclear analysis specification and reporting errors can impede analytic reproducibility, undermining the utility of data sharing and the credibility of scientific findings. }
}

@article {aac4716,
	author = {,},
	title = {Estimating the reproducibility of psychological science},
	volume = {349},
	number = {6251},
	elocation-id = {aac4716},
	year = {2015},
	doi = {10.1126/science.aac4716},
	publisher = {American Association for the Advancement of Science},
	abstract = {One of the central goals in any scientific endeavor is to understand causality. Experiments that seek to demonstrate a cause/effect relation most often manipulate the postulated causal factor. Aarts et al. describe the replication of 100 experiments reported in papers published in 2008 in three high-ranking psychology journals. Assessing whether the replication and the original experiment yielded the same result according to several criteria, they find that about one-third to one-half of the original findings were also observed in the replication study.Science, this issue 10.1126/science.aac4716INTRODUCTIONReproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. Scientific claims should not gain credence because of the status or authority of their originator but by the replicability of their supporting evidence. Even research of exemplary quality may have irreproducible empirical findings because of random or systematic error.RATIONALEThere is concern about the rate and predictors of reproducibility, but limited evidence. Potentially problematic practices include selective reporting, selective analysis, and insufficient specification of the conditions necessary or sufficient to obtain the results. Direct replication is the attempt to recreate the conditions believed sufficient for obtaining a previously observed finding and is the means of establishing reproducibility of a finding with new data. We conducted a large-scale, collaborative effort to obtain an initial estimate of the reproducibility of psychological science.RESULTSWe conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. There is no single standard for evaluating replication success. Here, we evaluated reproducibility using significance and P values, effect sizes, subjective assessments of replication teams, and meta-analysis of effect sizes. The mean effect size (r) of the replication effects (Mr = 0.197, SD = 0.257) was half the magnitude of the mean effect size of the original effects (Mr = 0.403, SD = 0.188), representing a substantial decline. Ninety-seven percent of original studies had significant results (P \&lt; .05). Thirty-six percent of replications had significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.CONCLUSIONNo single indicator sufficiently describes replication success, and the five indicators examined here are not the only ways to evaluate reproducibility. Nonetheless, collectively these results offer a clear conclusion: A large portion of replications produced weaker evidence for the original findings despite using materials provided by the original authors, review in advance for methodological fidelity, and high statistical power to detect the original effect sizes. Moreover, correlational evidence is consistent with the conclusion that variation in the strength of initial evidence (such as original P value) was more predictive of replication success than variation in the characteristics of the teams conducting the research (such as experience and expertise). The latter factors certainly can influence replication success, but they did not appear to do so here.Reproducibility is not well understood because the incentives for individual scientists prioritize novelty over replication. Innovation is the engine of discovery and is vital for a productive, effective scientific enterprise. However, innovative ideas become old news fast. Journal reviewers and editors may dismiss a new test of a published idea as unoriginal. The claim that {\textquotedblleft}we already know this{\textquotedblright} belies the uncertainty of scientific evidence. Innovation points out paths that are possible; replication points out paths that are likely; progress relies on both. Replication can increase certainty when findings are reproduced and promote innovation when they are not. This project provides accumulating evidence for many findings in psychological research and suggests that there is still more work to do to verify whether we know what we think we know.Original study effect size versus replication effect size (correlation coefficients).Diagonal line represents replication effect size equal to original effect size. Dotted line represents replication effect size of 0. Points below the dotted line were effects in the opposite direction of the original. Density plots are separated by significant (blue) and nonsignificant (red) effects.Reproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. Replication effects were half the magnitude of original effects, representing a substantial decline. Ninety-seven percent of original studies had statistically significant results. Thirty-six percent of replications had statistically significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.},
	issn = {0036-8075},
	URL = {https://science.sciencemag.org/content/349/6251/aac4716},
	eprint = {https://science.sciencemag.org/content/349/6251/aac4716.full.pdf},
	journal = {Science}
}

